{"number": 134062, "owner": "wconstab", "title": "TORCH_NCCL_NAN_CHECK fails in certain scenarios", "create_time": "2024-08-21T00:07:00Z", "update_time": "2024-08-23T15:16:23Z", "body": "### Repro 1: IMA during PP points to Bug1 in certain collectives/ P2P\r\ngives an Illegal Memory Access: `CUDA_LAUNCH_BLOCKING=1 TORCH_NCCL_NAN_CHECK=1 python test/distributed/pipelining/test_schedule_multiproc.py -k test_forward_only_ScheduleClass0`\r\n\r\n<details><summary>Test output showing 'irecv' failing DSA</summary>\r\n======================================================================                                                                                                                        \r\nERROR: test_forward_only_ScheduleClass0 (__mp_main__.ScheduleTest)                                                                                                                            \r\n----------------------------------------------------------------------                                                                                                                        \r\nTraceback (most recent call last):                                                                                                                                                            \r\n  File \"/data/users/whc/pytorch/torch/testing/_internal/common_utils.py\", line 2918, in wrapper                                                                                               \r\n    method(*args, **kwargs)                                                                                                                                                                   \r\n  File \"/data/users/whc/pytorch/torch/testing/_internal/common_utils.py\", line 529, in instantiated_test                                                                                      \r\n    test(self, **param_kwargs)                                                                                                                                                                \r\n  File \"/data/users/whc/pytorch/test/distributed/pipelining/test_schedule_multiproc.py\", line 100, in test_forward_only                                                                       \r\n    out = schedule.step()                                                                                                                                                                     \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 694, in step                                                                                                 \r\n    self._step_microbatches(args_split, kwargs_split, targets_split, losses)                                                                                                                  \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 733, in _step_microbatches                                                                                   \r\n    works = _sorted_batch_p2p(ops, desc=\"fwd_recv\")                                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 628, in _sorted_batch_p2p                                                                                    \r\n    work_by_peer[peer] = _batch_p2p(ops, desc=desc)                                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 603, in _batch_p2p                                                                                           \r\n    return dist.batch_isend_irecv(p2p_ops).pop()                                                                                                                                              \r\n  File \"/data/users/whc/pytorch/torch/distributed/distributed_c10d.py\", line 2374, in batch_isend_irecv                                                                                       \r\n    p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/distributed_c10d.py\", line 2109, in irecv                                                                                                   \r\n    return pg.recv([tensor], src, tag)                                                                                                                                                        \r\nRuntimeError: CUDA error: an illegal memory access was encountered                                                                                                                            \r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                                                                                  </details>\r\n\r\nThe error above made me wonder, are we checking for NAN on the \"input\" tensor to operations that don't have to have valid inputs?  (E.g. recv takes 'tensor' as input, but then writes to it.  the values in 'tensor' can be garbage at the beginning- checking for NAN would be pointless).\r\n\r\nIndeed, we are checking input tensors to all pointToPoint() and collective() without consideration of whether they are 'real' inputs or 'output placeholders'.  This is a definite bug.\r\n\r\n### **Bug 1**: we check for NAN even on 'output' inputs.  We should only check tensors that are true inputs, depending on the type of operation.\r\n\r\nHowever, a simple fix to avoid the nan check on RECV ops isn't enough.  It does fix the exact error above, but presents a new (more confusing) one:\r\n\r\nNote: a complete fix for this bug needs to also gate nan checking on certain collectives- e.g. for scatter, only one rank has a valid input, others should be ignored.\r\n\r\n<details><summary>Fix (only nancheck for OpType::SEND)</summary>```\r\ndiff --git a/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp b/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\nindex 5970e489abd..94a40d0c0fc 100644\r\n--- a/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\n+++ b/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\n@@ -2964,7 +2964,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\r\n     PreProcess pre,\r\n     PostProcess post,\r\n     const char* profilingTitle) {\r\n-  if (enableNanCheck_) {\r\n+  if (opType == OpType::SEND && enableNanCheck_) {\r\n     checkForNan(tensor);\r\n   }\r\n```</details>\r\n\r\n<details><summary>New error complaining about garbage during `~TensorImpl`</summary>\r\nEterminate called after throwing an instance of 'c10::Error'                                                                                                                                  \r\n  what():  CUDA error: an illegal memory access was encountered                                                                                                                               \r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                                                                                           \r\n                                                                                                                                                                                              \r\nException raised from c10_cuda_check_implementation at /data/users/whc/pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):                                                        \r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xf6 (0x7fa99339a206 in /data/users/whc/pytorch/torch/lib/\r\nlibc10.so)                                                                                                                                                                                    \r\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xd3 (0x7fa9933984e3 i\r\nn /data/users/whc/pytorch/torch/lib/libc10.so)                                                                                                                                                \r\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x139 (0x7fa9947700f9 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                     \r\nframe #3: <unknown function> + 0x247ba (0x7fa99473e7ba in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #4: <unknown function> + 0x23050 (0x7fa99473d050 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #5: <unknown function> + 0x3b66a (0x7fa99475566a in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                 \r\nframe #6: <unknown function> + 0x16565 (0x7fa994730565 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #7: <unknown function> + 0x74d0b6 (0x7fa989f4d0b6 in /data/users/whc/pytorch/torch/lib/libtorch_python.so)                                                                              \r\nframe #8: c10::TensorImpl::~TensorImpl() + 0xdf (0x7fa99337867f in /data/users/whc/pytorch/torch/lib/libc10.so)                                                                               \r\nframe #9: c10::TensorImpl::~TensorImpl() + 0xa (0x7fa99337879a in /data/users/whc/pytorch/torch/lib/libc10.so)                                                            \r\nframe #10: torch::autograd::SavedVariable::reset_data() + 0xbc (0x7fa976ba454c in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                          \r\nframe #11: <unknown function> + 0x3e0e61e (0x7fa97580e61e in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #12: <unknown function> + 0x5173107 (0x7fa976b73107 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #13: torch::autograd::deleteNode(torch::autograd::Node*) + 0x138 (0x7fa976b73038 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                              \r\nframe #14: <unknown function> + 0x40ad57b (0x7fa975aad57b in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #15: <unknown function> + 0xc6ddea (0x7fa97266ddea in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)\r\nframe #16: <unknown function> + 0x5153536 (0x7fa976b53536 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #17: <unknown function> + 0x51535b9 (0x7fa976b535b9 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                       \r\nframe #18: c10::TensorImpl::~TensorImpl() + 0x99 (0x7fa993378639 in /data/users/whc/pytorch/torch/lib/libc10.so)                                                                    \r\nframe #19: c10::TensorImpl::~TensorImpl() + 0xa (0x7fa99337879a in /data/users/whc/pytorch/torch/lib/libc10.so)                            \r\nframe #20: <unknown function> + 0x8256a1 (0x7fa98a0256a1 in /data/users/whc/pytorch/torch/lib/libtorch_python.so)                                                                     \r\nframe #21: THPVariable_subclass_dealloc(_object*) + 0x18e (0x7fa98a025b3e in /data/users/whc/pytorch/torch/lib/libtorch_python.so)           \r\n</details>\r\n\r\n### Repro2 shows another IMA that appears unrelated to Bug1\r\n<details><summary>Repro 2</summary>\r\nRun via `CUDA_LAUNCH_BLOCKING=1 torchrun --nproc_per_node 2 repro.py`\r\n\r\n```\r\nimport torch\r\nimport os\r\nimport torch.distributed.distributed_c10d as c10d\r\n\r\ndef repro(rank, world_size):\r\n    os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\r\n    device = torch.device(\"cuda:%d\" % rank)\r\n    c10d.init_process_group(\r\n        backend=\"nccl\", rank=rank, world_size=world_size\r\n    )\r\n    \r\n    x = torch.ones((10,), dtype=torch.float32, device=device)\r\n    c10d.all_reduce(x)\r\n    print(f\"After first allreduce: {x=}\")\r\n    c10d.all_reduce(x)\r\n    print(f\"After second allreduce: {x=}\")\r\n    torch.cuda.synchronize()\r\n    c10d.destroy_process_group()\r\n    print(\"clean exit\")\r\n\r\nif __name__ == \"__main__\":\r\n    repro(int(os.environ[\"RANK\"]), int(os.environ[\"WORLD_SIZE\"]))\r\n```\r\n</details>\r\n\r\nRepro 2 should avoid the issue of the 'output' inputs being used becuase it only uses AllReduce operator, for which all inputs are valid inputs on all ranks.  A few notable points for this repro:\r\n\r\n* having only 1 allreduce, things pass\r\n* adding extra cuda synchronize() calls and print statements does not change anything, but i tried them as an unsuccessful way to narrow down the error\r\n* adding the second allreduce (and nan-check) causes an IMA error with a stack trace similar to the one above- implicating deallocation and `~TensorImpl`\r\ncc @XilunWu @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @d4l3k @c-p-i-o"}

{"number": 133010, "owner": "d4l3k", "title": "torch.multiprocessing.start_processes is blocking with large input arguments", "create_time": "2024-08-08T16:21:49Z", "update_time": "2024-08-09T12:35:52Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nWith small input arguments (<64kb) `start_processes` runs quickly as the processes are launched asynchronously.\r\n\r\nWhen they're large we end up blocking in https://github.com/python/cpython/blob/main/Lib/multiprocessing/popen_spawn_posix.py#L62 when writing to the pipe. The default pipe buffer size is 64kb so larger than that requires the child process to fully start.\r\n\r\nhttps://unix.stackexchange.com/questions/11946/how-big-is-the-pipe-buffer\r\n\r\nRepro:\r\n```\r\nimport time\r\nfrom torch.multiprocessing.spawn import start_processes\r\nimport os\r\n\r\ntime.sleep(1)\r\n\r\ndef trainer(rank):\r\n    print(rank)\r\n\r\nif __name__ == '__main__':\r\n    world_size = 10\r\n\r\n    start = time.perf_counter()\r\n\r\n    args = [\"1\"*100000]\r\n\r\n    ctx = start_processes(\r\n            fn=trainer,\r\n            args=args,\r\n            nprocs=world_size,\r\n            start_method=\"spawn\",\r\n            join=False,\r\n        )\r\n\r\n    print(f\"Time taken: {time.perf_counter() - start}\")\r\n```\r\n\r\n### Versions\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.5.0a0+git21d4c48\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.2\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Stream 9 (x86_64)\r\nGCC version: (GCC) 11.4.1 20231218 (Red Hat 11.4.1-3)\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.34\r\n\r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.19.0-0_fbk21_hardened_12633_g4db063a1bcb5-x86_64-with-glibc2.34\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100\r\nGPU 1: NVIDIA H100\r\nGPU 2: NVIDIA H100\r\nGPU 3: NVIDIA H100\r\n\r\nNvidia driver version: 525.105.17\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.8.1\r\n/usr/lib64/libcudnn.so.9.1.1\r\n/usr/lib64/libcudnn_adv.so.9.1.1\r\n/usr/lib64/libcudnn_adv_infer.so.8.8.1\r\n/usr/lib64/libcudnn_adv_train.so.8.8.1\r\n/usr/lib64/libcudnn_cnn.so.9.1.1\r\n/usr/lib64/libcudnn_cnn_infer.so.8.8.1\r\n/usr/lib64/libcudnn_cnn_train.so.8.8.1\r\n/usr/lib64/libcudnn_engines_precompiled.so.9.1.1\r\n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.1.1\r\n/usr/lib64/libcudnn_graph.so.9.1.1\r\n/usr/lib64/libcudnn_heuristic.so.9.1.1\r\n/usr/lib64/libcudnn_ops.so.9.1.1\r\n/usr/lib64/libcudnn_ops_infer.so.8.8.1\r\n/usr/lib64/libcudnn_ops_train.so.8.8.1\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          184\r\nOn-line CPU(s) list:             0-183\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 9654 96-Core Processor\r\nCPU family:                      25\r\nModel:                           17\r\nThread(s) per core:              1\r\nCore(s) per socket:              184\r\nSocket(s):                       1\r\nStepping:                        1\r\nBogoMIPS:                        4792.79\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean pausefilter pfthreshold v_vmsave_vmload vgif avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm arch_capabilities\r\nVirtualization:                  AMD-V\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       11.5 MiB (184 instances)\r\nL1i cache:                       11.5 MiB (184 instances)\r\nL2 cache:                        92 MiB (184 instances)\r\nL3 cache:                        2.9 GiB (184 instances)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-183\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==6.1.0\r\n[pip3] flake8-bugbear==23.3.23\r\n[pip3] flake8-coding==1.3.3\r\n[pip3] flake8-comprehensions==3.15.0\r\n[pip3] flake8-executable==2.1.3\r\n[pip3] flake8-logging-format==0.9.0\r\n[pip3] flake8-pyi==23.3.1\r\n[pip3] flake8-simplify==0.19.3\r\n[pip3] mypy==1.10.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.0\r\n[pip3] optree==0.12.1\r\n[pip3] pytorch-triton==3.0.0+a9bc1a3647\r\n[pip3] torch==2.5.0a0+git21d4c48\r\n[pip3] torchx==0.6.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] magma-cuda121             2.6.1                         1    pytorch\r\n[conda] mkl                       2023.1.0         h213fc3f_46344  \r\n[conda] mkl-include               2023.2.0            intel_49495    intel\r\n[conda] mkl-service               2.4.0           py310h5eee18b_1  \r\n[conda] mkl-static                2023.2.0            intel_49495    intel\r\n[conda] mkl_fft                   1.3.8           py310h5eee18b_0  \r\n[conda] mkl_random                1.2.4           py310hdb19cb5_0  \r\n[conda] numpy                     1.26.0                   pypi_0    pypi\r\n[conda] numpy-base                1.26.4          py310hb5e798b_0  \r\n[conda] optree                    0.12.1                   pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+a9bc1a3647          pypi_0    pypi\r\n[conda] torch                     2.5.0a0+git21d4c48           dev_0    <develop>\r\n[conda] torchfix                  0.4.0                    pypi_0    pypi\r\n[conda] torchx                    0.6.0                    pypi_0    pypi\r\n```\r\n\r\ncc @VitalyFedyunin"}

{"number": 128626, "owner": "Skylion007", "title": "Generated some code to try to torch compile it, and it fails for the same code when it's torch compiled", "create_time": "2024-06-13T17:07:44Z", "update_time": "2024-06-13T18:45:27Z", "body": "### \ud83d\udc1b Describe the bug\n\nThe code runs fine without torch compile and fails with it.\n\n### Error logs\n\n```\r\n---------------------------------------------------------------------------\r\n\r\nAssertionError                            Traceback (most recent call last)\r\n\r\n[<ipython-input-10-62eb8aca15c2>](https://localhost:8080/#) in <cell line: 50>()\r\n     48 \r\n     49 # Benchmark the compiled model\r\n---> 50 compiled_time = benchmark_model(compiled_model, x)\r\n     51 print(f\"Compiled model time per iteration: {compiled_time:.6f} seconds\")\r\n     52 #Compiled model time per iteration: 0.002723 seconds\r\n\r\n20 frames\r\n\r\n[/tmp/torchinductor_root/xf/cxfiujff7d7xmb256m6kkhbnaffva4zxfz6adttpew65bo3mju4e.py](https://localhost:8080/#) in call(args)\r\n    135         # Source Nodes: [forward_output], Original ATen: [aten.convolution]\r\n    136         buf0 = extern_kernels.convolution(reinterpret_tensor(primals_5, (10, 1, 100), (100, 1, 1), 0), primals_1, stride=(1,), padding=(1,), dilation=(1,), transposed=False, output_padding=(0,), groups=1, bias=None)\r\n--> 137         assert_size_stride(buf0, (10, 16, 100), (1600, 100, 1))\r\n    138         buf1 = empty_strided_cuda((10, 1, 100), (100, 100, 1), torch.float32)\r\n    139         # Source Nodes: [x_flipped], Original ATen: [aten.flip]\r\n\r\nAssertionError: expected size 16==16, stride 1==100 at dim=1\r\n```\n\n### Minified repro\n\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport time\r\n\r\n# Define the BidirectionalConv1d module\r\nclass BidirectionalConv1d(nn.Module):\r\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\r\n        super(BidirectionalConv1d, self).__init__()\r\n        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\r\n        self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\r\n\r\n    def forward(self, x):\r\n        batch_size, time_steps, directions = x.shape\r\n        x = x.permute(0, 2, 1)\r\n\r\n        forward_output = self.conv1(x)\r\n        x_flipped = torch.flip(x, [2])\r\n        backward_output = self.conv2(x_flipped)\r\n\r\n        output = torch.cat((forward_output, backward_output), dim=1)\r\n        return output\r\n\r\n# Function to benchmark the model\r\ndef benchmark_model(model, inputs, iterations=100):\r\n    torch.cuda.synchronize()\r\n    start_time = time.time()\r\n\r\n    for _ in range(iterations):\r\n        outputs = model(inputs)\r\n\r\n    torch.cuda.synchronize()\r\n    end_time = time.time()\r\n    return (end_time - start_time) / iterations\r\n\r\n# Example usage\r\nbatch_size, time_steps, directions = 10, 100, 1\r\nx = torch.randn(batch_size, time_steps, directions).cuda()\r\n\r\nmodel = BidirectionalConv1d(1, 16, kernel_size=3, stride=1, padding=1).cuda()\r\n\r\n# Benchmark the uncompiled model\r\nuncompiled_time = benchmark_model(model, x)\r\nprint(f\"Uncompiled model time per iteration: {uncompiled_time:.6f} seconds\")\r\n\r\n# Compile the model with torch.jit.script\r\ncompiled_model = torch.compile(model)\r\n#Compiled model time per iteration: 0.002723 seconds\r\n\r\n# Benchmark the compiled model\r\ncompiled_time = benchmark_model(compiled_model, x)\r\nprint(f\"Compiled model time per iteration: {compiled_time:.6f} seconds\")\r\n```\n\n### Versions\n\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 535.104.05\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               2\r\nOn-line CPU(s) list:                  0,1\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\r\nCPU family:                           6\r\nModel:                                85\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   1\r\nSocket(s):                            1\r\nStepping:                             3\r\nBogoMIPS:                             4000.29\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            32 KiB (1 instance)\r\nL1i cache:                            32 KiB (1 instance)\r\nL2 cache:                             1 MiB (1 instance)\r\nL3 cache:                             38.5 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0,1\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Mitigation; PTE Inversion\r\nVulnerability Mds:                    Vulnerable; SMT Host state unknown\r\nVulnerability Meltdown:               Vulnerable\r\nVulnerability Mmio stale data:        Vulnerable\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Vulnerable\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Vulnerable\r\nVulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.25.2\r\n[pip3] torch==2.3.0+cu121\r\n[pip3] torchaudio==2.3.0+cu121\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.18.0\r\n[pip3] torchvision==0.18.0+cu121\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\n\ncc @ezyang @anijain2305 @chauhang @eellison"}

{"number": 128609, "owner": "XuehaiPan", "title": "[BUG] Circular dependency `torch.nn.parameter` -> `torch.nn` -> `torch.autograd` -> `torch._C._autograd_init()` -> `torch.nn.parameter`", "create_time": "2024-06-13T12:58:46Z", "update_time": "2024-06-28T15:17:10Z", "body": "### \ud83d\udc1b Describe the bug\n\nRemoving `import torch.nn.functional as F` in `torch/functional.py` causing circular import error:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/2b9465d62ab8733a36226f0d8e236a8a9bd60c23/torch/functional.py#L7\r\n\r\n```pytb\r\n$ python3 -c 'import torch'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/__init__.py\", line 1959, in <module>\r\n    from torch import _VF as _VF, functional as functional  # usort: skip\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/functional.py\", line 9, in <module>\r\n    from torch._jit_internal import _overload as overload, boolean_dispatch\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/_jit_internal.py\", line 43, in <module>\r\n    import torch.distributed as dist\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/distributed/__init__.py\", line 147, in <module>\r\n    from torch.distributed import rpc as rpc\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/distributed/rpc/__init__.py\", line 75, in <module>\r\n    from .server_process_global_profiler import (\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/distributed/rpc/server_process_global_profiler.py\", line 7, in <module>\r\n    from torch.autograd.profiler_legacy import profile\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/autograd/__init__.py\", line 491, in <module>\r\n    if not torch._C._autograd_init():\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/nn/__init__.py\", line 2, in <module>\r\n    from torch.nn.modules import *  # noqa: F403\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/nn/modules/__init__.py\", line 1, in <module>\r\n    from .activation import (\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/nn/modules/activation.py\", line 6, in <module>\r\n    import torch.nn.functional as F\r\n  File \"/Users/PanXuehai/Projects/pytorch/torch/nn/functional.py\", line 11, in <module>\r\n    from torch._jit_internal import (\r\nImportError: cannot import name '_overload' from partially initialized module 'torch._jit_internal' (most likely due to a circular import) (/Users/PanXuehai/Projects/pytorch/torch/_jit_internal.py)\r\n```\n\n### Versions\n\nnightly\n\ncc @ezyang @albanD @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7"}

{"number": 128121, "owner": "alex77g2", "title": "2nd compile of deepcopy(model) fails on multiple ubuntu-pc (fatal error: Python.h: file not found)", "create_time": "2024-06-06T07:06:29Z", "update_time": "2024-07-29T14:32:02Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nI use different Systems: `Python 3.11, torch 2.2.2, Ubuntu 22.04 or 23.10 (x64), all Cuda 12.x, all Intel CPUs (11..13 gen), all using pip not conda` system language setting english or german.\r\nFirst torch.compile(model) works fine on all 5 systems, and numeric results are not affected by compile.\r\nBut 2nd compile (as we use two model instances in parallel) fails on some systems (since some weeks - see below).\r\n\r\nAs far as I remember: the error started with torch 2.3, but downgrade to 2.2 did not recover. (not sure if this true for all 5 pc)\r\nSo I cannot afford to check on another system doing a torch version change at the moment (I'm afraid of more issues).\r\n\r\nBut anyway, I have no idea what could be the reason for suddenly not finding a header file, that was useable shortly before, again and again .. (what is different for 2nd compile regarding system setup?)\r\nPerhaps some post-compile cleanup is removing the header file after first compile, and it is not prepared a 2nd time (as most users only compile one model per run, it could be invisible issue usually, just speculation here ..)\r\n\r\n```\r\nclass Net(torch.nn.Module): # 10+10, N= 7960\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.fc1 = torch.nn.Linear(784, 10)\r\n        self.relu = torch.nn.ReLU()\r\n        self.fc2 = torch.nn.Linear(10, 10)\r\n\r\n    def forward(self, x): # line 59 from stderr log below\r\n        z1 = self.fc1(torch.flatten(x, start_dim=1))\r\n        z1 = self.fc2(self.relu(z1))\r\n        return F.log_softmax(z1, dim=1)\r\n\r\n# model is trivial Net above or ResNet (18 or 34 or Wide28 or 50)\r\nmodel = torch.compile(model) # always ok\r\n# here 10 epochs training (good progess, no issues)\r\nmodel2 = deepcopy(model) # ok\r\nmodel2 = torch.compile(model2) # fails (system dependend)\r\n```\r\n\r\nparts of stderr log:\r\n```\r\n/tmp/tmpmiq1y2mh/main.c:5:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden # german: cannot find file\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\n/tmp/tmpj15_8vtb/main.c:5:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated. # repeated for 8 C-files.\r\n....\r\ntorch._dynamo.convert_frame: [WARNING] CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp5o0_7ce5/main.c', '-O3', '-I/home/aleks/myvenv3.11/lib/python3.11/site-packages/triton/common/../third_party/cuda/include', '-I/usr/include/python3.11', '-I/tmp/tmp5o0_7ce5', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmp5o0_7ce5/triton_.cpython-311-x86_64-linux-gnu.so', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu']' returned non-zero exit status 1.\r\n```\r\n\r\n\r\n### Error logs\r\n\r\n/tmp/tmpmiq1y2mh/main.c:5:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden # german: cannot find file\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\n/tmp/tmpj15_8vtb/main.c:5:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\n/tmp/tmpqemdf8mu/main.c:5:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\n/tmp/tmpzbktfctd/main.c:5:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\n/tmp/tmp32faforh/main.c:5:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\n/tmp/tmplpa8sknu/main.c:5:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden\r\n    5 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\n/tmp/tmp5o0_7ce5/main.c:4:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden\r\n    4 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] WON'T CONVERT forward /data/Sourcen/git/numericspublication/main_elra.py line 59 \r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] due to: \r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 727, in _convert_frame\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     result = inner_convert(frame, cache_entry, hooks, frame_state)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 383, in _convert_frame_assert\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_product = _compile(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                        ^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 646, in _compile\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 562, in compile_inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     out_code = transform_code_object(code, transform)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     transformations(instructions, code_options)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 151, in _fn\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 527, in transform\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     tracer.run()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2128, in run\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     super().run()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     and self.step()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     getattr(self, inst.opname)(inst)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2243, in RETURN_VALUE\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     self.output.compile_subgraph(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 919, in compile_subgraph\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.11/contextlib.py\", line 81, in inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1087, in compile_and_call_fx_graph\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = self.call_user_compiler(gm)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1159, in call_user_compiler\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1140, in call_user_compiler\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = compiler_fn(gm, self.example_inputs())\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 117, in debug_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_gm = compiler_fn(gm, example_inputs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/__init__.py\", line 1668, in __call__\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return compile_fx(model_, inputs_, config_patches=self.config)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1168, in compile_fx\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return aot_autograd(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/backends/common.py\", line 55, in compiler_fn\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py\", line 887, in aot_module_simplified\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = create_aot_dispatcher_function(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py\", line 600, in create_aot_dispatcher_function\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 425, in aot_wrapper_dedupe\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 630, in aot_wrapper_synthetic_base\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 97, in aot_dispatch_base\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fw = compiler(fw_module, updated_flat_args)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1100, in fw_compiler_base\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return inner_compile(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py\", line 83, in debug_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     inner_compiled_fn = compiler_fn(gm, example_inputs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/debug.py\", line 305, in inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.11/contextlib.py\", line 81, in inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 320, in compile_fx_inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_graph = fx_codegen_and_compile(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                      ^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 550, in fx_codegen_and_compile\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = graph.compile_to_fn()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1116, in compile_to_fn\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return self.compile_to_module().call\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1070, in compile_to_module\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     mod = PyCodeCache.load_by_key_path(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 1892, in load_by_key_path\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     exec(code, mod.__dict__, mod.__dict__)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/tmp/torchinductor_aleks/si/csivrh32cwzjdjoq6hzozmar3c3khh7gta6rjyou63fgxeyso6zl.py\", line 115, in <module>\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     async_compile.wait(globals())\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 2486, in wait\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     scope[key] = result.result()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                  ^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 2329, in result\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     self.future.result()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return self.__get_result()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     raise self._exception\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp5o0_7ce5/main.c', '-O3', '-I/home/aleks/myvenv3.11/lib/python3.11/site-packages/triton/common/../third_party/cuda/include', '-I/usr/include/python3.11', '-I/tmp/tmp5o0_7ce5', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmp5o0_7ce5/triton_.cpython-311-x86_64-linux-gnu.so', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu']' returned non-zero exit status 1.\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] \r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] \r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 727, in _convert_frame\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     result = inner_convert(frame, cache_entry, hooks, frame_state)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 383, in _convert_frame_assert\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_product = _compile(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                        ^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 646, in _compile\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 562, in compile_inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     out_code = transform_code_object(code, transform)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     transformations(instructions, code_options)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 151, in _fn\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 527, in transform\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     tracer.run()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2128, in run\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     super().run()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     and self.step()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     getattr(self, inst.opname)(inst)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2243, in RETURN_VALUE\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     self.output.compile_subgraph(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 919, in compile_subgraph\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.11/contextlib.py\", line 81, in inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1087, in compile_and_call_fx_graph\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = self.call_user_compiler(gm)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1159, in call_user_compiler\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 1140, in call_user_compiler\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = compiler_fn(gm, self.example_inputs())\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 117, in debug_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_gm = compiler_fn(gm, example_inputs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/__init__.py\", line 1668, in __call__\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return compile_fx(model_, inputs_, config_patches=self.config)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1168, in compile_fx\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return aot_autograd(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/backends/common.py\", line 55, in compiler_fn\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     cg = aot_module_simplified(gm, example_inputs, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py\", line 887, in aot_module_simplified\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = create_aot_dispatcher_function(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py\", line 600, in create_aot_dispatcher_function\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 425, in aot_wrapper_dedupe\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 630, in aot_wrapper_synthetic_base\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 97, in aot_dispatch_base\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fw = compiler(fw_module, updated_flat_args)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 1100, in fw_compiler_base\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return inner_compile(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py\", line 83, in debug_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     inner_compiled_fn = compiler_fn(gm, example_inputs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/debug.py\", line 305, in inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.11/contextlib.py\", line 81, in inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return func(*args, **kwds)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 320, in compile_fx_inner\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_graph = fx_codegen_and_compile(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                      ^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 550, in fx_codegen_and_compile\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     compiled_fn = graph.compile_to_fn()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                   ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1116, in compile_to_fn\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return self.compile_to_module().call\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/graph.py\", line 1070, in compile_to_module\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     mod = PyCodeCache.load_by_key_path(\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 1892, in load_by_key_path\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     exec(code, mod.__dict__, mod.__dict__)\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/tmp/torchinductor_aleks/si/csivrh32cwzjdjoq6hzozmar3c3khh7gta6rjyou63fgxeyso6zl.py\", line 115, in <module>\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     async_compile.wait(globals())\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 2486, in wait\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     scope[key] = result.result()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]                  ^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 2329, in result\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     self.future.result()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     return self.__get_result()\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING]     raise self._exception\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp5o0_7ce5/main.c', '-O3', '-I/home/aleks/myvenv3.11/lib/python3.11/site-packages/triton/common/../third_party/cuda/include', '-I/usr/include/python3.11', '-I/tmp/tmp5o0_7ce5', '-shared', '-fPIC', '-lcuda', '-o', '/tmp/tmp5o0_7ce5/triton_.cpython-311-x86_64-linux-gnu.so', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu', '-L/lib/x86_64-linux-gnu', '-L/lib/i386-linux-gnu']' returned non-zero exit status 1.\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] \r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n[2024-06-06 07:46:58,066] torch._dynamo.convert_frame: [WARNING] \r\n/tmp/tmp4aexwpzz/main.c:4:10: fatal error: Python.h: Datei oder Verzeichnis nicht gefunden\r\n    4 | #include <Python.h>\r\n      |          ^~~~~~~~~~\r\ncompilation terminated.\r\nException ignored in atexit callback: <function _MultiProcessingDataLoaderIter._clean_up_worker at 0x7511dbf958a0>\r\nTraceback (most recent call last):\r\n  File \"/home/aleks/myvenv3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1473, in _clean_up_worker\r\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\r\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 149, in join\r\n    res = self._popen.wait(timeout)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\r\n    if not wait([self.sentinel], timeout):\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\r\n    ready = selector.select(timeout)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\r\n    fd_event_list = self._selector.poll(timeout)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nKeyboardInterrupt: \r\n\r\n### Minified repro\r\n\r\nA short snipset is above.\r\nIt takes significant effort to shrink the project (and I cannot do it this week). But not finding a header inside compile should not depend on our optimizer training code. Please read the above first. If needed, I could provide a hidden private repo next week.\r\n\r\n### Versions\r\n\r\nUbuntu 22.04 (x64) + Python 3.11.9\r\n\r\ntorch                    2.2.2\r\ntorchaudio               2.2.2\r\ntorchvideo               0.0.0\r\ntorchvision              0.17.2\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n|   0  NVIDIA GeForce RTX 4060 Ti     Off | 00000000:01:00.0  On |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\ngcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\n\r\n\r\ncc @ezyang @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @msaroufim"}

{"number": 127173, "owner": "vmoens", "title": "[inline-inbuilt-nn-modules] tensordict functional calls with nn.Module silently gives the wrong (non-functional) result", "create_time": "2024-05-25T14:42:47Z", "update_time": "2024-07-22T17:40:37Z", "body": "### \ud83d\udc1b Describe the bug\n\nThe following tests checks that swapping params (ie, making a functional call) works with torch.compile\r\nhttps://github.com/pytorch/tensordict/blob/80965e0db2a574a9510c833cfceb7cdb0c00eed2/test/test_compile.py#L306-L314\r\n\r\nFor context, the test takes a copy of the parameters of the module and zeroes that copy, then makes a functional call. The results should all be 0.\r\n\r\nThe test on this line fails with the compiled version (but works in eager)\r\n- compile, fails: https://github.com/pytorch/tensordict/blob/80965e0db2a574a9510c833cfceb7cdb0c00eed2/test/test_compile.py#L325\r\n- eager, runs:\r\nhttps://github.com/pytorch/tensordict/blob/80965e0db2a574a9510c833cfceb7cdb0c00eed2/test/test_compile.py#L318\r\nsuggesting that compile ignores the functional call.\r\nThis can be confirmed by uncommenting [this line](https://github.com/pytorch/tensordict/blob/80965e0db2a574a9510c833cfceb7cdb0c00eed2/test/test_compile.py#L324) which tests that the functional call is (wrongly) equal to the non-functional call. This passes, again confirming that compile ignores the functional call.\r\n\r\nSetting `backend=\"eager\"` in the call to compile does not solve the bug. \r\n\r\nThere is some shady optimization of `nn.Module.__setattr__` happening in `TensorDict.to_module` but I don't believe this is the explanation since skipping these and falling back on a regulat `setattr(module, param_name, value)` gives the exact same result. \r\n\r\n@bdhirsh pointed me to `TORCHDYNAMO_INLINE_INBUILT_NN_MODULES` which solves the problem!\r\n\r\nHappy to provide more context if needed\n\n### Versions\n\nnightly \n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng"}

{"number": 126200, "owner": "rmast", "title": "correct BLAS input", "create_time": "2024-05-14T19:41:43Z", "update_time": "2024-08-12T17:05:05Z", "body": "Fixes #32407\r\n\r\nWith this little correction to Dependencies.cmake it is possible to build an MKL-free version of Pytorch up from version v2.0.0 by explicitly choosing another MKL-free BLAS.\r\n\r\nThis pullrequest fulfills the \"if not already present\" part of the original comment in  Dependencies.cmake:\r\n\"setting default preferred BLAS options if not already present.\" \r\n\r\nIt's tested with this Action-.yml:\r\n```\r\nname: Build PyTorch v2.0.0 without AVX\r\n\r\non:\r\n  push:\r\n    branches:\r\n      - v2.0.0\r\n  pull_request:\r\n    branches:\r\n      - v2.0.0\r\n\r\njobs:\r\n  build:\r\n    runs-on: ubuntu-20.04\r\n    defaults:\r\n      run:\r\n        shell: bash -el {0}\r\n    steps:\r\n\r\n    - name: Checkout repository\r\n      uses: actions/checkout@v4\r\n      with:\r\n        #repository: 'pytorch/pytorch'\r\n        #ref: 'v2.3.0'\r\n        submodules: 'recursive'\r\n\r\n    - uses: conda-incubator/setup-miniconda@v3\r\n      with:\r\n        auto-activate-base: true\r\n        activate-environment: true\r\n        python-version: 3.10.13\r\n\r\n    - name: Install Dependencies - Common - Linux 2\r\n      run: |\r\n        conda info\r\n        conda list\r\n        conda install nomkl\r\n        conda install astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclasses\r\n        export PYTORCH_CPU_CAPABILITY=cpu\r\n        export ATEN_CPU_CAPABILITY_DEFAULT=cpu\r\n        export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\r\n        export ATEN_CPU_CAPABILITY=default\r\n        export USE_NNPACK=0\r\n        export MAX_JOBS=4\r\n        export USE_CUDA=0\r\n        export USE_ROCM=0\r\n        export BLAS=OpenBLAS\r\n        export CMAKE_ARGS=\"-D CMAKE_BUILD_TYPE=Release -D USE_AVX=OFF -D USE_NNPACK=OFF -D C_HAS_AVX_2=OFF -D C_HAS_AVX2_2=OFF -D CXX_HAS_AVX_2=OFF -D CXX_HAS_AVX2_2=OFF -D CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS=OFF -DPYTHON_INCLUDE_DIR=$(python -c \"import sysconfig; print(sysconfig.get_path('include'))\") -DPYTHON_LIBRARY=$(python -c \"import sysconfig; print(sysconfig.get_config_var('LIBDIR'))\") -DPYTHON_EXECUTABLE:FILEPATH=`which python`\"\r\n        pip install build wheel typing_extensions\r\n        python setup.py bdist_wheel\r\n    - name: Archive production artifacts\r\n      uses: actions/upload-artifact@v4\r\n      with:\r\n        name: dist-without-markdown\r\n        path: |\r\n          dist\r\n          !dist/**/*.md\r\n```"}

{"number": 125230, "owner": "fxmarty", "title": "ROCm: `fatal error: aotriton/flash.h: No such file or directory` when building with `USE_ROCM=1`", "create_time": "2024-04-30T14:02:56Z", "update_time": "2024-08-19T16:46:23Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nI am running in `rocm/dev-ubuntu-22.04:6.1`.\r\n\r\nBuilding PyTorch 2.3 (https://github.com/pytorch/pytorch/tree/release/2.3) with\r\n```\r\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\r\npip install -r requirements.txt --no-cache-dir\r\npip install numpy ninja --no-cache-dir\r\npip uninstall -y triton && \\\r\n    git clone --depth 1 --single-branch https://github.com/ROCm/triton.git && \\\r\n    cd triton/python && \\\r\n    pip install .\r\n_GLIBCXX_USE_CXX11_ABI=\"1\" PYTORCH_ROCM_ARCH=\"gfx90a;gfx942\" BUILD_CAFFE2=0 BUILD_CAFFE2_OPS=0 USE_CUDA=0 USE_ROCM=1 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 USE_FLASH_ATTENTION=0 USE_MEM_EFF_ATTENTION=0 python setup.py install\r\n```\r\n\r\nand getting \r\n```\r\n#33 891.8 cc1plus: warning: command-line option \u2018-Wno-duplicate-decl-specifier\u2019 is valid for C/ObjC but not for C++\r\n#33 891.8 /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:26:10: fatal error: aotriton/flash.h: No such file or directory\r\n#33 891.8    26 | #include <aotriton/flash.h>\r\n#33 891.8       |          ^~~~~~~~~~~~~~~~~~\r\n#33 891.8 compilation terminated.\r\n```\r\n\r\naround the end of the build.\r\n\r\nIt appears that the file `aotriton/flash.h` is not in PyTorch codebase. It is here though: https://github.com/ROCm/aotriton (which says the install should be smooth through pytorch https://github.com/ROCm/aotriton?tab=readme-ov-file#pytorch-consumption, so I am confused).\r\n\r\nIs something wrong in https://github.com/pytorch/pytorch/blob/main/cmake/External/aotriton.cmake? Maybe need an `include_directories(build/aotriton/src/include)`?\r\n\r\nApparently during the build both `./build/aotriton/src/include/aotriton/flash.h` and `./torch/include/aotriton/flash.h` are generated, but not used.\r\n\r\nThank you!\r\n\r\n### Versions\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0] (64-bit runtime)\r\nPython platform: /\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8480C\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           8\r\nBogoMIPS:                           4000.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 avx512vbmi umip waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm serialize amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          4.5 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           192 MiB (96 instances)\r\nL3 cache:                           210 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-47\r\nNUMA node1 CPU(s):                  48-95\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Unknown: No mitigations\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.11.0\r\n[pip3] triton==2.1.0\r\n[conda] mkl-include               2024.1.0              intel_691    intel\r\n[conda] mkl-static                2024.1.0              intel_691    intel\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] optree                    0.11.0                   pypi_0    pypi\r\n[conda] triton                    2.1.0                    pypi_0    pypi\r\n```\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang"}

{"number": 122529, "owner": "bigning", "title": "Distributed checkpoint state_dict load may report nccl error and hide the real root-cause exception", "create_time": "2024-03-22T22:31:14Z", "update_time": "2024-06-14T19:43:08Z", "body": "### \ud83d\udc1b Describe the bug\n\nHi, when we call `torch.distributed.checkpoint.load_state_dict(state_dict, storage_reader)`, and if there is exception in storage_reader.read_data(), the exception is not immediately raised.[ It's captured here, and then dist.all_gather_objects(exception) before raising. ](https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/utils.py#L246-L255)\r\n\r\nIssue is:\r\n1. [that `self.all_gather_object()`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/utils.py#L251) might timeout if only one rank failed storage_reader.read_data() and the remaining ranks continue read_data, which takes longer time than nccl timeout. \r\n2. User might use a customized `storage_reader` (e.g. downloading from cloud), and there might be nccl communication in the `customized_storage_reader.read_data()`. In this case, if one rank failed `customized_storage_reader.read_data()`, the failure rank starts this [all_gather_object()](https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/utils.py#L251), the remaining ranks start the other nccl comms in the `customized_storage_reader.read_data()`. This causes nccl comms mismatch 100%. \r\n\r\nWhat's worse, in both above case, the program crashes caused by nccl error before raising the real `storage_reader.read_data()` exception. The real exception message will never be visible to user. \r\n\r\nTo print the `read_data` exception message, we have to capture the exception and  immediately print the exception message inside `read_data`. Example: https://github.com/mosaicml/composer/pull/3131\r\n\r\nCan we just raise the `read_data()` exception immediately for the failure rank? I understand that the non-failure rank will continue run and timeout eventually at the next nccl comm, but at least the failure rank can print the real root-cause. \r\n\r\n\n\n### Versions\n\nPyTorch version: 2.3.0.dev20240110+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang @d4l3k @LucasLLC @MeetVadakkanchery @mhorowitz @rohan-varma"}

{"number": 122227, "owner": "Aidyn-A", "title": "[Inductor][Triton] test_mixed_mm2_cuda fails on Hopper", "create_time": "2024-03-19T20:44:17Z", "update_time": "2024-03-28T04:43:51Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nWe are seeing some segfault failures on Hopper GPUs on all Triton 3.0.0 versions:\r\n\r\n`pytorch_triton-3.0.0+989adb9a29`:\r\n```python\r\npython test/inductor/test_torchinductor.py -v -k test_mixed_mm2_cuda\r\ntest_mixed_mm2_cuda (__main__.GPUTests) ... python: /source/llvm-project/mlir/lib/IR/TypeRange.cpp:20: mlir::TypeRange::TypeRange(ArrayRef<mlir::Type>): Assertion `llvm::all_of(types, [](Type t) { return t; }) && \"attempting to construct a TypeRange with null types\"' failed.\r\n```\r\n\r\n`pytorch_triton-3.0.0+a9bc1a3647`:\r\n```python\r\npython test/inductor/test_torchinductor.py -v -k test_mixed_mm2_cuda\r\ntest_mixed_mm2_cuda (__main__.GPUTests) ... [d8aa2091f3fe:962  :0:962] Caught signal 11 (Segmentation fault: Sent by the kernel at address (nil))\r\n```\r\n\r\n`pytorch_triton-3.0.0+901819d2b6`\r\n```python\r\npython test/inductor/test_torchinductor.py -v -k test_mixed_mm2_cuda\r\ntest_mixed_mm2_cuda (__main__.GPUTests) ... [d8aa2091f3fe:1275 :0:1275] Caught signal 11 (Segmentation fault: invalid permissions for mapped object at address 0x7f745aeec067)\r\n```\r\n\r\n### Versions\r\n\r\nPyTorch version: 2.4.0a0+git0d845f7b\r\nNVIDIA H100 80GB\r\n\r\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire"}

{"number": 121725, "owner": "TheoBoyer", "title": "torch.logsumexp throws RuntimeError instead of reducing all dimensions when dim=None", "create_time": "2024-03-12T13:26:54Z", "update_time": "2024-03-15T16:47:50Z", "body": "### \ud83d\udc1b Describe the bug\n\nThe [documentation ](https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch-logsumexp) of the dim argument of torch.logsumexp says: \r\n\r\n> dim ([int](https://docs.python.org/3/library/functions.html#int) or [tuple](https://docs.python.org/3/library/stdtypes.html#tuple) of ints, optional) \u2013 the dimension or dimensions to reduce. If None, all dimensions are reduced. \r\n\r\nHowever, when setting `dim=None`, which according to the documentation should reduce across all dimensions, the function throws a `RuntimeError` instead:\r\n\r\n```python\r\nimport torch\r\nprint(torch.__version__) # 2.2.1+cu121\r\nprint(torch.logsumexp(torch.randn(3, 3), dim=(0, 1))) # tensor(2.4638)\r\nprint(torch.logsumexp(torch.randn(3, 3), dim=None))\r\n```\r\nRaises:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n[<ipython-input-3-ce5028272961>](https://localhost:8080/#) in <cell line: 4>()\r\n      2 print(torch.__version__) #\r\n      3 print(torch.logsumexp(torch.randn(3, 3), dim=(0, 1))) #\r\n----> 4 print(torch.logsumexp(torch.randn(3, 3), dim=None)) #\r\n\r\nRuntimeError: Please look up dimensions by name, got: name = None.\r\n```\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: version 3.27.9\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             2\r\nOn-line CPU(s) list:                0,1\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\r\nCPU family:                         6\r\nModel:                              79\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 1\r\nSocket(s):                          1\r\nStepping:                           0\r\nBogoMIPS:                           4399.99\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          32 KiB (1 instance)\r\nL1i cache:                          32 KiB (1 instance)\r\nL2 cache:                           256 KiB (1 instance)\r\nL3 cache:                           55 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0,1\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Mitigation; PTE Inversion\r\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\r\nVulnerability Meltdown:             Vulnerable\r\nVulnerability Mmio stale data:      Vulnerable\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.25.2\r\n[pip3] torch==2.2.1\r\n[pip3] torchaudio==2.1.0+cu121\r\n[pip3] torchdata==0.7.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.16.0\r\n[pip3] torchvision==0.16.0+cu121\r\n[pip3] triton==2.2.0\r\n[conda] Could not collect\n\ncc @zou3519 @albanD"}

{"number": 121711, "owner": "rtqichen", "title": "torch.func.grad hangs on nested differentiation", "create_time": "2024-03-12T05:29:35Z", "update_time": "2024-03-26T13:28:18Z", "body": "### \ud83d\udc1b Describe the bug\n\nI want to compute nested derivatives of the form `d/ddx_1 d/dx_2 d/dx_3 ... f(x_1, x_2, x_3,...)`. This works with autograd.grad but torch.func.grad seems to hang.\r\n\r\nMWE:\r\n\r\n```\r\nimport torch\r\nprint(torch.__version__)\r\nfrom torch.func import grad, jvp\r\n\r\nd = 10\r\nf = torch.prod\r\nx = torch.randn(d).requires_grad_(True)\r\n\r\n# Convert to indexable inputs.\r\ndef indexable_f(*args):\r\n    return f(torch.stack(args))\r\n\r\n# This allows us to take derivatives w.r.t. single elements.\r\ninputs = list(x)\r\nnested_deriv = indexable_f(*inputs)\r\nfor i in range(d):\r\n    nested_deriv = torch.autograd.grad(nested_deriv, inputs[i], create_graph=True)[0]\r\nprint(nested_deriv)  # This works fine.\r\n\r\n# This torch.func.grad version doesn't seem to work and runs forever.\r\nnested_deriv = indexable_f\r\nfor i in range(d):\r\n    nested_deriv = grad(nested_deriv, argnums=i)\r\nnested_deriv(*inputs)  # THIS HANGS. \r\n```\n\n### Versions\n\nThis is tested in colab notebook (https://colab.research.google.com/drive/1tbK-YYFVWMx6t6AulElap-JhInLpt-hT?usp=sharing) with the pytorch version 2.1.0+cu121.\n\ncc @ezyang @gchanan @zou3519 @kadeng @Chillee @samdow @kshitij12345 @janeyx99"}

{"number": 121439, "owner": "BoyuanFeng", "title": "copy_() produces wrong results for boolean tensors on mps", "create_time": "2024-03-07T20:49:53Z", "update_time": "2024-03-20T02:56:56Z", "body": "### \ud83d\udc1b Describe the bug\n\n`copy_()` generates wrong results on mps backend for boolean tensors of certain shapes. A minimal example is:\r\n\r\n```python\r\nimport torch\r\n\r\ndef f(device):\r\n    data = torch.zeros((2,2,1,1,1,1), device=device, dtype=torch.bool)\r\n    ones = torch.ones((2,1,1,1,1,1), device=device, dtype=torch.bool)\r\n    data.narrow(1, 0, 1).copy_(ones)\r\n    print(\"device:\", device, ', out:', data)\r\n\r\nf('cpu')\r\nf('mps')\r\n```\r\n\r\nThe code above produces wrong results on mps backend:\r\n```\r\ndevice: cpu , out: tensor([[[[[[ True]]]],\r\n         [[[[False]]]]],\r\n        [[[[[ True]]]],\r\n         [[[[False]]]]]])\r\ndevice: mps , out: tensor([[[[[[False]]]],\r\n         [[[[False]]]]],\r\n        [[[[[False]]]],\r\n         [[[[False]]]]]], device='mps:0')\r\n```\r\n\n\n### Versions\n\nCollecting environment information...\r\nPyTorch version: 2.3.0a0+git1018ce2\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 14.3 (arm64)\r\nGCC version: Could not collect\r\nClang version: 15.0.0 (clang-1500.1.0.2.5)\r\nCMake version: Could not collect\r\nLibc version: N/A\r\n\r\nPython version: 3.11.6 (v3.11.6:8b6ee5ba3b, Oct  2 2023, 11:18:21) [Clang 13.0.0 (clang-1300.0.29.30)] (64-bit runtime)\r\nPython platform: macOS-14.3-arm64-arm-64bit\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nApple M1 Pro\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.10.0\r\n[pip3] torch==2.3.0a0+gitaa51b3f\r\n[conda] numpy                     1.26.4          py311he598dae_0  \r\n[conda] numpy-base                1.26.4          py311hfbfe69c_0  \r\n[conda] numpydoc                  1.5.0           py311hca03da5_0  \n\ncc @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr"}

{"number": 121228, "owner": "YangChenyuan", "title": "[torch.compile] `merge_unbind_stack` returns a tensor with WRONG shape", "create_time": "2024-03-05T14:47:28Z", "update_time": "2024-04-21T06:10:48Z", "body": "### \ud83d\udc1b Describe the bug\n\nWhen using `torch.compile` and enabling `TORCHINDUCTOR_FREEZING=1`, `merge_unbind_stack` returns a tensor with **WRONG shape**.\r\n\r\n```py\r\nimport torch\r\n\r\nclass Model(torch.nn.Module):\r\n\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.dim = dim\r\n\r\n    def forward(self, x1):\r\n        v1 = torch.unbind(x1, dim=self.dim)\r\n        v2 = v1[1]\r\n        v3 = torch.stack([v1[0], v2], dim=self.dim)\r\n        return v3\r\n\r\nfunc = Model(1).to('cpu')\r\n\r\nx = torch.arange(12).view(3, 4).float()\r\n\r\nwith torch.no_grad():\r\n    print(func(x.clone()))\r\n    # tensor([[0., 1.],\r\n    #     [4., 5.],\r\n    #     [8., 9.]])\r\n\r\n    func1 = torch.compile(func)\r\n    print(func1(x.clone()))\r\n    # tensor([[ 0.,  1.,  2.,  3.],\r\n    #     [ 4.,  5.,  6.,  7.],\r\n    #     [ 8.,  9., 10., 11.]])\r\n```\n\n### Versions\n\n```\r\nPyTorch version: 2.3.0.dev20240301+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.1 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: 11.5.119\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture: x86_64\r\nCPU op-mode(s): 32-bit, 64-bit\r\nAddress sizes: 46 bits physical, 48 bits virtual\r\nByte Order: Little Endian\r\nCPU(s): 24\r\nOn-line CPU(s) list: 0-23\r\nVendor ID: GenuineIntel\r\nModel name: 12th Gen Intel(R) Core(TM) i9-12900K\r\nCPU family: 6\r\nModel: 151\r\nThread(s) per core: 2\r\nCore(s) per socket: 16\r\nSocket(s): 1\r\nStepping: 2\r\nCPU max MHz: 5200.0000\r\nCPU min MHz: 800.0000\r\nBogoMIPS: 6374.40\r\nFlags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities\r\nVirtualization: VT-x\r\nL1d cache: 640 KiB (16 instances)\r\nL1i cache: 768 KiB (16 instances)\r\nL2 cache: 14 MiB (10 instances)\r\nL3 cache: 30 MiB (1 instance)\r\nNUMA node(s): 1\r\nNUMA node0 CPU(s): 0-23\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit: Not affected\r\nVulnerability L1tf: Not affected\r\nVulnerability Mds: Not affected\r\nVulnerability Meltdown: Not affected\r\nVulnerability Mmio stale data: Not affected\r\nVulnerability Retbleed: Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds: Not affected\r\nVulnerability Tsx async abort: Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton==3.0.0+901819d2b6\r\n[pip3] torch==2.3.0.dev20240301+cu118\r\n[pip3] torchaudio==2.2.0.dev20240301+cu118\r\n[pip3] torchvision==0.18.0.dev20240301+cu118\r\n[conda] numpy 1.26.4 pypi_0 pypi\r\n[conda] pytorch-triton 3.0.0+901819d2b6 pypi_0 pypi\r\n[conda] torch 2.3.0.dev20240301+cu118 pypi_0 pypi\r\n[conda] torchaudio 2.2.0.dev20240301+cu118 pypi_0 pypi\r\n[conda] torchvision 0.18.0.dev20240301+cu118 pypi_0 pypi\r\n```\n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire"}

{"number": 121220, "owner": "LouisRouillard", "title": "Misleading `log_abs_det_jacobian` shape for `StackTransform`?", "create_time": "2024-03-05T11:08:24Z", "update_time": "2024-03-11T17:26:59Z", "body": "### \ud83d\udc1b Describe the bug\n\n```python\r\nimport torch\r\nimport torch.distributions.transforms as T\r\n\r\nstack_transform = T.StackTransform(\r\n    [\r\n        T.AffineTransform(loc=0.0, scale=1.0),\r\n        T.AffineTransform(loc=0.0, scale=1.0),\r\n    ],\r\n    dim=-1,\r\n)\r\n\r\nprint(stack_transform.transforms[0].event_dim)  # 0, operates on scalars\r\nprint(stack_transform.event_dim)  # 1, operates on vectors\r\n\r\nbatch_size = 5\r\nevent_size = 2\r\ny = torch.zeros(batch_size, event_size)\r\nx = stack_transform.inv(y)  # shape (5, 2)\r\n\r\nlog_abs_det_jacobian = stack_transform.log_abs_det_jacobian(x, y)\r\n\r\n# This is the results that feels misleading to me:\r\nprint(log_abs_det_jacobian.shape)  # (5, 2)\r\n# But since `stack_transform`'s event dim is 1, shouldn't the 2 event dimensions\r\n# be considered as independent, and the shape of `log_abs_det_jacobian` be (5,)?\r\n\r\n# Afaik, this is not solvable using an `IndependentTransform`\r\nindependent_transform = T.IndependentTransform(\r\n    stack_transform, reinterpreted_batch_ndims=1\r\n)\r\nind_log_abs_det_jacobian = independent_transform.log_abs_det_jacobian(x, y)\r\n\r\n# The `log_abs_det_jacobian` shape is, afaik, correct:\r\nprint(ind_log_abs_det_jacobian.shape)  # (5,)\r\n\r\n# But:\r\nprint(\r\n    independent_transform.event_dim\r\n)  # 2, operates on a tensor, which is not what one would want?\r\n```\n\n### Versions\n\nCollecting environment information...\r\n/home/mind/lrouilla/dragostore/conda_drago5/envs/pyro/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\r\n  return torch._C._cuda_getDeviceCount() > 0\r\nPyTorch version: 2.1.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nClang version: Could not collect\r\nCMake version: version 3.10.2\r\nLibc version: glibc-2.27\r\n\r\nPython version: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.15.0-213-generic-x86_64-with-glibc2.27\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture\u00a0:                          x86_64\r\nMode(s) op\u00e9ratoire(s) des processeurs\u00a0: 32-bit, 64-bit\r\nBoutisme\u00a0:                              Little Endian\r\nProcesseur(s)\u00a0:                         72\r\nListe de processeur(s) en ligne\u00a0:       0-71\r\nThread(s) par c\u0153ur\u00a0:                    2\r\nC\u0153ur(s) par socket\u00a0:                    18\r\nSocket(s)\u00a0:                             2\r\nN\u0153ud(s) NUMA\u00a0:                          2\r\nIdentifiant constructeur\u00a0:              GenuineIntel\r\nFamille de processeur\u00a0:                 6\r\nMod\u00e8le\u00a0:                                85\r\nNom de mod\u00e8le\u00a0:                         Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz\r\nR\u00e9vision\u00a0:                              4\r\nVitesse du processeur en MHz\u00a0:          3000.000\r\nBogoMIPS\u00a0:                              4600.00\r\nVirtualisation\u00a0:                        VT-x\r\nCache L1d\u00a0:                             32K\r\nCache L1i\u00a0:                             32K\r\nCache L2\u00a0:                              1024K\r\nCache L3\u00a0:                              25344K\r\nN\u0153ud NUMA\u00a00 de processeur(s)\u00a0:          0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70\r\nN\u0153ud NUMA\u00a01 de processeur(s)\u00a0:          1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71\r\nDrapaux\u00a0:                               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==6.1.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.2\r\n[pip3] torch==2.1.0\r\n[pip3] torchaudio==2.0.2\r\n[pip3] torchvision==0.15.2\r\n[pip3] triton==2.1.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               11.8.0              h4ba93d1_12    conda-forge\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] mkl                       2023.1.0         h6d00ec8_46342  \r\n[conda] mkl-service               2.4.0           py311h5eee18b_1  \r\n[conda] mkl_fft                   1.3.6           py311ha02d727_1  \r\n[conda] mkl_random                1.2.2           py311ha02d727_1  \r\n[conda] numpy                     1.26.2                   pypi_0    pypi\r\n[conda] pytorch-cuda              11.8                 h7e8668a_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torch                     2.1.0                    pypi_0    pypi\r\n[conda] torchaudio                2.0.2               py311_cu118    pytorch\r\n[conda] torchvision               0.15.2              py311_cu118    pytorch\r\n[conda] triton                    2.1.0                    pypi_0    pypi\r\n\n\ncc @fritzo @neerajprad @alicanb @nikitaved"}

{"number": 119168, "owner": "ceisenach", "title": "Restoring SequentialLR has undocumented side-effects on Optimizer", "create_time": "2024-02-05T06:33:45Z", "update_time": "2024-02-16T04:11:20Z", "body": "### \ud83d\udc1b Describe the bug\n\nWhen saving and restoring optimizer and LRScheduler states, the order in which the state_dicts are restored determines whether or not the restored optimizer behaves correctly.\r\n\r\n\r\nConsider the following example\r\n```\r\nimport torch\r\n\r\nmodel = torch.nn.Linear(10, 10)\r\noptim = torch.optim.SGD(model.parameters(), lr=3e-5)\r\nlr = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=cosine_decay_linear_warmup)\r\nlrs = []\r\n\r\nfor i in range(100):\r\n    optim.step()\r\n    lr.step()\r\n    lrs.append(lr.get_last_lr())\r\n\r\nmodel2 = torch.nn.Linear(10, 10)\r\noptim2 = torch.optim.SGD(model2.parameters(), lr=3e-4)\r\nscheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optim2, T_max=80, eta_min=3e-5)\r\nscheduler1 = torch.optim.lr_scheduler.LinearLR(optim2, start_factor=0.1, end_factor=1, total_iters=20)\r\nlr2 = torch.optim.lr_scheduler.SequentialLR(optim2, schedulers=[scheduler1, scheduler2], milestones=[20])\r\nlrs2 = []\r\nlrs3 = []\r\n\r\nfor i in range(25):\r\n    optim2.step()\r\n    lr2.step()\r\n    lrs2.append(lr2.get_last_lr())\r\n    lrs3.append(lr2.get_last_lr())\r\n\r\ntorch.save(lr2.state_dict(), '/home/ubuntu/save_seq2.pt')\r\ntorch.save(optim2.state_dict(), '/home/ubuntu/save_optim2.pt')\r\n    \r\n# Correct Behavior\r\nmodel2 = torch.nn.Linear(10, 10)\r\noptim2 = torch.optim.SGD(model2.parameters(), lr=3e-4)\r\nscheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optim2, T_max=80, eta_min=3e-5)\r\nscheduler1 = torch.optim.lr_scheduler.LinearLR(optim2, start_factor=0.1, end_factor=1, total_iters=20)\r\nlr2 = torch.optim.lr_scheduler.SequentialLR(optim2, schedulers=[scheduler1, scheduler2], milestones=[20])\r\nlr2.load_state_dict(torch.load('/home/ubuntu/save_seq2.pt'))\r\noptim2.load_state_dict(torch.load('/home/ubuntu/save_optim2.pt'))\r\n\r\nfor i in range(25, 100):\r\n    lr2.step()\r\n    lrs2.append(lr2.get_last_lr())\r\n    \r\n# Incorrect Behavior\r\nmodel2 = torch.nn.Linear(10, 10)\r\noptim2 = torch.optim.SGD(model2.parameters(), lr=3e-4)\r\noptim2.load_state_dict(torch.load('/home/ubuntu/save_optim2.pt'))\r\nscheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optim2, T_max=80, eta_min=3e-5)\r\nscheduler1 = torch.optim.lr_scheduler.LinearLR(optim2, start_factor=0.1, end_factor=1, total_iters=20)\r\nlr2 = torch.optim.lr_scheduler.SequentialLR(optim2, schedulers=[scheduler1, scheduler2], milestones=[20])\r\nlr2.load_state_dict(torch.load('/home/ubuntu/save_seq2.pt'))\r\n\r\nfor i in range(25, 100):\r\n    lr2.step()\r\n    lrs3.append(lr2.get_last_lr())\r\n```\r\n\r\nThe first example (with no restore) produces the following learning rate\r\n![image](https://github.com/pytorch/pytorch/assets/1673186/bcf97286-8343-43f2-afcc-4275989699ca)\r\n\r\n\r\nThe second example where the optimizer is restored **last**, the behavior is also correct\r\n![image](https://github.com/pytorch/pytorch/assets/1673186/b753fb87-1233-4ce0-be21-4e09cb6010db)\r\n\r\nThe third example, where the optimizer is restored **first**, the behavior is incorrect.\r\n![image](https://github.com/pytorch/pytorch/assets/1673186/a3b1bc7b-56ad-49bb-a42a-2f2c2d19318e)\r\n\r\nThis is caused because the SequentialLR has side effects on the optimizer when it is initialized. Other LRSchedulers do not cause the same side-effects (ie order of restoring objects does not matter).\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Versions\n\nPyTorch version: 2.2.0+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.16.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:21:28)  [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1026-aws-x86_64-with-glibc2.10\r\nIs CUDA available: True\r\nCUDA runtime version: 11.7.99\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\nGPU 2: NVIDIA A100-SXM4-40GB\r\nGPU 3: NVIDIA A100-SXM4-40GB\r\nGPU 4: NVIDIA A100-SXM4-40GB\r\nGPU 5: NVIDIA A100-SXM4-40GB\r\nGPU 6: NVIDIA A100-SXM4-40GB\r\nGPU 7: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 515.65.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nCPU(s):                          96\r\nOn-line CPU(s) list:             0-95\r\nThread(s) per core:              2\r\nCore(s) per socket:              24\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           85\r\nModel name:                      Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz\r\nStepping:                        7\r\nCPU MHz:                         3000.000\r\nBogoMIPS:                        6000.00\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       1.5 MiB\r\nL1i cache:                       1.5 MiB\r\nL2 cache:                        48 MiB\r\nL3 cache:                        71.5 MiB\r\nNUMA node0 CPU(s):               0-23,48-71\r\nNUMA node1 CPU(s):               24-47,72-95\r\nVulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:              Mitigation; PTE Inversion\r\nVulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:          Vulnerable\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.1\r\n[pip3] torch==2.2.0+cu118\r\n[pip3] torchaudio==2.2.0+cu118\r\n[pip3] torchvision==0.17.0+cu118\r\n[pip3] triton==2.2.0\r\n[conda] numpy                     1.24.1                   pypi_0    pypi\r\n[conda] torch                     2.2.0+cu118              pypi_0    pypi\r\n[conda] torchaudio                2.2.0+cu118              pypi_0    pypi\r\n[conda] torchvision               0.17.0+cu118             pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\r\n\n\ncc @vincentqb @jbschlosser @albanD @janeyx99 @crcrpar"}

{"number": 119162, "owner": "awaelchli", "title": "torch.compile changes the dtype of key matrix in attention implementation with torch.set_default_dtype", "create_time": "2024-02-05T03:55:34Z", "update_time": "2024-03-22T14:09:46Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nBelow is a minimal repro of an example where `torch.compile` uses different dtypes than eager for the computation, causing a type mismatch error:\r\n\r\n```\r\nExpected query, key, and value to have the same dtype, but got query.dtype: c10::BFloat16 key.dtype: float and value.dtype: float instead.\r\n```\r\n\r\n\r\n```py\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch\r\n\r\n\r\nclass VAE(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.tok_emb = nn.Embedding(128, embedding_dim=128)\r\n        self.cross_attn = MultiheadAttention(128, 8)\r\n\r\n    def forward(self, tokens):\r\n        embed = self.tok_emb(tokens)\r\n        z = torch.randn(embed.shape[0], 4, embed.shape[2], device=embed.device, dtype=embed.dtype)\r\n        logits = self.cross_attn(embed, z, z)\r\n        return logits.mean()\r\n\r\n\r\nclass MultiheadAttention(nn.Module):\r\n    def __init__(self, embed_dim: int, heads: int):\r\n        super().__init__()\r\n\r\n        self.embed_size = embed_dim\r\n        self.num_heads = heads\r\n        self.head_dim = embed_dim // heads\r\n\r\n        assert self.head_dim * heads == embed_dim, \"Embedding size needs to be divisible by heads\"\r\n\r\n        self.q_proj = nn.Linear(self.embed_size, self.embed_size)\r\n        self.k_proj = nn.Linear(self.embed_size, self.embed_size)\r\n        self.v_proj = nn.Linear(self.embed_size, self.embed_size)\r\n\r\n    def forward(self, query, key, value):\r\n        N = query.shape[0]\r\n\r\n        q = query.view(N, -1, self.num_heads, self.head_dim).swapaxes(1, 2)\r\n        k = key.view(N, -1, self.num_heads, self.head_dim).swapaxes(1, 2)\r\n        v = value.view(N, -1, self.num_heads, self.head_dim).swapaxes(1, 2)\r\n\r\n        # Without compile: torch.bfloat16 torch.bfloat16 torch.bfloat16\r\n        # With compile:    torch.bfloat16 torch.float32 torch.float32\r\n        print(q.dtype, k.dtype, v.dtype)\r\n\r\n        attn = F.scaled_dot_product_attention(q, k, v)\r\n        attn = attn.swapaxes(1, 2).reshape(N, -1, self.embed_size)\r\n        return attn\r\n\r\n\r\nmodel = VAE()\r\nmodel = VAE().to(device=\"cuda\", dtype=torch.bfloat16)\r\n\r\n# Without compile, this runs fine\r\nmodel = torch.compile(model)\r\n\r\ninput = torch.randint(0, 128, (512, 128), device=torch.device(\"cuda\"))\r\n\r\ntorch.set_default_dtype(torch.bfloat16)  # < -------- THIS LINE IS PROBLEMATIC?\r\nloss = model(input)\r\nloss.backward()\r\n\r\n\r\n```\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/adrian/repositories/lightning/examples/pytorch/bug_report/bug_report_model.py\", line 62, in <module>\r\n    loss = model(input)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 453, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/adrian/repositories/lightning/examples/pytorch/bug_report/bug_report_model.py\", line 15, in forward\r\n    logits = self.cross_attn(embed, z, z)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/adrian/repositories/lightning/examples/pytorch/bug_report/bug_report_model.py\", line 46, in forward\r\n    print(q.dtype, k.dtype, v.dtype)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 615, in catch_errors\r\n    return callback(frame, cache_entry, hooks, frame_state)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 748, in _convert_frame\r\n    result = inner_convert(frame, cache_entry, hooks, frame_state)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 390, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/contextlib.py\", line 79, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 650, in _compile\r\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 248, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 531, in compile_inner\r\n    out_code = transform_code_object(code, transform)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 155, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 496, in transform\r\n    tracer.run()\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2125, in run\r\n    super().run()\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 787, in run\r\n    and self.step()\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 750, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 469, in wrapper\r\n    return inner_fn(self, inst)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1196, in CALL_FUNCTION\r\n    self.call_function(fn, args, {})\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 651, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/variables/torch.py\", line 614, in call_function\r\n    tensor_variable = wrap_fx_proxy(\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py\", line 1285, in wrap_fx_proxy\r\n    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py\", line 1370, in wrap_fx_proxy_cls\r\n    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1653, in get_fake_value\r\n    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1599, in get_fake_value\r\n    ret_val = wrap_fake_exception(\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1140, in wrap_fake_exception\r\n    return fn()\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1600, in <lambda>\r\n    lambda: run_node(tx.output, node, args, kwargs, nnmodule)\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1720, in run_node\r\n    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1699, in run_node\r\n    return node.target(*args, **kwargs)\r\ntorch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function scaled_dot_product_attention>(*(FakeTensor(..., device='cuda:0', size=(512, 8, 128, 16),\r\n           grad_fn=<AsStridedBackward0>), FakeTensor(..., device='cuda:0', size=(512, 8, 4, 16), dtype=torch.float32), FakeTensor(..., device='cuda:0', size=(512, 8, 4, 16), dtype=torch.float32)), **{}):\r\nExpected query, key, and value to have the same dtype, but got query.dtype: c10::BFloat16 key.dtype: float and value.dtype: float instead.\r\n\r\nfrom user code:\r\n   File \"/home/adrian/repositories/lightning/examples/pytorch/bug_report/bug_report_model.py\", line 48, in torch_dynamo_resume_in_forward_at_46\r\n    attn = F.scaled_dot_product_attention(q, k, v)\r\n\r\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    import torch._dynamo\r\n    torch._dynamo.config.suppress_errors = True\r\n```\r\n\r\nThis is unexpected because the code runs fine without `torch.compile` applied. It looks like `torch.compile` can't handle this line properly:\r\n\r\n```py\r\ntorch.set_default_dtype(torch.bfloat16)\r\n```\r\n\r\n### Versions\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.3.0.dev20240201+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-75-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\nGPU 2: NVIDIA A100-SXM4-40GB\r\nGPU 3: NVIDIA A100-SXM4-40GB\r\nGPU 4: NVIDIA A100-SXM4-40GB\r\nGPU 5: NVIDIA A100-SXM4-40GB\r\nGPU 6: NVIDIA A100-SXM4-40GB\r\nGPU 7: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 525.125.06\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   43 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          256\r\nOn-line CPU(s) list:             0-254\r\nOff-line CPU(s) list:            255\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 7742 64-Core Processor\r\nCPU family:                      23\r\nModel:                           49\r\nThread(s) per core:              2\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nStepping:                        0\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     2250.0000\r\nCPU min MHz:                     0.0000\r\nBogoMIPS:                        4499.83\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB (128 instances)\r\nL1i cache:                       4 MiB (128 instances)\r\nL2 cache:                        64 MiB (128 instances)\r\nL3 cache:                        512 MiB (32 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-63,128-191\r\nNUMA node1 CPU(s):               64-127,192-254\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.4.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.24.2\r\n[pip3] onnx==1.12.0\r\n[pip3] onnxruntime==1.14.1\r\n[pip3] pytorch-lightning==2.0.6\r\n[pip3] pytorch-triton==3.0.0+901819d2b6\r\n[pip3] torch==2.3.0.dev20240201+cu121\r\n[pip3] torch-tb-profiler==0.4.3\r\n[pip3] torchmetrics==1.3.0.post0\r\n[pip3] torchvision==0.17.0\r\n[pip3] triton==2.2.0\r\n[conda] numpy                     1.24.2                   pypi_0    pypi\r\n[conda] pytorch-lightning         2.0.6                    pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi\r\n[conda] torch                     2.3.0.dev20240201+cu121          pypi_0    pypi\r\n[conda] torch-tb-profiler         0.4.3                    pypi_0    pypi\r\n[conda] torchmetrics              1.3.0.post0              pypi_0    pypi\r\n[conda] torchvision               0.17.0                   pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang"}

{"number": 119152, "owner": "ar0ck", "title": "Dynamo fails on scalar bit shifts when `dynamic=True`", "create_time": "2024-02-04T22:54:59Z", "update_time": "2024-03-21T03:11:22Z", "body": "### \ud83d\udc1b Describe the bug\n\n```python\r\nimport torch\r\n\r\n@torch.compile(dynamic=True, backend='eager')\r\ndef f(x):\r\n    return 1 << x\r\n\r\nf(2)\r\n```\r\n\r\nThe operations `>>` and  `<<` fail, while `{+, -, *, /}` are OK.\n\n### Error logs\n\n```python\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 7, in <module>\r\n    f(2)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 453, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 615, in catch_errors\r\n    return callback(frame, cache_entry, hooks, frame_state)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 748, in _convert_frame\r\n    result = inner_convert(frame, cache_entry, hooks, frame_state)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 390, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\r\n    return func(*args, **kwds)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 650, in _compile\r\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 248, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 531, in compile_inner\r\n    out_code = transform_code_object(code, transform)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 155, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 496, in transform\r\n    tracer.run()\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2126, in run\r\n    super().run()\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 787, in run\r\n    and self.step()\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 750, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 241, in impl\r\n    self.push(fn_var.call_function(self, self.popn(nargs), {}))\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py\", line 674, in call_function\r\n    res = binop_handler(tx, args[0], args[1], {})\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py\", line 322, in dynamic_handler\r\n    return wrap_fx_proxy(\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/variables/builder.py\", line 1291, in wrap_fx_proxy\r\n    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/variables/builder.py\", line 1376, in wrap_fx_proxy_cls\r\n    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1653, in get_fake_value\r\n    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1599, in get_fake_value\r\n    ret_val = wrap_fake_exception(\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1140, in wrap_fake_exception\r\n    return fn()\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1600, in <lambda>\r\n    lambda: run_node(tx.output, node, args, kwargs, nnmodule)\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1720, in run_node\r\n    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e\r\n  File \".../lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1699, in run_node\r\n    return node.target(*args, **kwargs)\r\n  File \".../lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 1242, in rbinary_magic_impl\r\n    ret = wrap_node(getattr(other_node, method_attr)(self.node))\r\n  File \".../lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 255, in lshift\r\n    return self._lshift(other)  # type: ignore[attr-defined]\r\n  File \".../lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 926, in binary_magic_impl\r\n    out = func(self.expr, other.expr)\r\n  File \".../lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 571, in _sympy_lshift\r\n    return LShift(a, b)\r\n  File \".../lib/python3.10/site-packages/sympy/core/cache.py\", line 70, in wrapper\r\n    retval = cfunc(*args, **kwargs)\r\n  File \".../lib/python3.10/site-packages/sympy/core/function.py\", line 469, in __new__\r\n    result = super().__new__(cls, *args, **options)\r\n  File \".../lib/python3.10/site-packages/sympy/core/cache.py\", line 70, in wrapper\r\n    retval = cfunc(*args, **kwargs)\r\n  File \".../lib/python3.10/site-packages/sympy/core/function.py\", line 309, in __new__\r\n    evaluated = cls.eval(*args)\r\n  File \".../lib/python3.10/site-packages/torch/utils/_sympy/functions.py\", line 249, in eval\r\n    if shift < 0:\r\n  File \".../lib/python3.10/site-packages/sympy/core/relational.py\", line 511, in __bool__\r\n    raise TypeError(\"cannot determine truth value of Relational\")\r\ntorch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function lshift>(*(1, s0), **{}):\r\ncannot determine truth value of Relational\r\n\r\nfrom user code:\r\n   File \"bug.py\", line 5, in f\r\n    return 1 << x\r\n\r\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    import torch._dynamo\r\n    torch._dynamo.config.suppress_errors = True\r\n```\n\n### Minified repro\n\n_No response_\n\n### Versions\n\n```\r\nCollecting environment information...\r\nPyTorch version: 2.3.0.dev20240204+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.16.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.13 (main, Aug 25 2023, 13:20:03) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-170-generic-x86_64-with-glibc2.31\r\nIs CUDA available: False\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090\r\nNvidia driver version: 515.43.04\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              106\r\nModel name:                         Intel(R) Xeon(R) Silver 4314 CPU @ 2.40GHz\r\nStepping:                           6\r\nFrequency boost:                    enabled\r\nCPU MHz:                            885.906\r\nCPU max MHz:                        3400.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4800.00\r\nVirtualization:                     VT-x\r\nL1d cache:                          768 KiB\r\nL1i cache:                          512 KiB\r\nL2 cache:                           20 MiB\r\nL3 cache:                           24 MiB\r\nNUMA node0 CPU(s):                  0-31\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.3\r\n[pip3] torch==2.3.0.dev20240204+cpu\r\n[conda] Could not collect\r\n```\n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519"}

{"number": 118741, "owner": "Gwihwan-Go", "title": "Inconsistent Behavior of `torch.dsplit` with torch.compile", "create_time": "2024-01-31T11:15:12Z", "update_time": "2024-02-01T04:14:53Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nWhen running in torch.compile mode, the following error is encountered:\r\n\r\n```\r\n**{'indices': [1, 1, 1, 1], 'input': FakeTensor(..., size=(1, 1))}):\r\ntorch.dsplit requires a tensor with at least 3 dimensions, but got a tensor with 2 dimensions!**\r\n```\r\nThis error occurs despite providing a 6-dimensional tensor as input to torch.dsplit. Furthermore, **disabling the line** _ = torch.special.logit(eps=0.2518168952606683, input=inp2, out=inp1) in the model results in the error not occurring.\r\n\r\n### Expected Behavior\r\ntorch.dsplit should process the 6-dimensional tensor without error in both eager and torch.compile modes, and the behavior should not be affected by the presence of a torch.special.logit call.\r\n\r\n### Error logs\r\n```\r\ntorch version:  2.3.0.dev20240130+cu118\r\n==== Eager mode ====\r\ninp1 size : torch.Size([1, 1, 1, 1, 1, 1])\r\nprog.py:13: UserWarning: An output with one or more elements was resized since it had shape [1, 1, 1, 1, 1, 1], which does not match the required output shape [1, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\r\n  _ = torch.special.logit(eps=0.2518168952606683, input=inp2, out=inp1)\r\n==== TorchComp mode ====\r\ninp1 size : torch.Size([1, 1])\r\nTraceback (most recent call last):\r\n  File \"prog.py\", line 21, in <module>\r\n    ret_exported = torch.compile(model)(*inputs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py\", line 452, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"prog.py\", line 11, in forward\r\n    print(\"inp1 size :\", inp1.size())\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py\", line 614, in catch_errors\r\n    return callback(frame, cache_entry, hooks, frame_state)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 748, in _convert_frame\r\n    result = inner_convert(frame, cache_entry, hooks, frame_state)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 390, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/opt/conda/envs/std/lib/python3.8/contextlib.py\", line 75, in inner\r\n    return func(*args, **kwds)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 650, in _compile\r\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 248, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 531, in compile_inner\r\n    out_code = transform_code_object(code, transform)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 155, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 496, in transform\r\n    tracer.run()\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 2125, in run\r\n    super().run()\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 787, in run\r\n    and self.step()\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 750, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 469, in wrapper\r\n    return inner_fn(self, inst)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1249, in CALL_FUNCTION_KW\r\n    self.call_function(fn, args, kwargs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 651, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/variables/torch.py\", line 614, in call_function\r\n    tensor_variable = wrap_fx_proxy(\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/variables/builder.py\", line 1285, in wrap_fx_proxy\r\n    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/variables/builder.py\", line 1370, in wrap_fx_proxy_cls\r\n    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1653, in get_fake_value\r\n    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1599, in get_fake_value\r\n    ret_val = wrap_fake_exception(\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1140, in wrap_fake_exception\r\n    return fn()\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1600, in <lambda>\r\n    lambda: run_node(tx.output, node, args, kwargs, nnmodule)\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1720, in run_node\r\n    raise RuntimeError(fn_str + str(e)).with_traceback(e.__traceback__) from e\r\n  File \"/opt/conda/envs/std/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1699, in run_node\r\n    return node.target(*args, **kwargs)\r\ntorch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method dsplit of type object at 0x7fe66c46efa0>(*(), **{'indices': [1, 1, 1, 1], 'input': FakeTensor(..., size=(1, 1))}):\r\ntorch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!\r\n\r\nfrom user code:\r\n   File \"prog.py\", line 12, in torch_dynamo_resume_in_forward_at_11\r\n    v9_0, _, _, _, _ = torch.dsplit(indices=[1, 1, 1, 1], input=inp1)\r\n\r\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    import torch._dynamo\r\n    torch._dynamo.config.suppress_errors = True\r\n```\r\n### Minified repro\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nprint(\"torch version: \",torch.__version__)\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n    def forward(self, inp1, inp2):\r\n        print(\"inp1 size :\", inp1.size())\r\n        v9_0, _, _, _, _ = torch.dsplit(indices=[1, 1, 1, 1], input=inp1)\r\n        _ = torch.special.logit(eps=0.2518168952606683, input=inp2, out=inp1)\r\n        return v9_0, \r\ninputs = [ torch.randn(1, 1, 1, 1, 1, 1), torch.randn(1, 1) ]\r\nmodel = Model().to(torch.device(\"cpu\"))\r\nprint('==== Eager mode ====')\r\nret_eager = model(*inputs)\r\n\r\nprint('==== TorchComp mode ====')\r\nret_exported = torch.compile(model)(*inputs)\r\nprint('OK!')\r\n```\r\n\r\n### Versions\r\n\r\nPyTorch version: 2.3.0.dev20240130+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.8.17 (default, Jul  5 2023, 21:04:15)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.2.0-39-generic-x86_64-with-glibc2.17\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             256\r\nOn-line CPU(s) list:                0-255\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7763 64-Core Processor\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 64\r\nSocket(s):                          2\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        3529.0520\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           4890.70\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                     AMD-V\r\nL1d cache:                          4 MiB (128 instances)\r\nL1i cache:                          4 MiB (128 instances)\r\nL2 cache:                           64 MiB (128 instances)\r\nL3 cache:                           512 MiB (16 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-63,128-191\r\nNUMA node1 CPU(s):                  64-127,192-255\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.23.5\r\n[pip3] pytorch-triton==3.0.0+901819d2b6\r\n[pip3] torch==2.3.0.dev20240130+cu118\r\n[pip3] torchaudio==2.2.0.dev20240130+cu118\r\n[pip3] torchvision==0.18.0.dev20240130+cu118\r\n[conda] numpy                     1.23.5                   pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+901819d2b6          pypi_0    pypi\r\n[conda] torch                     2.3.0.dev20240130+cu118          pypi_0    pypi\r\n[conda] torchaudio                2.2.0.dev20240130+cu118          pypi_0    pypi\r\n[conda] torchvision               0.18.0.dev20240130+cu118          pypi_0    pypi\n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519"}

{"number": 118739, "owner": "Cztery", "title": "Getting \"RuntimeError: Trying to backward through the graph a second time\" when calling backward on compiled unbind / split ops", "create_time": "2024-01-31T10:18:32Z", "update_time": "2024-03-22T14:09:41Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nSince Pytorch 2.2 (until now - current PT nightly) trying to run backward() on compiled unbind or split ops, a RuntimeError is thrown. As suggested in the error message, calling backward() with retain_graph=True prevents the error from occuring. However previous PT versions (2.1) did not require that to work. \r\nIs this an expected bahaviour or a new bug that a workaround with retain_graph=True hides?\r\n\r\n### Error logs\r\n\r\n\u279c  ~ python3 unbind_repro.py\r\nTraceback (most recent call last):\r\n  File \"/home/ubu/unbind_repro.py\", line 14, in <module>\r\n    i.backward() #retain_graph=True)\r\n    ^^^^^^^^^^^^\r\n  File \"/home/ubu/.local/lib/python3.11/site-packages/torch/_tensor.py\", line 524, in backward\r\n    torch.autograd.backward(\r\n  File \"/home/ubu/.local/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 267, in backward\r\n    _engine_run_backward(\r\n  File \"/home/ubu/.local/lib/python3.11/site-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\r\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubu/.local/lib/python3.11/site-packages/torch/autograd/function.py\", line 294, in apply\r\n    return user_fn(self, *args)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubu/.local/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 620, in backward\r\n    *ctx.saved_tensors,\r\n     ^^^^^^^^^^^^^^^^^\r\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\r\n\r\n### Minified repro\r\n```\r\nimport torch\r\n\r\ninput = torch.Tensor([1, 2, 3, 4, 8])\r\ninput.requires_grad = True\r\n\r\ndef foo(input):\r\n  outputs = torch.unbind(input, 0)\r\n  return outputs\r\n\r\nfooCompiled = torch.compile(foo)\r\nout = fooCompiled(input)\r\n\r\nfor i in out:\r\n  i.backward() #retain_graph=True)\r\n\r\nprint(out)\r\n```\r\n### Versions\r\n\r\nPyTorch version: 2.3.0.dev20240129+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 23.04 (x86_64)\r\nGCC version: (Ubuntu 12.3.0-1ubuntu1~23.04) 12.3.0\r\nClang version: 15.0.7\r\nCMake version: version 3.25.1\r\nLibc version: glibc-2.37\r\n\r\nPython version: 3.11.4 (main, Dec  7 2023, 15:43:41) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.37\r\nIs CUDA available: False\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      39 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             8\r\nOn-line CPU(s) list:                0-7\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Core(TM) i7-8665U CPU @ 1.90GHz\r\nCPU family:                         6\r\nModel:                              142\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 4\r\nSocket(s):                          1\r\nStepping:                           12\r\nBogoMIPS:                           4224.01\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 xsaves flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          128 KiB (4 instances)\r\nL1i cache:                          128 KiB (4 instances)\r\nL2 cache:                           1 MiB (4 instances)\r\nL3 cache:                           8 MiB (1 instance)\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Unknown: Dependent on hypervisor status\r\nVulnerability Tsx async abort:      Mitigation; TSX disabled\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.2\r\n[pip3] torch==2.3.0.dev20240129+cpu\r\n[pip3] torchaudio==2.2.0.dev20240129+cpu\r\n[pip3] torchvision==0.18.0.dev20240129+cpu\r\n[conda] Could not collect\r\n\r\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @gchanan @kadeng"}

{"number": 118642, "owner": "zoux1a", "title": "Different outputs of `div` and `threshold` with a specific input", "create_time": "2024-01-30T16:42:09Z", "update_time": "2024-06-07T02:22:27Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nwith a specific input in [inputs.pkl.zip](https://github.com/pytorch/pytorch/files/14101497/inputs.pkl.zip), `div` and `threshold` meet a inconsistency between eagermode and torch.compile.\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport os, pickle\r\n\r\nwith open('inputs.pkl', 'rb') as f:\r\n    inputs = pickle.load(f) \r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n    def forward(self, out):\r\n        res = torch.div(out=out, input=inputs[0]['input'],other=inputs[0]['other'],rounding_mode='trunc')        \r\n        res = torch.nn.functional.threshold(input=res, inplace=False,threshold=-1.9,value=5.2)        \r\n        return res\r\ninf = float('inf')\r\nnan = float('nan')\r\nis_valid = True\r\ninput1 = torch.randint(-2147483648, 2147483647, [4, 5, 8, 3], dtype=torch.int32)\r\ninput2 = input1.clone()\r\n\r\nmodel = Model().to(torch.device('cpu'))\r\neag = model(input1)\r\nopt = torch.compile(model.forward)(input2)\r\n\r\nsame_val = torch.allclose(eag.to('cpu'), \r\n                            opt.to('cpu'), \r\n                            rtol=1e-3, atol=1e-3, \r\n                            equal_nan=True)\r\nif same_val == False : \r\n        raise ValueError('diff value')\r\n```\r\ntrace\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/guihuan/LLM/exp_data/torch-2/2024-01-27-10-09/21/repro.py\", line 30, in <module>\r\n    raise ValueError('diff value')\r\nValueError: diff value\r\n```\r\n\r\n### Versions\r\n\r\n20240130 nightly\n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @SherlockNoMad @zou3519"}

{"number": 117510, "owner": "weifengpy", "title": "[FSDP] unexpected reshard in backward because of unused gradable input in frozon modules", "create_time": "2024-01-16T01:55:30Z", "update_time": "2024-01-16T14:28:20Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\n# problem\r\nwhen frozon module have unused gradable input, reshard happens without unshard, leading to runtime assertion error \"Expects storage to be allocated\"\r\n* unshard won't happen because ``output.requires_grad = False``` and we dont register pre-backward hooks in ``_register_post_backward_hook``\r\n* reshard happen because unused input have ``.requires_grad = True`` and we register post-backward hooks in ``_register_post_backward_reshard_only_hook``\r\n\r\n# repro\r\n```\r\n# torchrun --standalone --nproc_per_node=2 run_frozon.py\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nimport torch.nn as nn\r\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\r\nfrom torch.distributed.fsdp.wrap import ModuleWrapPolicy\r\n\r\nclass FrozonLinear(nn.Linear):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        for param in self.parameters():\r\n            param.requires_grad = False\r\n\r\n    def forward(self, frozon_input, learnable_input):\r\n        return super().forward(frozon_input)\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer1 = FrozonLinear(4, 4)\r\n        self.layer2 = nn.Linear(4, 4)\r\n\r\n    def forward(self, frozon_input, learnable_input):\r\n        z = self.layer1(frozon_input, learnable_input)\r\n        z = self.layer2(z)\r\n        return z + learnable_input.sum()\r\n\r\n\r\ndef main():\r\n    dist.init_process_group(backend=\"nccl\")\r\n    gpu_id = int(os.environ[\"LOCAL_RANK\"])\r\n    device = f\"cuda:{gpu_id}\"\r\n    torch.cuda.set_device(device)\r\n    torch.manual_seed(0)\r\n    model = Model()\r\n    fsdp_model = FSDP(\r\n        model,\r\n        device_id=torch.cuda.current_device(),\r\n        auto_wrap_policy=ModuleWrapPolicy({FrozonLinear, nn.Linear}),\r\n        use_orig_params=True,\r\n    )\r\n    if torch.distributed.get_rank() == 0:\r\n        print(fsdp_model)\r\n    torch.manual_seed(dist.get_rank() + 1)\r\n    batch_size = 3\r\n    frozon_input = torch.randn((batch_size, 4), device=\"cuda\")\r\n    learnable_input = torch.randn((batch_size, 4), device=\"cuda\", requires_grad=True)\r\n    loss = fsdp_model(frozon_input, learnable_input).sum()\r\n    loss.backward()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n```\r\n File \"/data/users/weif/pytorch/torch/autograd/graph.py\", line 439, in inner_hook\r\n    fn(buffer[id])\r\n  File \"/data/users/weif/pytorch/torch/distributed/fsdp/_runtime_utils.py\", line 813, in _post_backward_reshard\r\n    _reshard(state, handle, free_unsharded_flat_param)\r\n  File \"/data/users/weif/pytorch/torch/distributed/fsdp/_runtime_utils.py\", line 346, in _reshard\r\n    handle.reshard(free_unsharded_flat_param)\r\n  File \"/data/users/weif/pytorch/torch/distributed/fsdp/_flat_param.py\", line 1707, in reshard\r\n    self._free_unsharded_flat_param()\r\n  File \"/data/users/weif/pytorch/torch/distributed/fsdp/_flat_param.py\", line 1740, in _free_unsharded_flat_param\r\n    self._check_storage_allocated(unsharded_flat_param)\r\n  File \"/data/users/weif/pytorch/torch/distributed/fsdp/_flat_param.py\", line 2555, in _check_storage_allocated\r\n    _p_assert(_storage_size_allocated(tensor), \"Expects storage to be allocated\")\r\n  File \"/data/users/weif/pytorch/torch/distributed/utils.py\", line 146, in _p_assert\r\n    raise AssertionError(s)\r\nAssertionError: Expects storage to be allocated\r\n```\r\n\r\n### Versions\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.3.0a0+git7ba3bd5\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.0\r\nROCM used to build PyTorch: N/A\r\nOS: CentOS Stream 9 (x86_64)\r\nGCC version: (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2)\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.34\r\nPython version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.12.0-0_fbk16_zion_7661_geb00762ce6d2-x86_64-with-glibc2.34\r\nIs CUDA available: True\r\nCUDA runtime version: 12.0.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA PG509-210\r\nGPU 1: NVIDIA PG509-210\r\nGPU 2: NVIDIA PG509-210\r\nGPU 3: NVIDIA PG509-210\r\nGPU 4: NVIDIA PG509-210\r\nGPU 5: NVIDIA PG509-210\r\nGPU 6: NVIDIA PG509-210\r\nGPU 7: NVIDIA PG509-210\r\nNvidia driver version: 525.105.17\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.8.0\r\n/usr/lib64/libcudnn_adv_infer.so.8.8.0\r\n/usr/lib64/libcudnn_adv_train.so.8.8.0\r\n/usr/lib64/libcudnn_cnn_infer.so.8.8.0\r\n/usr/lib64/libcudnn_cnn_train.so.8.8.0\r\n/usr/lib64/libcudnn_ops_infer.so.8.8.0\r\n/usr/lib64/libcudnn_ops_train.so.8.8.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          192\r\nOn-line CPU(s) list:             0-191\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz\r\nCPU family:                      6\r\nModel:                           85\r\nThread(s) per core:              2\r\nCore(s) per socket:              24\r\nSocket(s):                       4\r\nStepping:                        11\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     1801.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        3600.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (96 instances)\r\nL1i cache:                       3 MiB (96 instances)\r\nL2 cache:                        96 MiB (96 instances)\r\nL3 cache:                        132 MiB (4 instances)\r\nNUMA node(s):                    4\r\nNUMA node0 CPU(s):               0-23,96-119\r\nNUMA node1 CPU(s):               24-47,120-143\r\nNUMA node2 CPU(s):               48-71,144-167\r\nNUMA node3 CPU(s):               72-95,168-191\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nVersions of relevant libraries:\r\n[pip3] flake8==6.0.0\r\n[pip3] flake8-bugbear==23.3.23\r\n[pip3] flake8-comprehensions==3.12.0\r\n[pip3] flake8-executable==2.1.3\r\n[pip3] flake8-logging-format==0.9.0\r\n[pip3] flake8-pyi==23.3.1\r\n[pip3] flake8-simplify==0.19.3\r\n[pip3] mypy==1.6.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.0\r\n[pip3] optree==0.10.0\r\n[pip3] pytorch-lightning==2.0.8\r\n[pip3] pytorch-triton==2.1.0+6e4932cda8\r\n[pip3] torch==2.3.0a0+git7e566a7\r\n[pip3] torchmetrics==1.1.2\r\n[pip3] torchvision==0.18.0a0+ffc58ef\r\n[conda] blas                      1.0                         mkl  \r\n[conda] magma-cuda116             2.6.1                         1    pytorch\r\n[conda] mkl                       2023.1.0         h213fc3f_46343  \r\n[conda] mkl-include               2023.1.0         h06a4308_46343  \r\n[conda] mkl-service               2.4.0           py310h5eee18b_1  \r\n[conda] mkl_fft                   1.3.6           py310h1128e8f_1  \r\n[conda] mkl_random                1.2.2           py310h1128e8f_1  \r\n[conda] numpy                     1.26.0                   pypi_0    pypi\r\n[conda] optree                    0.10.0                   pypi_0    pypi\r\n[conda] pytorch-lightning         2.0.8                    pypi_0    pypi\r\n[conda] pytorch-triton            2.1.0+6e4932cda8          pypi_0    pypi\r\n[conda] torch                     2.3.0a0+git7e566a7           dev_0    <develop>\r\n[conda] torchfix                  0.1.1                    pypi_0    pypi\r\n[conda] torchmetrics              1.1.2                    pypi_0    pypi\r\n[conda] torchvision               0.18.0a0+ffc58ef           dev_0    <develop>\r\n\n\ncc @zhaojuanmao @mrshenli @rohan-varma @awgu @fegin @penguinwu @kwen2501"}

{"number": 116300, "owner": "yf225", "title": "DDPOptimizer lazy compile causes shape mismatch error", "create_time": "2023-12-21T22:14:33Z", "update_time": "2024-03-12T17:53:49Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nAfter https://github.com/pytorch/pytorch/pull/116292 is landed, running unit test `test_graph_split_inductor_transpose` with `torch._dynamo.config.optimize_ddp_lazy_compile=True` gives this error:\r\n\r\n```\r\n  File \"pytorch/test/distributed/test_dynamo_distributed.py\", line 736, in test_graph_split_inductor_transpose\r\n    self.assertTrue(same(mod(x_2), ddp_compiled_mod(x_2)))\r\n                                   ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\r\n    else self._run_ddp_forward(*inputs, **kwargs)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\r\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 434, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pytorch/test/distributed/test_dynamo_distributed.py\", line 719, in forward\r\n    def forward(self, x):\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 434, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 737, in call_wrapped\r\n    return self._wrapped_call(self, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 317, in __call__\r\n    raise e\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 304, in __call__\r\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<eval_with_key>.74\", line 6, in forward\r\n    submod_0 = self.compiled_submod_0(l_x_);  l_x_ = None\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/backends/distributed.py\", line 344, in forward\r\n    x = self.submod(*args)\r\n        ^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 434, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py\", line 901, in forward\r\n    return compiled_fn(full_args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 81, in g\r\n    return f(*args)\r\n           ^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 83, in runtime_wrapper\r\n    all_outs = call_func_at_runtime_with_args(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 105, in call_func_at_runtime_with_args\r\n    out = normalize_as_list(f(args))\r\n                            ^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 81, in g\r\n    return f(*args)\r\n           ^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/autograd/function.py\", line 553, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 408, in forward\r\n    fw_outs = call_func_at_runtime_with_args(\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 105, in call_func_at_runtime_with_args\r\n    out = normalize_as_list(f(args))\r\n                            ^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 863, in __call__\r\n    return self.get_current_callable()(inputs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_inductor/compile_fx.py\", line 608, in run\r\n    return model(new_inputs)\r\n           ^^^^^^^^^^^^^^^^^\r\n  File \"/data/users/willfeng/miniconda3/lib/python3.11/site-packages/torch/_inductor/codecache.py\", line 891, in _run_from_cache\r\n    return compiled_graph.compiled_artifact(inputs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/tmp/torchinductor_willfeng/ph/cphrafcnfqdnbchgy2koxsawdvv6mpopghjmn2p5lgdrxy2yttqb.py\", line 110, in call\r\n    assert_size_stride(primals_3, (200, 30, 50), (1500, 50, 1))\r\nAssertionError: expected size 300==200, stride 1500==1500 at dim=0\r\n```\r\nHowever, we do what to turn on `optimize_ddp_lazy_compile` by default as soon as possible, to fix stride mismatch errors in other cases (see motivation: https://github.com/pytorch/pytorch/pull/114154). So we need to find the fix for this shape mismatch as soon as possible.\r\n\r\n### Versions\r\n\r\nPyTorch nightly\n\ncc @ezyang @gchanan @zou3519 @kadeng @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov"}

{"number": 116264, "owner": "guilhermeleobas", "title": "Dynamo fails to track dataclass", "create_time": "2023-12-21T13:59:37Z", "update_time": "2024-07-09T14:22:12Z", "body": "### \ud83d\udc1b Describe the bug\n\nShort reproducer:\r\n\r\n```python\r\nimport torch\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass C:\r\n    x: int\r\n\r\n\r\n@torch.compile(backend=\"aot_eager\", fullgraph=True)\r\ndef f():\r\n    l = C(3)\r\n    r = C(4)\r\n    return l == r\r\n\r\nf()\r\n```\r\n\r\nReproducer does not represent any real program, but the error happens with any program that reaches the `__init__` call on `UserDefinedVariable::call_function`:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f170d6665ca7dea844d20b06bd4dcb82d28f2879/torch/_dynamo/variables/user_defined.py#L227-L229\r\n\r\n`var` is first instantiated with `object.__new__` and the subsequent code fails to invoke the `__init__` method. As a result, `var` does not get populated with any of the dataclass fields.\n\n### Versions\n\nHappens on PyTorch main branch\n\ncc @ezyang @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @msaroufim @wconstab @bdhirsh @zou3519 @aakhundov"}

{"number": 116013, "owner": "youkaichao", "title": "[Dynamo] bytecode transformed by Dynamo is not serializable by marshal", "create_time": "2023-12-18T11:13:13Z", "update_time": "2024-06-18T17:19:21Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nBy default, Python code object uses [`marshal`](https://docs.python.org/3/library/marshal.html) for serialization. However, I find that bytecode transformed by Dynamo sometimes is not serializable by `marshal`.\r\n\r\nThe reason is that Dynamo stores a class variable inside `co_consts`.\r\n\r\nFinally, I managed to use [`dill`](https://pypi.org/project/dill/) to save the code object. For those who wants to see the live object, please download it [here](https://cloud.tsinghua.edu.cn/f/7f9dc5f9c27f4076bd56/?dl=1), and use the code below to load it (`pip install transformers` required):\r\n\r\n```python\r\nimport dill\r\ncode = dill.load(open(\"__transformed_code_0_for_resume_in_forward.py.transformed_bytecode\", \"rb\"))\r\n```\r\n\r\nThe object in question is `code.co_consts[7]`, which is a `transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput` class.\r\n\r\nThe class variable also causes problems for debuggers. When I view the code object in debuggers (I'm using VS Code), these variables are invisible. The following screenshot shows the problem: the object in index `7` is not shown.\r\n\r\n<img width=\"760\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/23236638/1fa32425-dff5-469f-b6c1-5e5b3429fd11\">\r\n\r\nIn summary, while storing class variables in `co_consts` works for Python VM execution, it might not be compatible with the Python ecosystem.\r\n\r\n### Versions\r\n\r\nI'm using PyTorch version `2.2.0.dev20231213`.\n\ncc @ezyang @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @msaroufim @wconstab @bdhirsh @zou3519 @aakhundov"}

{"number": 115581, "owner": "q10", "title": "Unable to build FBGEMM_GPU against PyTorch-ROCm: roc::hipblas not found", "create_time": "2023-12-11T20:26:44Z", "update_time": "2023-12-12T08:19:30Z", "body": "### \ud83d\udc1b Describe the bug\n\n## Description\r\nSince 11/17, FBGEMM_GPU OSS builds for PyTorch nightly ROCm variant have been consistently failing with the following error signature:\r\n\r\n```\r\nCMake Error at /github/home/miniconda/envs/build_binary/lib/python3.8/site-packages/torch/share/cmake/Caffe2/Caffe2Targets.cmake:101 (set_target_properties):\r\nThe link interface of target \"torch_hip_library\" contains:\r\n    roc::hipblas\r\nbut the target was not found.  Possible reasons include:\r\n```\r\n\r\nThere have been no recent changes in the build steps, and the only difference we found was in the installed packages, namely the PyTorch nightly version that was used (2.2.0.dev20231116+rocm5.7 vs 2.2.0.dev20231117+rocm5.7)\r\n\r\nAs such, we suspect the build failure in OSS FBGEMM_GPU-ROCm may be related to some upstream change in PyTorch.\r\n\r\nWhat were the changes that landed between the 11/16 and 11/17 PyTorch nightlies that may have affected the ROCm variant?\r\n\r\n## Repro\r\n\r\n### Script\r\nThe full FBGEMM_GPU OSS build instructions can be found [here](https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/docs/BuildInstructions.md), and the script that follows these instructions can be found [here](https://github.com/pytorch/FBGEMM/blob/main/.github/workflows/fbgemm_gpu_ci.yml#L31).\r\n\r\nIn short, simply run this command from the FBGEMM repo, `fbgemm_gpu` subdirectory, in a Conda environment with PyTorch ROCm nightly installed:\r\n\r\n```sh\r\npython setup.py build develop --package_variant=rocm -DTORCH_USE_HIP_DSA=1\r\n```\r\n\r\n### Env\r\nThe OSS builds take place in a GitHub CI instance but can be reproduced with on a local machine running ubuntu:20.0 Docker instance, provided that the instance contains all the toolchain installed per the build instructions.\r\n\r\n## Expected Outcome\r\nThe FBGEMM_GPU ROCm builds should be passing.  See example build log (11/16): https://github.com/pytorch/FBGEMM/actions/runs/6897226517/job/18764954938\r\n\r\n## Actual Outcome\r\nThe FBGEMM_GPU ROCm are currently failing at the CMake stage, where it complains it cannot find the roc::hipblas target.  See example build log  (11/17):\r\nhttps://github.com/pytorch/FBGEMM/actions/runs/6909935860/job/18802213192\n\n### Versions\n\n```\r\n[EXEC] [ATTEMPT 0/3]    + conda run -n build_binary python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 2.2.0.dev20231[21](https://github.com/pytorch/FBGEMM/actions/runs/7172899604/job/19531073976?pr=2203#step:12:22)1+rocm5.7\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 5.7.31921-d1770ee1b\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.13 (main, Sep 11 20[23](https://github.com/pytorch/FBGEMM/actions/runs/7172899604/job/19531073976?pr=2203#step:12:24), 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI100 (gfx908:sramecc+:xnack-)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 5.7.31921\r\nMIOpen runtime version: 2.20.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         [25](https://github.com/pytorch/FBGEMM/actions/runs/7172899604/job/19531073976?pr=2203#step:12:26)\r\nModel:                              1\r\nModel name:                         AMD EPYC 7543 32-Core Processor\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU MHz:                            1500.000\r\nCPU max MHz:                        3737.8899\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           5600.11\r\nVirtualization:                     AMD-V\r\nL1d cache:                          2 MiB\r\nL1i cache:                          2 MiB\r\nL2 cache:                           32 MiB\r\nL3 cache:                           512 MiB\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.[26](https://github.com/pytorch/FBGEMM/actions/runs/7172899604/job/19531073976?pr=2203#step:12:27).2\r\n[pip3] pytorch-triton-rocm==2.1.0+dafe145982\r\n[pip3] torch==2.2.0.dev202[31](https://github.com/pytorch/FBGEMM/actions/runs/7172899604/job/19531073976?pr=2203#step:12:32)211+rocm5.7\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2023.1.0         h213fc3f_46[34](https://github.com/pytorch/FBGEMM/actions/runs/7172899604/job/19531073976?pr=2203#step:12:35)4  \r\n[conda] mkl-service               2.4.0           py310h5eee18b_1  \r\n[conda] mkl_fft                   1.3.8           py310h5eee18b_0  \r\n[conda] mkl_random                1.2.4           py310hdb19cb5_0  \r\n[conda] numpy                     1.26.2          py310h5f9d8c6_0  \r\n[conda] numpy-base                1.26.2          py310hb5e798b_0  \r\n[conda] pytorch-triton-rocm       2.1.0+dafe1[45](https://github.com/pytorch/FBGEMM/actions/runs/7172899604/job/19531073976?pr=2203#step:12:46)9[82](https://github.com/pytorch/FBGEMM/actions/runs/7172899604/job/19531073976?pr=2203#step:12:83)          pypi_0    pypi\r\n[conda] torch                     2.2.0.dev20231211+rocm5.7          pypi_0    pypi\r\n```\n\ncc @malfet @zou3519 @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @seemethere"}

{"number": 115344, "owner": "RobertCsordas", "title": "cuda IMA using custom triton kernel with compile", "create_time": "2023-12-07T13:32:11Z", "update_time": "2024-07-30T18:31:04Z", "body": "### \ud83d\udc1b Describe the bug\n\nHi,\r\n\r\nI'm trying to make my [MoE Triton kernel](https://github.com/RobertCsordas/moe_layer/blob/master/triton_src/moe_layer/cvmm.py) work with torch.compile(). I know that this is not supported in the current stable version, but it is in the nightly (at least it worked with the simple kernels I tried). However, when I try to use my actual kernel, it fails.\r\n\r\nThere are two independent issues: one is the \"Illegal getattr invocation stride in strict mode\" issue (see the log below), which seems to prevent compilation but doesn't seem to be fatal. However the \"RuntimeError: CUDA error: an illegal memory access was encountered\" problem is fatal. Note that both the example and the full kernel work well without compile, and without running into illegal memory access issues.\r\n\r\nInterestingly, if I remove an unused, static IF  (which is never true in this example, and it depends only on external arguments), the code works. Also, if there is only one option in the triton.autotune(), it works as well. I marked both places with a comment starting with \"!!!!!!\" in the code below.\r\n\r\nThis is the simplified code I was able to come up with:\r\n```python3\r\nimport torch\r\n\r\nimport triton\r\nimport triton.language as tl\r\n\r\n# CVMM from: https://github.com/RobertCsordas/moe_layer/blob/master/triton_src/moe_layer/cvmm.py\r\n# Based on: https://github.com/openai/triton/blob/main/python/tutorials/03-matrix-multiplication.py\r\n\r\nfrom typing import Union, Optional\r\nfrom dataclasses import dataclass\r\n\r\n\r\n@dataclass\r\nclass CVMMSel:\r\n    raw_sel: torch.Tensor\r\n    sel: torch.Tensor\r\n    sel_index: torch.Tensor\r\n    out_index: Optional[torch.Tensor] = None\r\n\r\n\r\ndef cvmm_prepare_sel(sel: torch.Tensor, n_experts: int) -> CVMMSel:\r\n    fsel = sel.flatten()\r\n    ssel, sel_index = fsel.sort()\r\n    return CVMMSel(sel, ssel.view_as(sel), sel_index, None)\r\n\r\n\r\n\r\n# !!!!!! Leaving just one autotune config solves the \"RuntimeError: CUDA error: an illegal memory access was\r\n# encountered\" problem !!!!!!\r\n@triton.autotune(\r\n    configs=[\r\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\r\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\r\n    ],\r\n    key=['M', 'N', 'K']\r\n)\r\n@triton.jit\r\ndef cvmm_kernel(\r\n    # Pointers to matrices\r\n    a_ptr, b_ptr, c_ptr, index_ptr, sel_ptr, out_index_ptr,\r\n    # Matrix dimensions\r\n    M, N, K,\r\n    stride_cm, stride_cn,\r\n    stride_index, stride_sel, stride_out_index,\r\n    # Meta-parameters\r\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\r\n    GROUP_SIZE_M: tl.constexpr\r\n):\r\n    pid = tl.program_id(axis=0)\r\n\r\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\r\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\r\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\r\n    group_id = pid // num_pid_in_group\r\n    first_pid_m = group_id * GROUP_SIZE_M\r\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\r\n    pid_n = (pid % num_pid_in_group) // group_size_m\r\n\r\n    pid_m = first_pid_m + (pid % group_size_m)\r\n\r\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\r\n\r\n    remap_offs_am = tl.load(index_ptr + stride_index * offs_am)\r\n\r\n    # Create offset pointers\r\n    c = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float16)\r\n\r\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\r\n\r\n\r\n    # !!!!!! Removing this IF solves the \"RuntimeError: CUDA error: an illegal memory access was encountered\" problem,\r\n    # even though it is always False in this example !!!!!!\r\n    # To test it, keep the else branch.\r\n    if out_index_ptr is not None:\r\n        remap_offs_cm = tl.load(out_index_ptr + stride_out_index * offs_am)\r\n    else:\r\n        remap_offs_cm = remap_offs_am\r\n\r\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\r\n    c_ptrs = c_ptr + stride_cm * remap_offs_cm[:, None] + stride_cn * offs_cn[None, :]\r\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\r\n    tl.store(c_ptrs, c, mask=c_mask)\r\n\r\n\r\n\r\ndef cvmm_triton(x: torch.Tensor, sel_index: torch.Tensor, sel: torch.Tensor, keys: torch.Tensor, out_dtype: torch.dtype, out_index: Optional[torch.Tensor] = None):\r\n    x = x.flatten(end_dim=-2)\r\n    assert x.shape[-1] == keys.shape[1]\r\n\r\n    sel_shape = sel.shape\r\n    sel = sel.flatten()\r\n\r\n    M = sel.shape[0]\r\n    O, K, N = keys.shape\r\n    # Allocates output.\r\n    out = torch.empty((M, N), device=x.device, dtype=out_dtype)\r\n\r\n    grid = lambda META: (\r\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\r\n    )\r\n\r\n    cvmm_kernel[grid](\r\n        x, keys, out, sel_index, sel, None,\r\n        M, N, K,\r\n        out.stride(0), out.stride(1),\r\n        sel_index.stride(0), sel.stride(0), 0,\r\n    )\r\n\r\n    return out.view(*sel_shape, N)\r\n\r\n\r\nclass CVMM(torch.autograd.Function):\r\n    warned = False\r\n\r\n    @staticmethod\r\n    def forward(ctx, x: torch.Tensor, sel_index: torch.Tensor, sel: torch.Tensor, keys: torch.Tensor, out_index: Optional[torch.Tensor] = None):\r\n        ctx.save_for_backward(x, keys, sel, sel_index, out_index)\r\n\r\n        out_type = torch.float16 if torch.is_autocast_enabled() else x.dtype\r\n        res = cvmm_triton(x, sel_index, sel, keys, out_type, out_index)\r\n        ctx.op_type = out_type\r\n        ctx.keys_type = keys.dtype\r\n        ctx.is_autocast = torch.is_autocast_enabled()\r\n        return res\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        x, keys, sel, sel_index, out_index = ctx.saved_tensors\r\n\r\n        keys_dt = keys\r\n\r\n        grad_x_full = cvmm_triton(grad_output, sel_index, sel, keys_dt.transpose(1,2), ctx.op_type, None)\r\n        grad_x = grad_x_full.view_as(x)\r\n\r\n        return grad_x, None, None, None, None\r\n\r\n\r\ndef cvmm(x: torch.Tensor, sel: Union[torch.Tensor, CVMMSel], keys: torch.Tensor):\r\n    if not isinstance(sel, CVMMSel):\r\n        sel = cvmm_prepare_sel(sel, keys.shape[0])\r\n\r\n    return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index)\r\n\r\n# Compile test\r\n\r\n\r\nclass Model(torch.nn.Module):\r\n    def forward(self, x, sel, w):\r\n        return cvmm(x, sel, w)\r\n\r\nmodel = torch.compile(Model().cuda())\r\n# model = Model().cuda()\r\n\r\n\r\ntorch.manual_seed(0)\r\nn_experts = 8\r\nn_channels = 64\r\nexpert_size = 64\r\nbs = 64\r\n\r\ndevice = torch.device(\"cuda\")\r\ndtype = torch.float16\r\n\r\nkeys = torch.nn.Parameter(torch.randn(n_experts, n_channels, expert_size, dtype=dtype, device=device))\r\ntestvec = torch.randn(bs, n_channels, dtype=dtype, device=device)\r\nsel = torch.randint(0, n_experts, (bs,), dtype=torch.int32, device=device)\r\n\r\nprint(model(testvec, sel, keys).shape)\r\n\r\n```\n\n### Error logs\n\n```\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [WARNING] speculate_subgraph: while introspecting the user-defined autograd.Function, we were unable to trace function `trampoline_autograd_bwd` into a single graph. This means that Dynamo was unable to prove safety for this API and will fall back to eager-mode PyTorch, which could lead to a slowdown.\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR] Illegal getattr invocation stride in strict mode\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR] Traceback (most recent call last):\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 231, in speculate_subgraph\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     output = f.call_function(tx, args, sub_kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 248, in call_function\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return super().call_function(tx, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 81, in call_function\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_user_function_return(\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in inline_user_function_return\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return inner_fn(self, inst)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1252, in CALL_FUNCTION_EX\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.call_function(fn, argsvars.items, kwargsvars.items)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.push(fn.call_function(self, args, kwargs))\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 660, in call_function\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return self.obj.call_method(tx, self.name, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 505, in call_method\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_call(tx, backward, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return inner_fn(self, inst)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1213, in CALL_FUNCTION\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.call_function(fn, args, {})\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.push(fn.call_function(self, args, kwargs))\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 248, in call_function\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return super().call_function(tx, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 81, in call_function\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_user_function_return(\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in inline_user_function_return\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1303, in LOAD_ATTR\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     result = BuiltinVariable(getattr).call_function(\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py\", line 651, in call_function\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     result = handler(tx, *args, **kwargs)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py\", line 1229, in call_getattr\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return obj.var_getattr(tx, name).clone(source=source)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/tensor.py\", line 218, in var_getattr\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     unimplemented(f\"Illegal getattr invocation {name} in strict mode\")\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 193, in unimplemented\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR]     raise Unsupported(msg)\r\n[2023-12-07 14:21:05,882] [0/0] torch._dynamo.variables.higher_order_ops: [ERROR] torch._dynamo.exc.Unsupported: Illegal getattr invocation stride in strict mode\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [WARNING] speculate_subgraph: while introspecting the user-defined autograd.Function, we were unable to trace function `trampoline_autograd_bwd` into a single graph. This means that Dynamo was unable to prove safety for this API and will fall back to eager-mode PyTorch, which could lead to a slowdown.\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR] Illegal getattr invocation stride in strict mode\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR] Traceback (most recent call last):\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 231, in speculate_subgraph\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     output = f.call_function(tx, args, sub_kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 248, in call_function\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return super().call_function(tx, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 81, in call_function\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_user_function_return(\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in inline_user_function_return\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return inner_fn(self, inst)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1252, in CALL_FUNCTION_EX\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.call_function(fn, argsvars.items, kwargsvars.items)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.push(fn.call_function(self, args, kwargs))\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 660, in call_function\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return self.obj.call_method(tx, self.name, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 505, in call_method\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_call(tx, backward, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return inner_fn(self, inst)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1213, in CALL_FUNCTION\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.call_function(fn, args, {})\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     self.push(fn.call_function(self, args, kwargs))\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 248, in call_function\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return super().call_function(tx, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 81, in call_function\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return tx.inline_user_function_return(\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in inline_user_function_return\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return cls.inline_call_(parent, func, args, kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     tracer.run()\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     and self.step()\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     getattr(self, inst.opname)(inst)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1303, in LOAD_ATTR\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     result = BuiltinVariable(getattr).call_function(\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py\", line 651, in call_function\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     result = handler(tx, *args, **kwargs)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py\", line 1229, in call_getattr\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     return obj.var_getattr(tx, name).clone(source=source)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/variables/tensor.py\", line 218, in var_getattr\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     unimplemented(f\"Illegal getattr invocation {name} in strict mode\")\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]   File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 193, in unimplemented\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR]     raise Unsupported(msg)\r\n[2023-12-07 14:21:05,929] [1/0] torch._dynamo.variables.higher_order_ops: [ERROR] torch._dynamo.exc.Unsupported: Illegal getattr invocation stride in strict mode\r\nTraceback (most recent call last):\r\n  File \"/home/robert/rnn_generalization_test/compile_test3.py\", line 167, in <module>\r\n    print(model(testvec, sel, keys).shape)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/robert/rnn_generalization_test/compile_test3.py\", line 148, in forward\r\n    return cvmm(x, sel, w)\r\n  File \"/home/robert/rnn_generalization_test/compile_test3.py\", line 141, in cvmm\r\n    return CVMM.apply(x, sel.sel_index, sel.sel, keys, sel.out_index)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/autograd/function.py\", line 553, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/home/robert/rnn_generalization_test/compile_test3.py\", line 114, in forward\r\n    @staticmethod\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\r\n    return fn(*args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 901, in forward\r\n    return compiled_fn(full_args)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 81, in g\r\n    return f(*args)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 94, in runtime_wrapper\r\n    all_outs = call_func_at_runtime_with_args(\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 105, in call_func_at_runtime_with_args\r\n    out = normalize_as_list(f(args))\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 118, in rng_functionalization_wrapper\r\n    return compiled_fw(args)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py\", line 864, in __call__\r\n    return self.get_current_callable()(inputs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 611, in run\r\n    return model(new_inputs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/codecache.py\", line 892, in _run_from_cache\r\n    return compiled_graph.compiled_artifact(inputs)\r\n  File \"/tmp/torchinductor_robert/vr/cvrlscv7n2ne4vcxcvozlnvsvyiddsior53it7ifax2hfs2uosni.py\", line 111, in call\r\n    cvmm_kernel_0.run(a_ptr=arg0_1, b_ptr=arg1_1, c_ptr=buf0, index_ptr=arg3_1, sel_ptr=arg2_1, out_index_ptr=None, M=64, N=64, K=64, stride_cm=64, stride_cn=1, stride_index=1, stride_sel=1, stride_out_index=0, grid=grid_wrapper_for_cvmm_kernel_0, stream=stream0)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py\", line 540, in run\r\n    self.autotune_to_one_config(*args, grid=grid, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py\", line 444, in autotune_to_one_config\r\n    timings = self.benchmark_all_configs(*args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py\", line 420, in benchmark_all_configs\r\n    timings = {\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py\", line 421, in <dictcomp>\r\n    launcher: self.bench(launcher, *args, **kwargs)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/triton_heuristics.py\", line 392, in bench\r\n    return do_bench(kernel_call, rep=40, fast_flush=True)\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/_inductor/utils.py\", line 167, in do_bench\r\n    return triton_do_bench(*args, **kwargs)[0]\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/triton/testing.py\", line 103, in do_bench\r\n    torch.cuda.synchronize()\r\n  File \"/home/robert/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 801, in synchronize\r\n    return torch._C._cuda_synchronize()\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\n\n### Minified repro\n\nThe minifier creates a directory structure which has no files. I'm not sure what I'm supposed to paste here. The directory structure looks like this:\r\n\r\n```\r\n~/rnn_generalization_test/torch_compile_debug >>> find .                                                                                                                                                                                                                                                                                                    \r\n.\r\n./run_2023_12_07_14_23_32_124760-pid_906807\r\n./run_2023_12_07_14_23_32_124760-pid_906807/minifier\r\n./run_2023_12_07_14_23_32_124760-pid_906807/minifier/checkpoints\r\n```\n\n### Versions\n\n```\r\nCollecting environment information...\r\nPyTorch version: 2.2.0.dev20231206+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Manjaro Linux (x86_64)\r\nGCC version: (GCC) 12.2.1 20230201\r\nClang version: 15.0.7\r\nCMake version: version 3.25.0\r\nLibc version: glibc-2.37\r\n\r\nPython version: 3.10.10 (main, Mar  5 2023, 22:26:53) [GCC 12.2.1 20230201] (64-bit runtime)\r\nPython platform: Linux-5.15.109-1-MANJARO-x86_64-with-glibc2.37\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA TITAN V\r\nNvidia driver version: 530.41.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/libcudnn.so.8.8.0\r\n/usr/lib/libcudnn_adv_infer.so.8.8.0\r\n/usr/lib/libcudnn_adv_train.so.8.8.0\r\n/usr/lib/libcudnn_cnn_infer.so.8.8.0\r\n/usr/lib/libcudnn_cnn_train.so.8.8.0\r\n/usr/lib/libcudnn_ops_infer.so.8.8.0\r\n/usr/lib/libcudnn_ops_train.so.8.8.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   39 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          12\r\nOn-line CPU(s) list:             0-11\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\r\nCPU family:                      6\r\nModel:                           158\r\nThread(s) per core:              2\r\nCore(s) per socket:              6\r\nSocket(s):                       1\r\nStepping:                        10\r\nCPU(s) scaling MHz:              25%\r\nCPU max MHz:                     3200.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        6402.62\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities\r\nL1d cache:                       192 KiB (6 instances)\r\nL1i cache:                       192 KiB (6 instances)\r\nL2 cache:                        1.5 MiB (6 instances)\r\nL3 cache:                        12 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-11\r\nVulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:              Mitigation; PTE Inversion\r\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Mitigation; IBRS\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Mitigation; Microcode\r\nVulnerability Tsx async abort:   Mitigation; TSX disabled\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==6.0.0\r\n[pip3] kmeans-pytorch==0.3\r\n[pip3] numpy==1.24.2\r\n[pip3] pytorch-lightning==1.9.0\r\n[pip3] pytorch-triton==2.1.0+bcad9dabe1\r\n[pip3] torch==2.2.0.dev20231206+cu121\r\n[pip3] torch-dct==0.1.6\r\n[pip3] torch-tb-profiler==0.4.1\r\n[pip3] torchaudio==2.2.0.dev20231206+cu121\r\n[pip3] torchdata==0.6.1\r\n[pip3] torchmetrics==0.11.0\r\n[pip3] torchpq==0.3.0.5\r\n[pip3] torchtext==0.15.2\r\n[pip3] torchvision==0.17.0.dev20231206+cu121\r\n[conda] Could not collect\r\n```\n\ncc @ezyang @gchanan @zou3519 @kadeng @msaroufim @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @muchulee8 @ColinPeppler @amjames @desertfire @oulgen @aakhundov @bdhirsh @anijain2305 @peterbell10 @wconstab"}

{"number": 114859, "owner": "leslie-fang-intel", "title": "PT2 QAT flow fails to get reasonable accuracy with  mobilenet_v3_large", "create_time": "2023-11-30T09:11:12Z", "update_time": "2024-07-11T14:49:01Z", "body": "### \ud83d\udc1b Describe the bug\n\nFor the `mobilenet_v3_large` model from the torch-vision repository, the fp32 accuracy achieves `Acc@1 74.054 and Acc@5 91.340` on the ImageNet validation dataset. Through PT2 PTQ quantization, combining `X86InductorQuantizer` and the Inductor CPP backend, the quantized accuracy remains reasonable at `Acc@1 73.512 and Acc@5 91.130`. However, when applying PT2 QAT quantization, the top-1 accuracy experiences a significant drop, reaching approximately 63.\r\n\r\n- To reproduce this issue, we employed the `XNNPACKQuantizer` without lowering the converted model into Inductor, as outlined in [script here](https://gist.github.com/leslie-fang-intel/07569566add5f7e66df768e992d3c217). The resulting accuracy on the ImageNet validation dataset is `* Acc@1 62.170 and Acc@5 84.050`.\r\n\r\n- In additional, it seems that this issue is not directly related to the numeric differences between `torch.ops.aten._native_batch_norm_legit.default ` and `torch.ops.aten.cudnn_batch_norm.default` as point out in https://github.com/pytorch/pytorch/issues/111384, since if we [move model to cuda before export](https://gist.github.com/leslie-fang-intel/f88691d26b02552b2accbf555bb78fa5) also gives similar accuracy as `* Acc@1 61.326 Acc@5 83.250`.\n\n### Versions\n\n```\r\n(quantization) [root@CPX-4 quantization]# python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 2.2.0a0+git805e4e1\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 11.2.1 20210728 (Red Hat 11.2.1-1)\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.9.10 (main, Mar  2 2022, 12:02:00)  [GCC 9.3.0] (64-bit runtime)\r\nPython platform: Linux-4.19.5-1.el7.elrepo.x86_64-x86_64-with-glibc2.17\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                56\r\nOn-line CPU(s) list:   0-55\r\nThread(s) per core:    1\r\nCore(s) per socket:    28\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 85\r\nModel name:            Intel Genuine CPU\r\nStepping:              10\r\nCPU MHz:               1347.854\r\nCPU max MHz:           2900.0000\r\nCPU min MHz:           1200.0000\r\nBogoMIPS:              5800.00\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              1024K\r\nL3 cache:              39424K\r\nNUMA node0 CPU(s):     0-27\r\nNUMA node1 CPU(s):     28-55\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==3.8.2\r\n[pip3] flake8-bugbear==20.1.4\r\n[pip3] flake8-coding==1.3.3\r\n[pip3] flake8-comprehensions==3.3.0\r\n[pip3] flake8-executable==2.0.4\r\n[pip3] flake8-logging-format==0.9.0\r\n[pip3] flake8-pyi==20.5.0\r\n[pip3] flake8-simplify==0.19.3\r\n[pip3] intel-extension-for-pytorch==2.2.0+git5eb4697\r\n[pip3] mypy==1.4.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.24.3\r\n[pip3] torch==2.2.0a0+gitc1e51fc\r\n[pip3] torchvision==0.16.0a0+370134\r\n[pip3] triton==2.0.0\r\n[conda] mkl                       2023.0.0            intel_25398    intel\r\n[conda] mkl-include               2023.0.0            intel_25398    intel\r\n[conda] mkl-include               2023.0.0                  <pip>\r\n[conda] mkl-service               2.4.0           py39h3605609_14    intel\r\n[conda] mkl-static                2023.0.0                  <pip>\r\n[conda] mkl_fft                   1.3.1           py39hcab1719_22    intel\r\n[conda] mkl_random                1.2.2           py39hbf47bc3_22    intel\r\n[conda] mkl_umath                 0.1.1           py39hf66a691_32    intel\r\n[conda] numpy                     1.22.3           py39hf0956d0_5    intel\r\n[conda] numpy                     1.24.3                    <pip>\r\n[conda] numpy-base                1.22.3           py39h45c9ace_5    intel\r\n[conda] torchvision               0.16.0a0+370134           <pip>\r\n[conda] triton                    2.0.0                     <pip>\r\n\r\n```\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @msaroufim @ezyang @anijain2305 @chauhang @penguinwu @wconstab @bdhirsh @zou3519"}

{"number": 113443, "owner": "akihironitta", "title": "`CompileProfiler` reports graph breaks while `dynamo.explain` reports no graph breaks", "create_time": "2023-11-10T12:32:32Z", "update_time": "2024-06-18T17:38:12Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nI'm seeing inconsistent number of graph breaks between (1) `CompileProfiler` and (2) `dynamo.explain` as reproduced with a script below.\r\n\r\n### Error logs\r\n\r\n(1) `CompileProfiler`\r\n\r\n```\r\nTorchdynamo Profiler Report\r\n===========================\r\n\r\nGraph Breaks\r\n------------\r\nGraph breaks happen when torchdynamo encounters code it can't safely trace.\r\nIf you want to find out why breaks are happening, check below for each break reason\r\nYou may gain additional insight by passing `fullgraph=True` to torch.compile,\r\nto stop at the first break.\r\n\r\nGraph Break Reason                     Count\r\n-----------------------------------  -------\r\nhasattr: UserDefinedClassVariable()        1\r\nhasattr no source                          2\r\n\r\nRecompilation\r\n-------------\r\nThese subgraphs were recompiled more than once due to guard failures\r\nGuard failures indicate some condition assumed to be static by the tracer changed,\r\nmaking it unsafe to reuse the compiled program.\r\n\r\nNo recompilation detected\r\n```\r\n\r\n(2) `dynamo.explain`\r\n\r\n```\r\nGraph Count: 1\r\nGraph Break Count: 0\r\nOp Count: 0\r\nBreak Reasons:\r\nOps per Graph:\r\n  Ops 1:\r\nOut Guards:\r\n  Guard 1:\r\n    Name: \"G['edge_type']\"\r\n    Source: global\r\n    Create Function: LIST_LENGTH\r\n    Guard Types: ['LIST_LENGTH']\r\n    Code List: [\"___check_type_id(G['edge_type'], 94630539513280)\", \"len(G['edge_type']) == 3\"]\r\n    Object Weakref: None\r\n    Guarded Class Weakref: <weakref at 0x7f33bc8b7790; to 'type' at 0x5610e3b14dc0 (tuple)>\r\n  Guard 2:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: DETERMINISTIC_ALGORITHMS\r\n    Guard Types: ['DETERMINISTIC_ALGORITHMS']\r\n    Code List: ['not ___are_deterministic_algorithms_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 3:\r\n    Name: \"L['x']\"\r\n    Source: local\r\n    Create Function: TENSOR_MATCH\r\n    Guard Types: ['TENSOR_MATCH']\r\n    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\r\n    Object Weakref: <weakref at 0x7f32cadd3790; dead>\r\n    Guarded Class Weakref: <weakref at 0x7f330243d760; to 'torch._C._TensorMeta' at 0x5610e753afd0 (Tensor)>\r\n  Guard 4:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: DEFAULT_DEVICE\r\n    Guard Types: ['DEFAULT_DEVICE']\r\n    Code List: ['utils_device.CURRENT_DEVICE == None']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 5:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: GRAD_MODE\r\n    Guard Types: ['GRAD_MODE']\r\n    Code List: ['___is_grad_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 6:\r\n    Name: \"G['edge_type'][0]\"\r\n    Source: global\r\n    Create Function: CONSTANT_MATCH\r\n    Guard Types: ['EQUALS_MATCH']\r\n    Code List: [\"___check_type_id(G['edge_type'][0], 94630539501152)\", \"G['edge_type'][0] == 'a'\"]\r\n    Object Weakref: None\r\n    Guarded Class Weakref: <weakref at 0x7f33bc8c1030; to 'type' at 0x5610e3b11e60 (str)>\r\n  Guard 7:\r\n    Name: \"G['edge_type'][1]\"\r\n    Source: global\r\n    Create Function: CONSTANT_MATCH\r\n    Guard Types: ['EQUALS_MATCH']\r\n    Code List: [\"___check_type_id(G['edge_type'][1], 94630539501152)\", \"G['edge_type'][1] == 'to'\"]\r\n    Object Weakref: None\r\n    Guarded Class Weakref: <weakref at 0x7f33bc8c1030; to 'type' at 0x5610e3b11e60 (str)>\r\n  Guard 8:\r\n    Name: \"G['edge_type'][2]\"\r\n    Source: global\r\n    Create Function: CONSTANT_MATCH\r\n    Guard Types: ['EQUALS_MATCH']\r\n    Code List: [\"___check_type_id(G['edge_type'][2], 94630539501152)\", \"G['edge_type'][2] == 'b'\"]\r\n    Object Weakref: None\r\n    Guarded Class Weakref: <weakref at 0x7f33bc8c1030; to 'type' at 0x5610e3b11e60 (str)>\r\n  Guard 9:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: TORCH_FUNCTION_STATE\r\n    Guard Types: ['TORCH_FUNCTION_STATE']\r\n    Code List: ['___is_torch_function_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 10:\r\n    Name: ''\r\n    Source: shape_env\r\n    Create Function: SHAPE_ENV\r\n    Guard Types: None\r\n    Code List: None\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 11:\r\n    Name: \"G['ModuleDict']\"\r\n    Source: global\r\n    Create Function: FUNCTION_MATCH\r\n    Guard Types: None\r\n    Code List: None\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 12:\r\n    Name: \"L['x']\"\r\n    Source: local\r\n    Create Function: TENSOR_MATCH\r\n    Guard Types: ['TENSOR_MATCH']\r\n    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\r\n    Object Weakref: <weakref at 0x7f32cadd3790; dead>\r\n    Guarded Class Weakref: <weakref at 0x7f330243d760; to 'torch._C._TensorMeta' at 0x5610e753afd0 (Tensor)>\r\n  Guard 13:\r\n    Name: \"L['self'].module_dict\"\r\n    Source: local_nn_module\r\n    Create Function: NN_MODULE\r\n    Guard Types: None\r\n    Code List: None\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 14:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: DETERMINISTIC_ALGORITHMS\r\n    Guard Types: ['DETERMINISTIC_ALGORITHMS']\r\n    Code List: ['not ___are_deterministic_algorithms_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 15:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: DEFAULT_DEVICE\r\n    Guard Types: ['DEFAULT_DEVICE']\r\n    Code List: ['utils_device.CURRENT_DEVICE == None']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 16:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: GRAD_MODE\r\n    Guard Types: ['GRAD_MODE']\r\n    Code List: ['___is_grad_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 17:\r\n    Name: \"L['___stack0']\"\r\n    Source: local\r\n    Create Function: CONSTANT_MATCH\r\n    Guard Types: ['EQUALS_MATCH']\r\n    Code List: [\"___check_type_id(L['___stack0'], 94630539501152)\", \"L['___stack0'] == '<a___to___b>'\"]\r\n    Object Weakref: None\r\n    Guarded Class Weakref: <weakref at 0x7f33bc8c1030; to 'type' at 0x5610e3b11e60 (str)>\r\n  Guard 18:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: TORCH_FUNCTION_STATE\r\n    Guard Types: ['TORCH_FUNCTION_STATE']\r\n    Code List: ['___is_torch_function_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 19:\r\n    Name: \"L['self']\"\r\n    Source: local\r\n    Create Function: NN_MODULE\r\n    Guard Types: ['ID_MATCH']\r\n    Code List: [\"___check_obj_id(L['self'], 139860182476800)\"]\r\n    Object Weakref: <weakref at 0x7f32c24753f0; to 'SomeModel' at 0x7f33bc7ea800>\r\n    Guarded Class Weakref: <weakref at 0x7f33bc8550d0; to 'type' at 0x5610ea6587d0 (SomeModel)>\r\n  Guard 20:\r\n    Name: ''\r\n    Source: shape_env\r\n    Create Function: SHAPE_ENV\r\n    Guard Types: None\r\n    Code List: None\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 21:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: GRAD_MODE\r\n    Guard Types: ['GRAD_MODE']\r\n    Code List: ['___is_grad_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 22:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: DETERMINISTIC_ALGORITHMS\r\n    Guard Types: ['DETERMINISTIC_ALGORITHMS']\r\n    Code List: ['not ___are_deterministic_algorithms_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 23:\r\n    Name: \"L['___stack0']\"\r\n    Source: local\r\n    Create Function: NN_MODULE\r\n    Guard Types: ['ID_MATCH']\r\n    Code List: [\"___check_obj_id(L['___stack0'], 139860182476848)\"]\r\n    Object Weakref: <weakref at 0x7f32c22e8720; to 'Linear' at 0x7f33bc7ea830>\r\n    Guarded Class Weakref: <weakref at 0x7f3301f98ea0; to 'type' at 0x5610e77ae410 (Linear)>\r\n  Guard 24:\r\n    Name: \"L['x']\"\r\n    Source: local\r\n    Create Function: TENSOR_MATCH\r\n    Guard Types: ['TENSOR_MATCH']\r\n    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\r\n    Object Weakref: <weakref at 0x7f32cadd3790; dead>\r\n    Guarded Class Weakref: <weakref at 0x7f330243d760; to 'torch._C._TensorMeta' at 0x5610e753afd0 (Tensor)>\r\n  Guard 25:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: TORCH_FUNCTION_STATE\r\n    Guard Types: ['TORCH_FUNCTION_STATE']\r\n    Code List: ['___is_torch_function_enabled()']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 26:\r\n    Name: ''\r\n    Source: global\r\n    Create Function: DEFAULT_DEVICE\r\n    Guard Types: ['DEFAULT_DEVICE']\r\n    Code List: ['utils_device.CURRENT_DEVICE == None']\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\n  Guard 27:\r\n    Name: ''\r\n    Source: shape_env\r\n    Create Function: SHAPE_ENV\r\n    Guard Types: None\r\n    Code List: None\r\n    Object Weakref: None\r\n    Guarded Class Weakref: None\r\nCompile Times: TorchDynamo compilation metrics:\r\nFunction                         Runtimes (s)\r\n-------------------------------  --------------------------------------\r\n_compile.<locals>.compile_inner  0.1674, 0.0107, 0.0078, 0.0017, 0.0185\r\nOutputGraph.call_user_compiler   0.0042\r\n```\r\n\r\n### Minified repro\r\n\r\n```python\r\nimport torch\r\nfrom torch_geometric.nn.module_dict import ModuleDict\r\n\r\nedge_type = (\"a\", \"to\", \"b\")\r\n\r\nclass SomeModel(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.module_dict = ModuleDict({\r\n            edge_type: torch.nn.Linear(1, 1),\r\n        })\r\n\r\n    def forward(self, x):\r\n        key = ModuleDict.to_internal_key(edge_type)\r\n        x = self.module_dict[key](x)\r\n        return x\r\n\r\n# (1) shows that module has graph breaks\r\nfrom torch._dynamo.utils import CompileProfiler\r\nwith CompileProfiler() as prof:\r\n    model = torch.compile(SomeModel())\r\n    model(torch.randn(1, 1))\r\n    print(prof.report())\r\n\r\n# (2) shows that module has NO graph break\r\nmodel = SomeModel()\r\nexplain = torch._dynamo.explain(model)(torch.randn(1, 1))\r\nprint(explain)\r\n```\r\n\r\nFor the note, https://github.com/pyg-team/pytorch_geometric/commit/40cc3b1b8e13dbe66a77bcf6bbd9f939a2d8fa27 was used to reproduce this.\r\n\r\n### Versions\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.1.0+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.5\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.13.0-1031-aws-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 11.5.119\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 510.47.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nThread(s) per core:              2\r\nCore(s) per socket:              8\r\nSocket(s):                       1\r\nNUMA node(s):                    1\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           85\r\nModel name:                      Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\r\nStepping:                        7\r\nCPU MHz:                         2499.996\r\nBogoMIPS:                        4999.99\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       256 KiB\r\nL1i cache:                       256 KiB\r\nL2 cache:                        8 MiB\r\nL3 cache:                        35.8 MiB\r\nNUMA node0 CPU(s):               0-15\r\nVulnerability Itlb multihit:     KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:              Mitigation; PTE Inversion\r\nVulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke avx512_vnni\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==6.1.0\r\n[pip3] numpy==1.24.1\r\n[pip3] onnx==1.14.1\r\n[pip3] onnxruntime==1.16.0\r\n[pip3] pytorch_frame==0.1.0\r\n[pip3] pytorch-lightning==2.0.9.post0\r\n[pip3] pytorch-memlab==0.3.0\r\n[pip3] torch==2.1.0+cu118\r\n[pip3] torch_frame==0.1.0\r\n[pip3] torch_geometric==2.4.0\r\n[pip3] torchmetrics==1.2.0\r\n[pip3] torchvision==0.16.0\r\n[pip3] triton==2.1.0\r\n[conda] numpy                     1.24.1                   pypi_0    pypi\r\n[conda] pytorch-frame             0.1.0                    pypi_0    pypi\r\n[conda] pytorch-lightning         2.0.9.post0              pypi_0    pypi\r\n[conda] pytorch-memlab            0.3.0                    pypi_0    pypi\r\n[conda] torch                     2.1.0+cu118              pypi_0    pypi\r\n[conda] torch-frame               0.1.0                    pypi_0    pypi\r\n[conda] torch-geometric           2.4.0                    pypi_0    pypi\r\n[conda] torchmetrics              1.2.0                    pypi_0    pypi\r\n[conda] torchvision               0.16.0                   pypi_0    pypi\r\n[conda] triton                    2.1.0                    pypi_0    pypi\r\n```\r\n\r\ncc @ezyang @anijain2305 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @msaroufim @wconstab @bdhirsh @zou3519 @aakhundov"}

{"number": 113415, "owner": "ErnestChan", "title": "[inline-inbuilt-nn-modules] Torch compile with DDP errors on parameterized modules", "create_time": "2023-11-10T02:05:54Z", "update_time": "2024-07-11T20:35:35Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nOn Pytorch 2.1.0, when there is a parametrized module in a model, torch.compile on the DDP-wrapped module results in an error like:\r\n```\r\n[2023-11-08 17:28:50,112]   File \"~/.local/lib/python3.10/site-packages/torch/_dynamo/backends/distributed.py\", line 268, in compile_fn\r\n[2023-11-08 17:28:50,112]     if maybe_param.requires_grad and not self._ignore_parameter(\r\n[2023-11-08 17:28:50,112]   File \"~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1695, in __getattr__\r\n[2023-11-08 17:28:50,112]     raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\r\n[2023-11-08 17:28:50,112] torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:\r\n[2023-11-08 17:28:50,112] AttributeError: 'ParametrizedConv2d' object has no attribute 'requires_grad'\r\n```\r\n\r\nThe following snippet should reproduce the error:\r\n```Python\r\nfrom types import SimpleNamespace\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.nn.parallel.distributed import DistributedDataParallel\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = nn.Conv2d(3, 3, 1, bias=False)\r\n        nn.utils.parametrizations.spectral_norm(self.conv, eps=1e-12)\r\n\r\n    def forward(self, input):\r\n        output = self.conv(input)\r\n        return output.sum()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    input = torch.rand((4, 3, 12, 12)).to(device=\"cuda\")\r\n    original_model = Model().to(\"cuda\")\r\n\r\n    fake_ddp = SimpleNamespace(bucket_bytes_cap=int(100 * 1024 * 1024))\r\n    DistributedDataParallel._get_active_ddp_module = lambda: fake_ddp\r\n    model = torch.compile(original_model, mode=\"default\")\r\n    out = model(input)\r\n    out.backward()\r\n```\r\n\r\nThe error happens because Pytorch 2.1.0 generates the following FX graph:\r\n```\r\nipdb> print(gm.graph)\r\ngraph():\r\n    %l_input_ : torch.Tensor [num_users=1] = placeholder[target=L_input_]\r\n    %l__self___conv : [num_users=1] = get_attr[target=L__self___conv]\r\n    %forward : [num_users=1] = call_method[target=forward](args = (%l__self___conv, %l_input_), kwargs = {})\r\n    %sum_1 : [num_users=1] = call_method[target=sum](args = (%forward,), kwargs = {})\r\n    return (sum_1,)\r\n```\r\nThe `get_attr`'s target is the `ParametrizedConv2d` object which becomes `maybe_param` in [this line](https://github.com/pytorch/pytorch/blob/1488bafb274fcc82c8aac429bad61738bc3f950e/torch/_dynamo/backends/distributed.py#L285).\r\n\r\nWhile in Pytorch 2.0.1 we get the following graph which gets the weight from the ParametrizedConv2d and uses it with the functional version of Conv2d.\r\n```\r\nipdb> print(gm.graph)\r\ngraph():\r\n    %input_1 : torch.Tensor [#users=1] = placeholder[target=input]\r\n    %weight_weight : [#users=1] = call_module[target=weight_weight](args = (), kwargs = {})\r\n    %conv2d : [#users=1] = call_function[target=torch.conv2d](args = (%input_1, %weight_weight, None, (1, 1), (0, 0), (1, 1), 1), kwargs = {})\r\n    %sum_1 : [#users=1] = call_method[target=sum](args = (%conv2d,), kwargs = {})\r\n    return (sum_1,) \r\n```\r\n\r\nNot quite sure why the FX graph is different between the 2 versions, but the fix may be as simple as adding:\r\n```\r\nif torch.nn.utils.parametrize.is_parametrized(maybe_param):\r\n    maybe_param = maybe_param.parametrizations.weight.original\r\n```\r\nHappy to put up a fix if this sounds good.\r\n\r\n### Versions\r\n\r\nRan on Ubuntu 20.04 LTS, Nvidia A100, using Pytorch 2.1.0 built from source.\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @penguinwu @fegin @XilunWu @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @wanchaol @fduwjj @wz337 @kiukchung @d4l3k @lucasllc @tianyu-l"}

{"number": 111962, "owner": "jon-chuang", "title": "[pytree] `pytree.tree_map` does not respect type of `torch.Size`", "create_time": "2023-10-24T20:45:31Z", "update_time": "2023-10-25T15:41:24Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nI don't think this is right. It needs to preserve the original types, shouldn't it?\r\n\r\n```python\r\ntorch.utils._pytree.tree_map(lambda x: x, torch.Size([1]))\r\n# (1,)\r\n```\r\n\r\nThe reason is that `isinstance(torch.Size([1]), tuple)` is True.\r\n\r\nThis behaviour may not be desirable as `torch.Size` has methods that `tuple` does not.\r\n\r\n### Versions\r\n\r\nmain\n\ncc @zou3519"}

{"number": 111713, "owner": "jon-chuang", "title": "[dynamo] generic `is_` type shortcut is not appropriately guarded", "create_time": "2023-10-20T22:31:04Z", "update_time": "2024-06-28T20:10:22Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\n\r\nThis hack\r\nhttps://github.com/pytorch/pytorch/blob/5a2f97dee80ca27b732e12b61359d6e475a9c03b/torch/_dynamo/variables/builtin.py#L1310\r\nin https://github.com/pytorch/pytorch/pull/104840\r\n\r\nis too strong.\r\n\r\n### Use-Cases\r\nSupport for tracing `is_` when there's type mismatch: https://github.com/pytorch/pytorch/issues/109504\r\nPart of the way to: https://github.com/pytorch/pytorch/issues/111550\r\n\r\n### Solution\r\nPerhaps installing unalias check and guarding on as python constant might be good enough to solve generic is_ check without resorting to hacks like this.\r\n\r\n\r\n### Repro\r\n```python\r\nimport collections\r\n\r\ndef fn(x, y, z):\r\n    z += 1\r\n    return x is y, z\r\n\r\nx = collections.OrderedDict({1: 2})\r\ny = {1: 2}\r\nz = torch.tensor([1])\r\n\r\nopt_fn = torch.compile(fn, backend=\"eager\", fullgraph=True)\r\n\r\nassert opt_fn(x, y, z) == fn(x, y, z)  # Compile with x is y == False\r\nassert opt_fn(x, x, z) == fn(x, x, z)  # Does not recompile as input types are not guarded\r\n```\r\n\r\ncc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng \r\n\r\n### Versions\r\n\r\nmain\r\n\r\ncc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519"}

{"number": 108984, "owner": "ax3l", "title": "PPC64le: GCC 11.2.1 Linker Error in bin/torch_shm_manager", "create_time": "2023-09-10T19:39:25Z", "update_time": "2023-10-31T01:02:47Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nWhile debugging a compile issue on PPC64le in #108934, we realized there is a linker issue in pytorch 2.1.0-rc3 with GCC 11.2.1:\r\n```bash\r\npython3 -m pip install -r requirements.txt\r\nrm -rf build\r\nCC=gcc CXX=g++ USE_CUDA=1 BLAS=OpenBLAS MAX_JOBS=64 ATEN_AVX512_256=OFF BUILD_TEST=0 python3 setup.py develop\r\n```\r\n\r\n### Error Message\r\n\r\n```\r\n[2310/2316] Linking CXX executable bin/torch_shm_manager\r\nFAILED: bin/torch_shm_manager \r\n: && /usr/tcetmp/bin/g++ -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow -O3 -DNDEBUG -DNDEBUG -rdynamic -L/usr/tce/packages/spectrum-mpi/ibm/spectrum-mpi-2023.02.10/lib -pthread @CMakeFiles/torch_shm_manager.rsp -o bin/torch_shm_manager  && :\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `at::cuda::jit::generate_reduction_code(at::cuda::jit::KernelDescriptor const&, int, bool, bool, int, int)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `c10::Error::Error(c10::SourceLocation, std::string)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `c10::detail::torchInternalAssertFail(char const*, char const*, unsigned int, char const*, std::string const&)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `torch::Library::Library(torch::Library::Kind, std::string, c10::optional<c10::DispatchKey>, char const*, unsigned int)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `at::cuda::jit::generate_code(at::cuda::jit::KernelDescriptor const&, bool, bool, at::cuda::jit::BinaryFuncVariant, bool, int, bool)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `at::cuda::jit::generate_code(int, int, std::string const&, std::string const&, std::string const&, std::string const&, std::string const&, bool, bool, at::cuda::jit::BinaryFuncVariant, c10::SmallVector<std::string, 6u>&, bool, int, bool)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `c10::DeviceTypeName(c10::DeviceType, bool)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `at::TensorBase::toString() const'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `c10::Device::Device(std::string const&)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `c10::detail::LogAPIUsageFakeReturn(std::string const&)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `at::cuda::jit::jit_pwise_function(std::string const&, std::string const&)'\r\n/g/g90/huebl1/src/pytorch/build/lib/libtorch_cuda.so: undefined reference to `c10::Warning::Warning(c10::variant<c10::Warning::UserWarning, c10::Warning::DeprecationWarning>, c10::SourceLocation const&, std::string, bool)'\r\ncollect2: error: ld returned 1 exit status\r\n[2312/2316] Linking CXX shared library lib/libtorch_python.so\r\nninja: build stopped: subcommand failed.\r\n```\r\n\r\n### Versions\r\n\r\n- pytorch 2.1.0-rc3\r\n- CUDA 12.0.76\r\n- GCC 11.2.1\r\n- RHEL 8.7"}

{"number": 107478, "owner": "pearu", "title": "Adding batched CSR tensors with different sparsities produces an invalid tensor", "create_time": "2023-08-18T19:45:41Z", "update_time": "2023-08-18T19:56:14Z", "body": "\r\n## Issue description\r\n\r\nAs in the title.\r\n\r\nAdding batched CSR tensors with the same sparsities produces correct results.\r\nAlso, adding CSR tensors without batch dimensions and different sparsities produce correct results.\r\n\r\n## Code example\r\n\r\n```python\r\n>>> a = torch.ones(2, 2, 2); a[:, :, 0] = 0; a = a.to_sparse_csr()\r\n>>> b = torch.ones(2, 2, 2).to_sparse_csr()\r\n>>> c = a + b\r\n>>> c\r\ntensor(crow_indices=tensor([[0, 2, 4],\r\n                            [0, 1, 2]]),\r\n       col_indices=tensor([0, 1, 0, 1]),\r\n       values=tensor([1., 2., 1., 2.]), size=(2, 2, 2), nnz=4,\r\n       layout=torch.sparse_csr)\r\n>>> torch._validate_sparse_csr_tensor_args(c.crow_indices(), c.col_indices(), c.values(), c.shape)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: crow_indices and col_indices dimensionalities must be equal but got 2 and 1, respectively\r\n```\r\n\r\nThe expected result is:\r\n```python\r\n>>> (a.to_dense() + b.to_dense()).to_sparse_csr()\r\ntensor(crow_indices=tensor([[0, 2, 4],\r\n                            [0, 2, 4]]),\r\n       col_indices=tensor([[0, 1, 0, 1],\r\n                           [0, 1, 0, 1]]),\r\n       values=tensor([[1., 2., 1., 2.],\r\n                      [1., 2., 1., 2.]]), size=(2, 2, 2), nnz=4,\r\n       layout=torch.sparse_csr)\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch version: main\n\ncc @alexsamardzic @nikitaved @cpuhrsch @amjames @bhosmer"}

{"number": 107451, "owner": "pearu", "title": "Conversion from COO with two sparse dimensions to CSR with dense_dim specified fails", "create_time": "2023-08-18T11:32:03Z", "update_time": "2023-08-18T13:01:41Z", "body": "## Issue description\r\n\r\nAs in the title. Not specifying `dense_dim` in the conversion leads to an expected result.\r\n\r\n## Code example\r\n\r\n```python\r\n>>> torch.ones(2, 2, 2).to_sparse(2).to_sparse(layout=torch.sparse_csr, dense_dim=1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: sparse_coo_to_sparse: conversion from Sparse to SparseCsr with dense_dim argument given is not supported\r\n```\r\n\r\nThe expected behavior is no exception and with the following result:\r\n```python\r\n>>> torch.ones(2, 2, 2).to_sparse(2).to_sparse(layout=torch.sparse_csr)\r\ntensor(crow_indices=tensor([0, 2, 4]),\r\n       col_indices=tensor([0, 1, 0, 1]),\r\n       values=tensor([[1., 1.],\r\n                      [1., 1.],\r\n                      [1., 1.],\r\n                      [1., 1.]]), size=(2, 2, 2), nnz=4,\r\n       layout=torch.sparse_csr)\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch version: main\n\ncc @alexsamardzic @nikitaved @cpuhrsch @amjames @bhosmer"}

{"number": 107381, "owner": "pearu", "title": "sparse_mask method ignores masked-in elements of sparse compressed input tensors", "create_time": "2023-08-17T15:09:50Z", "update_time": "2023-08-26T07:35:28Z", "body": "## Issue description\r\n\r\nAs in the title. The current behavior with sparse compressed inputs contradicts [sparse_mask](https://pytorch.org/docs/main/generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask) documentation note: \"The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.\" as well as the current behavior with strided or sparse COO inputs.\r\n\r\n## Code example\r\n\r\n```python\r\n>>> mask = torch.tensor([[0, 1], [1, 1]], dtype=torch.bool).to_sparse_csr()\r\n>>> data = torch.tensor([[1, 2], [3, 0]]).to_sparse_csr()\r\n>>> data.sparse_mask(mask)\r\ntensor(crow_indices=tensor([0, 1, 2]),\r\n       col_indices=tensor([1, 0]),\r\n       values=tensor([2, 3]), size=(2, 2), nnz=2, layout=torch.sparse_csr)\r\n```\r\n\r\nThe expected result is\r\n```python\r\ntensor(crow_indices=tensor([0, 1, 3]),\r\n       col_indices=tensor([1, 0, 1]),\r\n       values=tensor([2, 3, 0]), size=(2, 2), nnz=3, layout=torch.sparse_csr)\r\n```\r\nto meet the documented specification and the behavior with COO and strided inputs:\r\n```python\r\n>>> data.to_dense().sparse_mask(mask)\r\ntensor(crow_indices=tensor([0, 1, 3]),\r\n       col_indices=tensor([1, 0, 1]),\r\n       values=tensor([2, 3, 0]), size=(2, 2), nnz=3, layout=torch.sparse_csr)\r\n>>> data.to_sparse_coo().sparse_mask(mask)\r\ntensor(crow_indices=tensor([0, 1, 3]),\r\n       col_indices=tensor([1, 0, 1]),\r\n       values=tensor([2, 3, 0]), size=(2, 2), nnz=3, layout=torch.sparse_csr)\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch version: main\n\ncc @alexsamardzic @nikitaved @cpuhrsch @amjames @bhosmer"}

{"number": 103800, "owner": "lkct", "title": "Mis-annotated return for `F._no_grad_embedding_renorm_` (also JIT related)", "create_time": "2023-06-17T11:33:55Z", "update_time": "2023-06-21T16:00:29Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nThis is a sub-task of #103761.\r\n\r\nThis is not user-facing as this only involves functions prefixed with an underscore.\r\n\r\n`_no_grad_embedding_renorm_` has mismatched typing on the return value:\r\nhttps://github.com/pytorch/pytorch/blob/918fe519a0f9bdb2dd213eaaffbcd420b7391280/torch/nn/functional.py#L2123-L2124\r\n\r\n1. This function does not return anything at all. This is modified on purpose in #18684. (PS: the return value is not used anyway.)\r\n2. The return now got annotated as a `Tuple[Tensor, Tensor]` in #78560. I don't know why it's changed in this way. @samdow Can you please explain this as the author of that PR?\r\n\r\n---\r\n\r\nOther occurrences of `_no_grad_embedding_renorm_`: (I'm not quite sure whether these are related, nor which is correct)\r\nhttps://github.com/pytorch/pytorch/blob/918fe519a0f9bdb2dd213eaaffbcd420b7391280/torch/csrc/api/include/torch/nn/functional/embedding.h#L15-L22\r\nhttps://github.com/pytorch/pytorch/blob/918fe519a0f9bdb2dd213eaaffbcd420b7391280/torch/csrc/jit/runtime/register_special_ops.cpp#L330\r\nhttps://github.com/pytorch/pytorch/blob/918fe519a0f9bdb2dd213eaaffbcd420b7391280/torch/csrc/jit/runtime/serialized_shape_function_registry.cpp#L3196\r\nhttps://github.com/pytorch/pytorch/blob/918fe519a0f9bdb2dd213eaaffbcd420b7391280/torch/jit/_shape_functions.py#L1109\r\n\r\n### Versions\r\n\r\nmaster\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @malfet @rgommers @xuzhao9 @gramster"}

{"number": 102673, "owner": "williamwen42", "title": "Fix dynamo-related debug Python 3.11 failures", "create_time": "2023-06-01T00:06:24Z", "update_time": "2023-12-04T11:15:32Z", "body": "Multiple `PYTORCH_TEST_WITH_DYNAMO=1` tests raise asserts on debug Python 3.11 or otherwise fail. These should be fixed.\r\n\r\nScript to run all tests: https://gist.github.com/williamwen42/c430d1a5667c270c209be51b24490d6e (Each individual test will stop running at the first abort/segfault. To continue, skip the failing test.)\r\n\r\nTests to fix:\r\n\r\n- [ ] `python test/test_optim.py -k test_adagrad_sparse` (`Python/ceval.c:2431: _PyEval_EvalFrameDefault: Assertion 'EMPTY()' failed.`)\r\n\r\nFor reference, these tests also fail without `PYTORCH_TEST_WITH_DYNAMO=1` on 3.11:\r\n\r\n- [ ] `python test/test_tensor_creation_ops.py -k test_non_writable_buffer_cpu_bool` (`Fatal Python error: _Py_CheckSlotResult: Slot getbuffer of type bytes succeeded with an exception set`) (debug 3.11 only) (also fails in debug 3.10)\r\n- [ ] `python test/functorch/test_ops.py -k test_vmapvjpvjp_linalg_tensorsolve_cpu_float32` (`AssertionError: Tensor-likes are not close!`) (debug 3.11 only)\r\n- [ ] Tests `test_bool_indices_cpu` and `test_bool_indices_cuda` in `python test/test_indexing.py` (`AssertionError: Scalars are not equal!`) (only occurs when the entire test suite is run -- running individually passes, fails locally on release 3.11, passes in CI) (https://github.com/pytorch/pytorch/issues/103355)\r\n- [ ] `python test/test_ops_gradients.py -k test_fn_grad_linalg_det_singular_cpu_float64` (`torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,`) (debug 3.11 only)\r\n- [ ] `python test/test_sparse_csr.py -k test_invalid_input_csr_large_cpu` (`AssertionError: \"32-bit integer overflow in nnz\" does not match \"value cannot be converted to type int32 without overflow\"`) (skipped in CI)"}

{"number": 101314, "owner": "qxcv", "title": "Shared library loading logic breaks when CUDA packages are installed in a non-standard location", "create_time": "2023-05-12T21:56:07Z", "update_time": "2023-05-17T17:50:14Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\n**tl;dr:** Some CUDA libraries are distributed alongside Torch via PyPI packages. These packages include `nvidia-cudnn-cu11`, `nvidia-cusparse-cu11`, and so on. Torch's `__init__.py` has various tricks to find and load these libraries, but one of these tricks break when Torch is installed in a different location to the `nvidia-*` packages. This could be fixed by linking all of Torch's CUDA dependencies into `libtorch_global_deps.so`.\r\n\r\n----\r\n\r\n**Longer version:**\r\n\r\nI'm using Torch PyPI with the [pants](https://www.pantsbuild.org/) build system, which creates Python environments with a slightly weird layout. Specifically, each package ends up in its own directory, rather than everything landing in `site-packages` like it would in a virtualenv. This causes problems when I attempt to import PyTorch 2.0.0:\r\n\r\n```\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-20-eb42ca6e4af3> in <cell line: 1>()\r\n----> 1 import torch\r\n\r\n~/.cache/pants/named_caches/pex_root/installed_wheels/6befaad784004b7af357e3d87fa0863c1f642866291f12a4c2af2de435e8ac5c/torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl/torch/__init__.py in <module>\r\n--> 239     from torch._C import *  # noqa: F403\r\n    240 \r\n    241 # Appease the type checker; ordinarily this binding is inserted by the\r\n\r\nImportError: libcudnn.so.8: cannot open shared object file: No such file or directory\r\n```\r\n\r\nI think this may point at an issue with the shared library loading logic in Torch. Specifically, `_load_global_deps()` in Torch's `__init__.py` has this logic that first attempts to load globals deps from `libtorch_global_deps.so`, and then attempts to load any missing libraries if the `CDLL()` call fails:\r\n\r\n```python\r\n# See Note [Global dependencies]\r\ndef _load_global_deps():\r\n    # ... snip ...\r\n\r\n    lib_name = 'libtorch_global_deps' + ('.dylib' if platform.system() == 'Darwin' else '.so')\r\n    here = os.path.abspath(__file__)\r\n    lib_path = os.path.join(os.path.dirname(here), 'lib', lib_name)\r\n\r\n    try:\r\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\r\n    except OSError as err:\r\n        cuda_libs: Dict[str, str] = {\r\n            'cublas': 'libcublas.so.*[0-9]',\r\n            'cudnn': 'libcudnn.so.*[0-9]',\r\n            'cuda_nvrtc': 'libnvrtc.so.*[0-9].*[0-9]',\r\n            'cuda_runtime': 'libcudart.so.*[0-9].*[0-9]',\r\n            'cuda_cupti': 'libcupti.so.*[0-9].*[0-9]',\r\n            'cufft': 'libcufft.so.*[0-9]',\r\n            'curand': 'libcurand.so.*[0-9]',\r\n            'cusolver': 'libcusolver.so.*[0-9]', X\r\n            'cusparse': 'libcusparse.so.*[0-9]', X\r\n            'nccl': 'libnccl.so.*[0-9]', X\r\n            'nvtx': 'libnvToolsExt.so.*[0-9]',\r\n        }\r\n        is_cuda_lib_err = [lib for lib in cuda_libs.values() if(lib.split('.')[0] in err.args[0])]\r\n        # ... some more logic to load libs by looking through `sys.path` ...\r\n```\r\n\r\nOn my system, the `CDLL()` call succeeds at loading `torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl/torch/lib/libtorch_global_deps.so`, so it returns immediately without attempting to load the libraries in the `cuda_libs` dict. However, that `.so` file only links to a subset of the libraries listed above:\r\n\r\n```\r\n$ ldd /long/path/to/torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl/torch/lib/libtorch_global_deps.so\r\n        linux-vdso.so.1 (0x00007ffe3b7d1000)\r\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6d85c92000)\r\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6d85b41000)\r\n        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6d85b3b000)\r\n        libcurand.so.10 => /lib/x86_64-linux-gnu/libcurand.so.10 (0x00007f6d7ff4b000)\r\n        libcufft.so.10 => /lib/x86_64-linux-gnu/libcufft.so.10 (0x00007f6d774be000)\r\n        libcublas.so.11 => /lib/x86_64-linux-gnu/libcublas.so.11 (0x00007f6d6dd40000)\r\n        libcublasLt.so.11 => /lib/x86_64-linux-gnu/libcublasLt.so.11 (0x00007f6d58cda000)\r\n        libcudart.so.11.0 => /lib/x86_64-linux-gnu/libcudart.so.11.0 (0x00007f6d58a34000)\r\n        libnvToolsExt.so.1 => /lib/x86_64-linux-gnu/libnvToolsExt.so.1 (0x00007f6d5882a000)\r\n        libgomp-a34b3233.so.1 => /long/path/to/torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl/torch/lib/libgomp-a34b3233.so.1 (0x00007f6d58600000)\r\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6d5840e000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007f6d85cd9000)\r\n        librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6d58404000)\r\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6d583e7000)\r\n        libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f6d58205000)\r\n```\r\n\r\nSome libraries from `cuda_libs` are missing from the `ldd` output. This is fine when the `nvidia-*` Python packages are installed in the same directory as Torch, because Python can Torch's RPATH  to find the packages. Specifically, the RPATH has a bunch of relative paths to the nvidia libraries, which look like this: \r\n\r\n```\r\n$ORIGIN/../../nvidia/cublas/lib:$ORIGIN/../../nvidia/cuda_cupti/lib:$ORIGIN/../../nvidia/cuda_nvrtc/lib:$ORIGIN/../../nvidia/cuda_runtime/lib:$ORIGIN/../../nvidia/cudnn/lib:$ORIGIN/../../nvidia/cufft/lib:$ORIGIN/../../nvidia/curand/lib:$ORIGIN/../../nvidia/cusolver/lib:$ORIGIN/../../nvidia/cusparse/lib:$ORIGIN/../../nvidia/nccl/lib:$ORIGIN/../../nvidia/nvtx/lib:$ORIGIN\r\n```\r\n\r\nUnfortunately these relative paths do not work when Torch is installed in a different directory to the `nvidia-*` packages, which is the case for me.\r\n\r\n`__init__.py` already has the logic necessary to fix this problem by scanning `sys.path` for the missing libraries. However, that logic currently only gets triggered when the `libtorch_global_deps` import fails. When I modify the code to always look for these libraries, I can import PyTorch again:\r\n\r\n```python\r\n    try:\r\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\r\n        raise OSError(\"libcudnn libnvrtc libcupti libcusolver libcusparse libnccl\")  # always look for these libraries\r\n    except OSError as err:\r\n        cuda_libs: Dict[str, str] = {\r\n            # ... etc. ...\r\n```\r\n\r\nIdeally `__init__.py` should use a more robust test to determine whether `libcudnn` and friends can be loaded. Probably the easiest fix is to link all the libs from `cuda_libs` into `libtorch_global_deps`.\r\n\r\n### Versions\r\n\r\nCollecting environment information...                           \r\nPyTorch version: N/A              \r\nIs debug build: N/A                     \r\nCUDA used to build PyTorch: N/A          \r\nROCM used to build PyTorch: N/A           \r\n                                                                                                              \r\nOS: Ubuntu 20.04.6 LTS (x86_64)         \r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nClang version: 10.0.0-4ubuntu1 \r\nCMake version: version 3.16.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-125-generic-x86_64-with-glibc2.31\r\nIs CUDA available: N/A\r\nCUDA runtime version: 11.6.124\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: \r\nGPU 0: NVIDIA RTX A6000\r\nGPU 1: NVIDIA RTX A6000\r\nGPU 2: NVIDIA RTX A6000\r\nGPU 3: NVIDIA RTX A6000\r\nGPU 4: NVIDIA RTX A6000\r\nGPU 5: NVIDIA RTX A6000\r\nGPU 6: NVIDIA RTX A6000\r\nGPU 7: NVIDIA RTX A6000\r\n\r\nNvidia driver version: 510.60.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              1\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      25\r\nModel:                           1\r\nModel name:                      AMD EPYC 7763 64-Core Processor\r\nStepping:                        1\r\nFrequency boost:                 enabled\r\nCPU MHz:                         3249.791\r\nCPU max MHz:                     2450.0000\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        4900.34\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        64 MiB\r\nL3 cache:                        512 MiB\r\nNUMA node0 CPU(s):               0-63\r\nNUMA node1 CPU(s):               64-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tc\r\ne topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_\r\nvmsave_vmload vgif umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==3.7.9\r\n[pip3] numpy==1.17.4\r\n[conda] No relevant packages"}

{"number": 101160, "owner": "w11wo", "title": "Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile`", "create_time": "2023-05-11T06:33:41Z", "update_time": "2024-03-13T19:27:59Z", "body": "### \ud83d\udc1b Describe the bug\n\nI followed the example to fine-tune HuggingFace's wav2vec 2.0 for speech recognition, using `torch.compile`, aiming to get faster training. However, I ran into an issue as outlined in the error logs.\r\n\r\nI suspect that HuggingFace's wav2vec 2.0 is not yet supported in PyTorch 2.0 and needs some modification to ensure compatibility when running `torch.compile`. It's mostly related to creating mask tensors for SpecAugment.\r\n\r\nThis issue seems to also be related: [fairseq Hubert with torch.compile](https://github.com/pytorch/pytorch/issues/97511). Same issue also raised in [HuggingFace](https://github.com/huggingface/transformers/issues/22849).\n\n### Error logs\n\n```\r\n***** Running training *****\r\n  Num examples = 3,478\r\n  Num Epochs = 15\r\n  Instantaneous batch size per device = 4\r\n  Total train batch size (w. parallel, distributed & accumulation) = 4\r\n  Gradient Accumulation steps = 1\r\n  Total optimization steps = 13,050\r\n  Number of trainable parameters = 311,270,569\r\n  0%|                                                                                                                                                                    | 0/13050 [00:00<?, ?it/sTraceback (most recent call last):\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/output_graph.py\", line 670, in call_user_compiler\r\n    compiled_fn = compiler_fn(gm, self.fake_example_inputs())\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py\", line 1055, in debug_wrapper\r\n    compiled_gm = compiler_fn(gm, example_inputs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/__init__.py\", line 1390, in __call__\r\n    return compile_fx(model_, inputs_, config_patches=self.config)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_inductor/compile_fx.py\", line 455, in compile_fx\r\n    return aot_autograd(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/backends/common.py\", line 48, in compiler_fn\r\n    cg = aot_module_simplified(gm, example_inputs, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 2822, in aot_module_simplified\r\n    compiled_fn = create_aot_dispatcher_function(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 2515, in create_aot_dispatcher_function\r\n    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 1715, in aot_wrapper_dedupe\r\n    return compiler_fn(flat_fn, leaf_flat_args, aot_config)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 2104, in aot_dispatch_autograd\r\n    fx_g = make_fx(joint_forward_backward, aot_config.decompositions)(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py\", line 714, in wrapped\r\n    t = dispatch_trace(wrap_key(func, args, fx_tracer), tracer=fx_tracer, concrete_args=tuple(phs))\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py\", line 443, in dispatch_trace\r\n    graph = tracer.trace(root, concrete_args)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py\", line 778, in trace\r\n    (self.create_arg(fn(*args)),),\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py\", line 652, in flatten_fn\r\n    tree_out = root_fn(*tree_args)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py\", line 459, in wrapped\r\n    out = f(*tensors)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 1158, in traced_joint\r\n    return functionalized_f_helper(primals, tangents)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 1110, in functionalized_f_helper\r\n    f_outs = flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 1078, in flat_fn_no_input_mutations\r\n    outs = flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 1050, in flat_fn_with_synthetic_bases_expanded\r\n    outs = forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py\", line 1019, in forward_or_joint\r\n    backward_out = torch.autograd.grad(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 269, in grad\r\n    return handle_torch_function(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/overrides.py\", line 1534, in handle_torch_function\r\n    result = mode.__torch_function__(public_api, types, args, kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_inductor/overrides.py\", line 38, in __torch_function__\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 303, in grad\r\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/utils/_stats.py\", line 20, in wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py\", line 487, in __torch_dispatch__\r\n    return self.inner_torch_dispatch(func, types, args, kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py\", line 512, in inner_torch_dispatch\r\n    out = proxy_call(self, func, args, kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py\", line 345, in proxy_call\r\n    out = func(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_ops.py\", line 287, in __call__\r\n    return self._op(*args, **kwargs or {})\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/utils/_stats.py\", line 20, in wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 987, in __torch_dispatch__\r\n    return self.dispatch(func, types, args, kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 1162, in dispatch\r\n    op_impl_out = op_impl(self, func, *args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 453, in index_tensor\r\n    check_no_bool_index_tensors(func, *args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 432, in check_no_bool_index_tensors\r\n    raise DynamicOutputShapeException(func)\r\ntorch._subclasses.fake_tensor.DynamicOutputShapeException: aten.index.Tensor\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/wilson_bookbotkids_com/transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py\", line 775, in <module>\r\n    main()\r\n  File \"/home/wilson_bookbotkids_com/transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py\", line 723, in main\r\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/trainer.py\", line 1664, in train\r\n    return inner_training_loop(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/trainer.py\", line 1940, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/trainer.py\", line 2735, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/trainer.py\", line 2767, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 82, in forward\r\n    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1684, in forward\r\n    outputs = self.wav2vec2(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1316, in forward\r\n    hidden_states = self._mask_hidden_states(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1249, in _mask_hidden_states\r\n    if not getattr(self.config, \"apply_spec_augment\", True):\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1259, in <graph break in _mask_hidden_states>\r\n    mask_time_indices = _compute_mask_indices(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1266, in <graph break in _mask_hidden_states>\r\n    mask_time_indices = torch.tensor(mask_time_indices, device=hidden_states.device, dtype=torch.bool)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 337, in catch_errors\r\n    return callback(frame, cache_size, hooks)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 404, in _convert_frame\r\n    result = inner_convert(frame, cache_size, hooks)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 104, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 262, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 324, in _compile\r\n    out_code = transform_code_object(code, transform)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py\", line 445, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 311, in transform\r\n    tracer.run()\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 1726, in run\r\n    super().run()\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 576, in run\r\n    and self.step()\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 540, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 1792, in RETURN_VALUE\r\n    self.output.compile_subgraph(\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/output_graph.py\", line 517, in compile_subgraph\r\n    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/output_graph.py\", line 588, in compile_and_call_fx_graph\r\n    compiled_fn = self.call_user_compiler(gm)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\r\n    raise BackendCompilerFailed(self.compiler_fn, e) from e\r\ntorch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised DynamicOutputShapeException: aten.index.Tensor\r\n\r\nSet torch._dynamo.config.verbose=True for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    torch._dynamo.config.suppress_errors = True\r\n```\n\n### Minified repro\n\nInstall HuggingFace Transformers from source:\r\n```\r\ngit clone https://github.com/huggingface/transformers.git\r\npip install transformers/\r\n```\r\n\r\nRun training script\r\n```sh\r\npython transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py \\\r\n    --dataset_name=\"common_voice\" \\\r\n    --model_name_or_path=\"facebook/wav2vec2-large-xlsr-53\" \\\r\n    --dataset_config_name=\"tr\" \\\r\n    --output_dir=\"./wav2vec2-common_voice-tr-demo-dist\" \\\r\n    --preprocessing_num_workers=\"16\" \\\r\n    --overwrite_output_dir \\\r\n    --num_train_epochs=\"15\" \\\r\n    --per_device_train_batch_size=\"4\" \\\r\n    --gradient_accumulation_steps=\"1\" \\\r\n    --learning_rate=\"3e-4\" \\\r\n    --warmup_steps=\"500\" \\\r\n    --evaluation_strategy=\"steps\" \\\r\n    --text_column_name=\"sentence\" \\\r\n    --save_steps=\"400\" \\\r\n    --eval_steps=\"100\" \\\r\n    --logging_steps=\"1\" \\\r\n    --layerdrop=\"0.0\" \\\r\n    --save_total_limit=\"3\" \\\r\n    --freeze_feature_encoder \\\r\n    --gradient_checkpointing \\\r\n    --chars_to_ignore , ? . ! - \\; \\: \\\" \u201c % \u2018 \u201d \ufffd \\\r\n    --fp16 \\\r\n    --group_by_length \\\r\n    --do_train --do_eval \\\r\n    --torch_compile True\r\n```\n\n### Versions\n\n<details>\r\n<summary>Versions</summary>\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.0.1+cu117\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.7\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 10 (buster) (x86_64)\r\nGCC version: (Debian 8.3.0-6) 8.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.3\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.9.0 | packaged by conda-forge | (default, Nov 26 2020, 07:57:39)  [GCC 9.3.0] (64-bit runtime)\r\nPython platform: Linux-4.19.0-23-cloud-amd64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: 12.0.76\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\r\nNvidia driver version: 525.60.13\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nAddress sizes:       46 bits physical, 48 bits virtual\r\nCPU(s):              12\r\nOn-line CPU(s) list: 0-11\r\nThread(s) per core:  2\r\nCore(s) per socket:  6\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) CPU @ 2.20GHz\r\nStepping:            7\r\nCPU MHz:             2200.152\r\nBogoMIPS:            4400.30\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            39424K\r\nNUMA node0 CPU(s):   0-11\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.3\r\n[pip3] torch==2.0.1+cu117\r\n[pip3] torchaudio==2.0.2+cu117\r\n[pip3] triton==2.0.0\r\n[conda] numpy                     1.24.3                   pypi_0    pypi\r\n[conda] torch                     2.0.1+cu117              pypi_0    pypi\r\n[conda] torchaudio                2.0.2+cu117              pypi_0    pypi\r\n[conda] triton                    2.0.0                    pypi_0    pypi\r\n\r\n</details>\n\ncc @ezyang @gchanan @zou3519 @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @msaroufim @wconstab @ngimel @bdhirsh @anijain2305 @soumith"}

{"number": 99999, "owner": "fernando-neto-ai", "title": "Runtime Error", "create_time": "2023-04-25T17:16:43Z", "update_time": "2023-04-25T22:32:05Z", "body": "### \ud83d\udc1b Describe the bug\r\nHi!, While using the simpletransformers library, I've had the following bug:\r\n\r\n``` Bash\r\n/opt/conda/lib/python3.7/site-packages/simpletransformers/t5/t5_model.py in train(self, train_dataset, output_dir, show_running_loss, eval_data, verbose, **kwargs)\r\n    603                     verbose=verbose and args.evaluate_during_training_verbose,\r\n    604                     silent=args.evaluate_during_training_silent,\r\n--> 605                     **kwargs,\r\n    606                 )\r\n    607 \r\n\r\n/opt/conda/lib/python3.7/site-packages/simpletransformers/t5/t5_model.py in eval_model(self, eval_data, output_dir, verbose, silent, **kwargs)\r\n    700         self._move_model_to_device()\r\n    701 \r\n--> 702         eval_dataset = self.load_and_cache_examples(eval_data, evaluate=True, verbose=verbose, silent=silent)\r\n...\r\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\r\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]))]'. Reason: 'RuntimeError('falseINTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1634272178570/work/aten/src/ATen/MapAllocator.cpp\":300, please report a bug to PyTorch. unable to write to file </torch_4843_2733>')'\r\n```\r\n\r\n### Versions\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.10.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.6 LTS (x86_64)\r\nGCC version: Could not collect\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.7.11 (default, Jul 27 2021, 14:32:16)  [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-5.19.0-38-generic-x86_64-with-debian-buster-sid\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: \r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\r\nNvidia driver version: 525.105.17\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              16\r\nOn-line CPU(s) list: 0-15\r\nThread(s) per core:  2\r\nCore(s) per socket:  8\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               158\r\nModel name:          Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\r\nStepping:            12\r\nCPU MHz:             4794.428\r\nCPU max MHz:         5000.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            7200.00\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            256K\r\nL3 cache:            16384K\r\nNUMA node0 CPU(s):   0-15\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.20.2\r\n[pip3] torch==1.10.0\r\n[pip3] torchelastic==0.2.0\r\n[pip3] torchtext==0.11.0\r\n[pip3] torchvision==0.11.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cudatoolkit               11.1.74              h6bb024c_0    nvidia\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] mkl                       2021.4.0           h06a4308_640  \r\n[conda] mkl-service               2.4.0            py37h7f8727e_0  \r\n[conda] mkl_fft                   1.3.1            py37hd3c417c_0  \r\n[conda] mkl_random                1.2.2            py37h51133e4_0  \r\n[conda] numpy                     1.20.2                   pypi_0    pypi\r\n[conda] pytorch                   1.10.0          py3.7_cuda11.1_cudnn8.0.5_0    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torchelastic              0.2.0                    pypi_0    pypi\r\n[conda] torchtext                 0.11.0                     py37    pytorch\r\n[conda] torchvision               0.11.1               py37_cu111    pytorch"}

{"number": 98467, "owner": "desertfire", "title": "tacotron2 slow compile", "create_time": "2023-04-06T00:47:45Z", "update_time": "2024-04-09T18:40:32Z", "body": "Repro:\r\n```\r\npython benchmarks/dynamo/torchbench.py --accuracy --inference --amp --backend inductor --disable-cudagraphs --device cuda --only tacotron2\r\n```\r\n\r\nctrl+c gives this stack information, which looks like a problem in the fuser heuristic,\r\n```\r\n  File \"/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 636, in __init__\r\n    self.fuse_nodes()\r\n  File \"/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 817, in fuse_nodes\r\n    self.fuse_nodes_once()\r\n  File \"/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 833, in fuse_nodes_once\r\n    if self.can_fuse(node1, node2) and not self.will_fusion_create_cycle(\r\n  File \"/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 907, in will_fusion_create_cycle\r\n    return any(check(self.name_to_fused_node[n]) for n in combined_predecessors)\r\n  File \"/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 907, in <genexpr>\r\n    return any(check(self.name_to_fused_node[n]) for n in combined_predecessors)\r\n  File \"/fsx/users/binbao/conda/envs/release/lib/python3.10/site-packages/torch/_inductor/scheduler.py\", line 896, in check\r\n    return bool(combined_names & node.recursive_predecessors) or any(\r\nKeyboardInterrupt\r\n```\n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @wconstab @soumith @ngimel @Xia-Weiwen"}

{"number": 97992, "owner": "Interpause", "title": "torch.compile not compatible with multiprocessing pool", "create_time": "2023-03-30T16:45:08Z", "update_time": "2024-06-28T05:21:47Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nAttempting to call `torch.compile()` within a multiprocessing pool results in:\r\n\r\n```sh\r\ntorch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised AssertionError: daemonic processes are not allowed to have children\r\n```\r\n\r\nI am using `transformers` `ViTModel` to encode images and cache them to disk inside my DataLoader transform. To improve performance, I was hoping to use `torch.compile()` since I perform this pre-processing transform on CPU. However, compiling in my transform's `__init__` runs into pickling issues when the DataLoader forks the workers. As for compiling after the workers have forked, I run into the issue above.\r\n\r\n### Error logs\r\n\r\n```sh\r\nRuntimeError: Caught BackendCompilerFailed in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 670, in call_user_compiler\r\n    compiled_fn = compiler_fn(gm, self.fake_example_inputs())\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py\", line 1055, in debug_wrapper\r\n    compiled_gm = compiler_fn(gm, example_inputs)\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/__init__.py\", line 1388, in __call__\r\n    from torch._inductor.compile_fx import compile_fx\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 21, in <module>\r\n    from . import config, metrics, overrides, pattern_matcher\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/pattern_matcher.py\", line 19, in <module>\r\n    from .lowering import lowerings as L\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/lowering.py\", line 3868, in <module>\r\n    import_submodule(kernel)\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 1304, in import_submodule\r\n    importlib.import_module(f\"{mod.__name__}.{filename[:-3]}\")\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/kernel/bmm.py\", line 4, in <module>\r\n    from ..select_algorithm import (\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/select_algorithm.py\", line 20, in <module>\r\n    from .codecache import code_hash, DiskCache, PyCodeCache\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/codecache.py\", line 721, in <module>\r\n    AsyncCompile.warm_pool()\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/site-packages/torch/_inductor/codecache.py\", line 660, in warm_pool\r\n    pool._adjust_process_count()\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/concurrent/futures/process.py\", line 692, in _adjust_process_count\r\n    self._spawn_process()\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/concurrent/futures/process.py\", line 709, in _spawn_process\r\n    p.start()\r\n  File \"/home/jovyan/.conda/envs/videocv/lib/python3.10/multiprocessing/process.py\", line 118, in start\r\n    assert not _current_process._config.get('daemon'), \\\r\nAssertionError: daemonic processes are not allowed to have children\r\n```\r\n\r\n### Minified repro\r\n\r\n_No response_\r\n\r\n### Versions\r\n\r\n```sh\r\nPyTorch version: 2.0.0+cu117\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.7\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.0\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-121-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 11.6.112\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A40\r\nNvidia driver version: 470.57.02\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.3.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.3.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.3.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.3.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.3.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.3.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.3.3\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU: removed\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.0.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.24.2\r\n[pip3] pytorch-optimizer==2.5.0\r\n[pip3] pytorch-triton==2.0.0+b8b470bc59\r\n[pip3] torch==2.0.0\r\n[pip3] torchinfo==1.7.2\r\n[pip3] torchvision==0.15.1\r\n[pip3] triton==2.0.0\r\n[conda] numpy                     1.24.2                   pypi_0    pypi\r\n[conda] pytorch-optimizer         2.5.0                    pypi_0    pypi\r\n[conda] pytorch-triton            2.0.0+b8b470bc59          pypi_0    pypi\r\n[conda] torch                     2.0.0                    pypi_0    pypi\r\n[conda] torchinfo                 1.7.2                    pypi_0    pypi\r\n[conda] torchvision               0.15.1                   pypi_0    pypi\r\n[conda] triton                    2.0.0                    pypi_0    pypi\r\n```\r\n\r\ncc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @soumith @ngimel @Xia-Weiwen @desertfire"}

{"number": 97693, "owner": "entron", "title": " Compile targts cuda:0 rather than the device the model is on", "create_time": "2023-03-27T19:02:56Z", "update_time": "2024-02-15T19:42:46Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nThe pytorch compile works great when I only have one GPU (gtx 1660 super, no tensor cores) installed on my computer. However, when I installed a new GPU card RTX 3060 (has tensor cores), which becomes the new \u201ccuda:0\u201d, even though I still chose gtx 1660 (\u201ccuda:1\u201d now) to run the model, the pytorch compile still choose rtx 3060, which has tensor cores as the targeted archecture. And I got the following warning:\r\n\r\n```\r\ncompile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\r\n```\r\nand error:\r\n```\r\ncompiler.py\", line 1671, in _init_handles\r\n\r\nmod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\r\n\r\nRuntimeError: Triton Error [CUDA]: device kernel image is invalid\r\n```\r\n\r\nWould be great if the compile function can either detect which GPU the model is on or has a device option.\r\n\r\n### Versions\r\n\r\npytorch 2.0\r\ncuda 11.8\n\ncc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @soumith @ngimel @Xia-Weiwen @desertfire"}

{"number": 97268, "owner": "kwen2501", "title": "Desync debugger encounters traceMap error", "create_time": "2023-03-21T18:17:42Z", "update_time": "2023-03-22T15:47:13Z", "body": "### \ud83d\udc1b Describe the bug\n\nAn internal user reported the following stack trace:\r\n```\r\n[ProcessGroupNCCL.cpp:847] [Rank 58] NCCL watchdog thread terminated with exception: traceMap[thisSeq][myRank].second == kEventStart INTERNAL ASSERT FAILED at \"torch/csrc/distributed/c10d/TraceUtils.h\":244, please report a bug to PyTorch. Timeout rank [58] last trace item must be kEventStart. thisSeq = 463038, col = ALLREDUCE\r\nException raised from retrieveDesyncReport at torch/csrc/distributed/c10d/TraceUtils.h:244 (most recent call first):\r\n# 2  c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)\r\n# 3  c10::detail::torchInternalAssertFail(char const*, char const*, unsigned int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)\r\n# 4  c10d::retrieveDesyncReport(c10::intrusive_ptr<c10d::Store, c10::detail::intrusive_target_default_null_type<c10d::Store> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int)\r\n# 5  c10d::ProcessGroupNCCL::abortTimedOutCollectives(std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >&)\r\n# 6  c10d::ProcessGroupNCCL::ncclCommWatchdogInternal()\r\n# 7  c10d::ProcessGroupNCCL::ncclCommWatchdog()\r\n# 8  execute_native_thread_routine\r\n# 9  start_thread\r\n# 10 __clone3\r\n```\r\nIt seems that the desync debugger missed to log the start of a collective, and hence when logging the completion it failed to find a match.\n\n### Versions\n\nmaster as of today\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @awgu"}

{"number": 94186, "owner": "pearu", "title": "Addition of hybrid CSR tensors produces incorrect and invalid CSR tensor", "create_time": "2023-02-06T15:19:42Z", "update_time": "2023-04-21T12:36:49Z", "body": "\r\n## Issue description\r\n\r\nAs in the title\r\n\r\n## Code example\r\n\r\n```python\r\n>>> x=torch.ones((2, 2, 2)).to_sparse(layout=torch.sparse_csr, dense_dim=1)\r\n>>> x\r\ntensor(crow_indices=tensor([0, 2, 4]),\r\n       col_indices=tensor([0, 1, 0, 1]),\r\n       values=tensor([[1., 1.],\r\n                      [1., 1.],\r\n                      [1., 1.],\r\n                      [1., 1.]]), size=(2, 2, 2), nnz=4,\r\n       layout=torch.sparse_csr)\r\n>>> x + x\r\ntensor(crow_indices=tensor([0, 2, 4]),\r\n       col_indices=tensor([0, 1, 0, 1]),\r\n       values=tensor([2., 2., 2., 2.]), size=(2, 2, 2), nnz=4,\r\n       layout=torch.sparse_csr)\r\n```\r\n\r\nThe expected result is\r\n```python\r\n>>> x + x\r\ntensor(crow_indices=tensor([0, 2, 4]),\r\n       col_indices=tensor([0, 1, 0, 1]),\r\n       values=tensor([[2., 2.],\r\n                      [2., 2.],\r\n                      [2., 2.],\r\n                      [2., 2.]]), size=(2, 2, 2), nnz=4,\r\n       layout=torch.sparse_csr)\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch: master\r\n\n\ncc @alexsamardzic @nikitaved @cpuhrsch @amjames @bhosmer"}

{"number": 94183, "owner": "pearu", "title": "Addition of batch CSR tensors produces incorrect and invalid CSR tensor", "create_time": "2023-02-06T15:01:12Z", "update_time": "2023-04-21T14:45:09Z", "body": "## Issue description\r\n\r\nAs in the title.\r\n\r\n## Code example\r\n\r\n```python\r\n>>> x=torch.ones((2, 2, 3)).to_sparse_csr()\r\n>>> x\r\ntensor(crow_indices=tensor([[0, 3, 6],\r\n                            [0, 3, 6]]),\r\n       col_indices=tensor([[0, 1, 2, 0, 1, 2],\r\n                           [0, 1, 2, 0, 1, 2]]),\r\n       values=tensor([[1., 1., 1., 1., 1., 1.],\r\n                      [1., 1., 1., 1., 1., 1.]]), size=(2, 2, 3), nnz=6,\r\n       layout=torch.sparse_csr)\r\n>>> y=x.add(x)  # or x + x\r\n>>> y\r\ntensor(crow_indices=tensor([[0, 3, 6],\r\n                            [0, 3, 6]]),\r\n       col_indices=tensor([0, 1, 2, 0, 1, 2]),\r\n       values=tensor([2., 2., 2., 2., 2., 2.]), size=(2, 2, 3), nnz=6,\r\n       layout=torch.sparse_csr)\r\n>>> torch._validate_sparse_csr_tensor_args(y.crow_indices(), y.col_indices(), y.values(), y.shape)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: crow_indices and col_indices dimensionalities must be equal but got 2 and 1, respectively\r\n```\r\n\r\nThe expected result is\r\n```python\r\n>>> y\r\ntensor(crow_indices=tensor([[0, 3, 6],\r\n                            [0, 3, 6]]),\r\n       col_indices=tensor([[0, 1, 2, 0, 1, 2],\r\n                           [0, 1, 2, 0, 1, 2]]),\r\n       values=tensor([[2., 2., 2., 2., 2., 2.],\r\n                      [2., 2., 2., 2., 2., 2.]]), size=(2, 2, 3), nnz=6,\r\n       layout=torch.sparse_csr)\r\n```\r\n\r\nIn place addition produces garbage as well:\r\n```python\r\n>>> x.add_(x)\r\ntensor(crow_indices=tensor([[0, 3, 6],\r\n                            [0, 3, 6]]),\r\n       col_indices=tensor([0, 1, 2, 0, 1, 2]),\r\n       values=tensor([2., 2., 2., 2., 2., 2.]), size=(2, 2, 3), nnz=6,\r\n       layout=torch.sparse_csr)\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch: master\r\n\n\ncc @alexsamardzic @nikitaved @cpuhrsch @amjames @bhosmer"}

{"number": 93515, "owner": "shunting314", "title": "Dynamo can not trace 'int(a_scalar_tensor.item())'", "create_time": "2023-01-21T01:16:11Z", "update_time": "2024-01-31T20:35:44Z", "body": "### \ud83d\udc1b Describe the bug\n\nRCNN model in detectron2 need this support:  ( https://www.internalfb.com/code/fbsource/[ca89130b9eae0efd5bb1cbbff1006cc1556d0b4f]/fbcode/vision/fair/detectron2/detectron2/export/c10.py?lines=483-489 )\r\n\r\n\n\n### Error logs\n\nTraceback (most recent call last):\r\n  File \"/home/shunting/learn/dynamo/temp.py\", line 12, in <module>\r\n    gm, guards = export(f, inp, aten_graph=True, tracing_mode=\"symbolic\")\r\n  File \"/home/shunting/pytorch/torch/_dynamo/eval_frame.py\", line 605, in export\r\n    result_traced = opt_f(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/eval_frame.py\", line 211, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/eval_frame.py\", line 332, in catch_errors\r\n    return callback(frame, cache_size, hooks)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/convert_frame.py\", line 103, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/convert_frame.py\", line 261, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/home/shunting/pytorch/torch/_dynamo/utils.py\", line 154, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/convert_frame.py\", line 324, in _compile\r\n    out_code = transform_code_object(code, transform)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/bytecode_transformation.py\", line 341, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/convert_frame.py\", line 311, in transform\r\n    tracer.run()\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 1693, in run\r\n    super().run()\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 539, in run\r\n    and self.step()\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 502, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 307, in wrapper\r\n    return inner_fn(self, inst)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 975, in CALL_FUNCTION\r\n    self.call_function(fn, args, {})\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 435, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))\r\n  File \"/home/shunting/pytorch/torch/_dynamo/variables/builtin.py\", line 346, in call_function\r\n    return super().call_function(tx, args, kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/variables/base.py\", line 230, in call_function\r\n    unimplemented(f\"call_function {self} {args} {kwargs}\")\r\n  File \"/home/shunting/pytorch/torch/_dynamo/exc.py\", line 71, in unimplemented\r\n    raise Unsupported(msg)\r\ntorch._dynamo.exc.Unsupported: call_function BuiltinVariable(int) [TensorVariable()] {}\n\n### Minified repro\n\n```\r\nfrom torch._dynamo import export\r\nfrom torch._dynamo import config\r\nimport torch\r\nfrom torch import nn\r\n\r\nconfig.capture_scalar_outputs = True\r\ninp = torch.rand(3)\r\n\r\ndef f(inp):\r\n    return torch.full(int(inp[0].item()), 5)\r\n\r\ngm, guards = export(f, inp, aten_graph=True, tracing_mode=\"symbolic\")\r\nprint(f\"Export return graph:\\n{gm.graph}\")\r\n```\n\ncc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @soumith @ngimel"}

{"number": 93511, "owner": "shunting314", "title": "support setattr of arbitrary user provided types in tracing", "create_time": "2023-01-17T22:33:23Z", "update_time": "2024-06-18T19:04:01Z", "body": "### \ud83d\udc1b Describe the bug\n\nDynamo already support patching nn.Module attribute outside of forward call (e.g. during model initialization): https://github.com/pytorch/pytorch/pull/91018 . But some use cases (e.g. detectrons's RCNN model) need patch nn.Module attribute in forward method ( fb internal link: https://fburl.com/code/vvekrxl6 ). Dynamo does not support this right now.\r\n\r\n\n\n### Error logs\n\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/shunting/cpython/build/install/lib/python3.9/importlib/__init__.py\", line 169, in reload\r\n    _bootstrap._exec(spec, module)\r\n  File \"<frozen importlib._bootstrap>\", line 613, in _exec\r\n  File \"<frozen importlib._bootstrap_external>\", line 790, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"/home/shunting/learn/misc.py\", line 20, in <module>\r\n    gm, guards = dynamo.export(MyModule(), *inputs, aten_graph=True, tracing_mode=\"symbolic\")\r\n  File \"/home/shunting/pytorch/torch/_dynamo/eval_frame.py\", line 616, in export\r\n    result_traced = opt_f(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/nn/modules/module.py\", line 1482, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/eval_frame.py\", line 82, in forward\r\n    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/eval_frame.py\", line 211, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/eval_frame.py\", line 332, in catch_errors\r\n    return callback(frame, cache_size, hooks)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/convert_frame.py\", line 103, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/utils.py\", line 90, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/convert_frame.py\", line 339, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/home/shunting/pytorch/torch/_dynamo/convert_frame.py\", line 398, in _compile\r\n    out_code = transform_code_object(code, transform)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/bytecode_transformation.py\", line 341, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/convert_frame.py\", line 385, in transform\r\n    tracer.run()\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 1686, in run\r\n    super().run()\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 537, in run\r\n    and self.step()\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 500, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/symbolic_convert.py\", line 1048, in STORE_ATTR\r\n    BuiltinVariable(setattr)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/variables/builtin.py\", line 375, in call_function\r\n    return super().call_function(tx, args, kwargs)\r\n  File \"/home/shunting/pytorch/torch/_dynamo/variables/base.py\", line 230, in call_function\r\n    unimplemented(f\"call_function {self} {args} {kwargs}\")\r\n  File \"/home/shunting/pytorch/torch/_dynamo/exc.py\", line 67, in unimplemented\r\n    raise Unsupported(msg)\r\ntorch._dynamo.exc.Unsupported: call_function BuiltinVariable(setattr) [UserDefinedClassVariable(), ConstantVariable(str), GetAttrVariable(UserDefinedClassVariable(), run_cos)] {}\r\n\r\nfrom user code:\r\n   File \"/home/shunting/learn/misc.py\", line 10, in forward\r\n    MyModule.run = MyModule.run_cos\r\n\r\nSet torch._dynamo.config.verbose=True for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    torch._dynamo.config.suppress_errors = True\n\n### Minified repro\n\n```\r\nimport torch\r\nfrom torch import nn\r\nimport torch._dynamo as dynamo\r\n\r\nclass MyModule(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        MyModule.run = MyModule.run_cos\r\n        return self.run(x)\r\n\r\n    def run(self, x):\r\n        return torch.sin(x)\r\n\r\n    def run_cos(self, x):\r\n        return torch.cos(x)\r\n\r\ninputs = [torch.rand(5)]\r\ngm, guards = dynamo.export(MyModule(), *inputs, aten_graph=True, tracing_mode=\"symbolic\")\r\nprint(f\"Graph is {gm.graph}\")\r\n```\n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @soumith @wconstab @ngimel"}

{"number": 93502, "owner": "shibdas", "title": "Torchdynamo with onnxrt backend generating fake tensor errors", "create_time": "2023-01-10T18:52:14Z", "update_time": "2023-12-12T00:22:33Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nHi,\r\n  I'm trying to use torchdynamo with onnxrt (both onnxrt_cuda and onnxrt_cpu backends) following the resnet example described in  https://pytorch.org/tutorials/intermediate/dynamo_tutorial.html but I'm hitting an error\r\n\r\n`torch._dynamo.exc.BackendCompilerFailed: onnxrt_cuda raised Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.convolution.default(*(FakeTensor(FakeTensor(..., device='meta', size=(16, 3, 128, 128)), cuda:0), Parameter containing:\r\ntensor([[[[...]]]], device='cuda:0', requires_grad=True),`\r\n\r\nI've tried many examples from torchbench and all are hitting same error. It does not happen with other backends e.g. aot_cudagraphs etc. I've also force set the FakeTensorMode in fake_tensor.py but that hit another error down the line. Am I missing something obvious here?\r\n\r\nTorch version: 2.0.0a0+gitdf46ba4\r\nonnxrt version: 1.13.1\r\n\r\nI was thinking something obvious is missing here before I dig into this in detail.\r\n\r\n### Error logs\r\n\r\n`torch._dynamo.exc.BackendCompilerFailed: onnxrt_cuda raised Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.convolution.default(*(FakeTensor(FakeTensor(..., device='meta', size=(16, 3, 128, 128)), cuda:0), Parameter containing:\r\ntensor([[[[...]]]], device='cuda:0', requires_grad=True),`\r\n\r\n### Minified repro\r\n\r\n```python\r\nimport torch\r\nimport torch._dynamo as dynamo\r\n\r\n# Returns the result of running `fn()` and the time it took for `fn()` to run,\r\n# in seconds. We use CUDA events and synchronization for the most accurate\r\n# measurements.\r\ndef timed(fn):\r\n    start = torch.cuda.Event(enable_timing=True)\r\n    end = torch.cuda.Event(enable_timing=True)\r\n    start.record()\r\n    result = fn()\r\n    end.record()\r\n    torch.cuda.synchronize()\r\n    return result, start.elapsed_time(end) / 1000\r\n\r\n# Generates random input and targets data for the model, where `b` is\r\n# batch size.\r\ndef generate_data(b):\r\n    return (\r\n        torch.randn(b, 3, 128, 128).to(torch.float32).cuda(),\r\n        torch.randint(1000, (b,)).cuda(),\r\n    )\r\n\r\nfrom torchvision.models import resnet18\r\ndef init_model():\r\n    return resnet18().to(torch.float32).cuda()\r\n\r\ndef eval(mod, inp):\r\n    return mod(inp)\r\ntorch._dynamo.config.verbose=True\r\nmodel = init_model()\r\neval_opt = dynamo.optimize(\"onnxrt_cuda\")(eval)\r\n\r\ninp = generate_data(16)[0]\r\nprint(\"eager:\", timed(lambda: eval(model, inp))[1])\r\nprint(\"dynamo:\", timed(lambda: eval_opt(model, inp))[1])\r\n```\r\n\r\ncc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @Chillee @samdow @kshitij12345 @janeyx99 @soumith @ngimel"}

{"number": 91508, "owner": "drisspg", "title": "Update map_nt to take into account size and strides", "create_time": "2022-12-29T15:53:20Z", "update_time": "2022-12-29T15:53:22Z", "body": "# Summary\r\n\r\nSince map_nt doesn't check that the input is contiguous but only that the numels of the input match that of the buffer. This is in order to allow for the unary ops to be run on transposed inputs. We should also update the  map_nt to construct the output tensor with the same stride and offset information.\n\ncc @cpuhrsch @jbschlosser @bhosmer @mikaylagawarecki"}

{"number": 93493, "owner": "davidberard98", "title": "tensor.to_sparse() handling indices incorrectly under dynamo/fake tensor", "create_time": "2022-12-21T21:21:05Z", "update_time": "2023-02-07T20:31:24Z", "body": "### \ud83d\udc1b Describe the bug\n\nto_sparse() is returning a FakeTensor where the indices attribute has the wrong shape/size.\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n\r\n\r\nRepro 1:\r\n```python\r\nimport torch\r\nimport torch._dynamo\r\nimport logging\r\n\r\n# torch._dynamo.config.verbose = True\r\n# torch._dynamo.config.log_level = logging.DEBUG\r\n\r\ndef fn():\r\n    x = torch.rand((7, 3))\r\n    return x.to_sparse().indices().stride()\r\n\r\nprint(fn()) # returns (21, 1)\r\nprint(torch._dynamo.optimize(\"eager\")(fn)()) # returns (1, 1)\r\n```\r\n\r\nRepro 2:\r\n```python\r\nimport torch\r\nimport torch._dynamo\r\nimport logging\r\n\r\n# torch._dynamo.config.verbose = True\r\n# torch._dynamo.config.log_level = logging.DEBUG\r\n\r\nfake_mode = torch._subclasses.fake_tensor.FakeTensorMode()\r\n\r\nprint(\"~~ fake mode below\")\r\nwith fake_mode:\r\n    x = torch.rand((7, 3))\r\n    y = x.to_sparse()\r\n\r\nprint(f\"fake mode indices size: {y.indices().size()}\") # torch.Size([2, 0]); should be [2, 21]\r\n```\n\ncc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh"}

{"number": 91251, "owner": "Skylion007", "title": "Potential bug found with pybind11 dec_ref while gil released", "create_time": "2022-12-21T16:40:44Z", "update_time": "2022-12-27T23:22:45Z", "body": "### \ud83d\udc1b Describe the bug\n\nI tried updating pybind11 in this PR: https://github.com/pytorch/pytorch/pull/91248/ and in the new version, we added an assert to make sure that the GIL is held during all inc_ref / dec_ref of the python object's underlying reference count. This seems to be failing on one of the tests https://github.com/pytorch/pytorch/actions/runs/3750738878/jobs/6371337256 . Since the GIL isn't held here, this could cause invalid reference counts to develop which could corrupt the python / gc state and lead to memory leaks and or crashes.\n\n### Versions\n\nRan on master https://github.com/pytorch/pytorch/actions/runs/3750738878/jobs/6371337256\n\ncc @ezyang"}

{"number": 93488, "owner": "ezyang", "title": "If minifier test fails, stderr/stdout of subprocess calls is not printed", "create_time": "2022-12-17T04:55:49Z", "update_time": "2024-08-22T00:10:55Z", "body": "### \ud83d\udc1b Describe the bug\n\n```\r\n    # Runs the minifier launcher script in `repro_dir`, patched with `patch_code`.\r\n    def _run_minifier_launcher(self, patch_code, repro_dir):\r\n        self.assertIsNotNone(repro_dir)\r\n        launch_file = os.path.join(repro_dir, \"minifier_launcher.py\")\r\n        self.assertTrue(os.path.exists(launch_file))\r\n        launch_code = self._inject_code(patch_code, launch_file)\r\n\r\n        launch_proc = subprocess.run(\r\n            [\"python3\", launch_file],\r\n            capture_output=True,\r\n            cwd=repro_dir,\r\n        )\r\n\r\n        return launch_proc, launch_code\r\n\r\n```\r\n\r\nthis captures output, but then there is nothing that ensures this output is printed if, e.g., something else fails\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 93486, "owner": "wschin", "title": "A Simple Function Causing Graph Break", "create_time": "2022-12-16T18:42:16Z", "update_time": "2023-02-01T01:02:44Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nAn error raised when calling `torch._dynamo.optimize` when optimizing huggingface GPT2 and enforcing a single optimized graph for the whole model.\r\n\r\n```\r\n  File \"conda/dexport/lib/python3.9/site-packages/torch/_dynamo/variables/user_defined.py\", line 243, in call_function\r\n    return super().call_function(tx, args, kwargs)\r\n  File \"conda/dexport/lib/python3.9/site-packages/torch/_dynamo/variables/base.py\", line 230, in call_function\r\n    unimplemented(f\"call_function {self} {args} {kwargs}\")\r\n  File \"conda/dexport/lib/python3.9/site-packages/torch/_dynamo/exc.py\", line 67, in unimplemented\r\n    raise Unsupported(msg)\r\ntorch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(_lru_cache_wrapper) [] {}\r\n\r\nfrom user code:\r\n   File \"dexport/tests/loop.py\", line 12, in forward\r\n    if transformers.is_torch_tpu_available():\r\n\r\nSet torch._dynamo.config.verbose=True for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    torch._dynamo.config.suppress_errors = True\r\n```\r\n\r\nBelow is a minimal repro. If `transformers.is_torch_tpu_available()` is removed, then the code works fine.\r\n\r\n```python\r\nimport torch\r\nimport torch._dynamo\r\nfrom torch import nn\r\nimport transformers\r\n\r\nclass ToyModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = nn.Linear(10, 10)\r\n\r\n    def forward(self, x):\r\n        if transformers.is_torch_tpu_available():\r\n            x = x + 1.0\r\n        else:\r\n            x = x + 2.0\r\n        return self.linear(x)\r\n\r\n\r\nmodel = ToyModel()\r\nx = torch.rand(10, 10, dtype=torch.float32)\r\ny = model(x)\r\n\r\nclass GraphCaptureCompiler:\r\n    def __init__(self):\r\n        self.captured_graph = None\r\n        self.captured_graph_count =  0\r\n    def compile(self, gm, _):\r\n        assert self.captured_graph_count == 0\r\n        self.captured_graph = gm\r\n        self.captured_graph_count += 1\r\n        return gm\r\ncompiler = GraphCaptureCompiler()\r\ny_optimized = torch._dynamo.optimize(compiler.compile, nopython=True)(model)(x)\r\n```\r\n\r\nI'd expect dynamo can just trace through this simple function.\r\n\r\n### Error logs\r\n\r\n_No response_\r\n\r\n### Minified repro\r\n\r\n_No response_"}

{"number": 93480, "owner": "ezyang", "title": "Umbrella issue for only populate real_value_cache in export test suite fails", "create_time": "2022-12-14T21:42:03Z", "update_time": "2024-03-22T17:18:00Z", "body": "### \ud83d\udc1b Describe the bug\n\nSee https://github.com/pytorch/pytorch/pull/90468 for the list of introduced skips\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_\n\ncc @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @soumith @wconstab @ngimel"}

{"number": 93479, "owner": "ezyang", "title": "Umbrella issue for PyTorch test suite failures from torch.* returned non-Tensor output unimplemented", "create_time": "2022-12-14T21:35:37Z", "update_time": "2024-06-18T19:02:38Z", "body": "### \ud83d\udc1b Describe the bug\n\n_No response_\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_\n\ncc @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @soumith @wconstab @ngimel"}

{"number": 93470, "owner": "fladventurerob", "title": "Get the error: AttributeError: Can't pickle local object 'convert_frame.<locals>._convert_frame'", "create_time": "2022-12-07T08:47:08Z", "update_time": "2024-08-06T18:31:46Z", "body": "### \ud83d\udc1b Describe the bug\n\nWhen adding the line:\r\n`model = torch.compile(model)` after loading the model, this error occurs. When removing the line, the script functions as intended. \n\n### Error logs\n\n  File \"/opt/anaconda3/envs/ml1/lib/python3.8/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/opt/anaconda3/envs/ml1/lib/python3.8/multiprocessing/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/opt/anaconda3/envs/ml1/lib/python3.8/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/opt/anaconda3/envs/ml1/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/opt/anaconda3/envs/ml1/lib/python3.8/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/opt/anaconda3/envs/ml1/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/opt/anaconda3/envs/ml1/lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'convert_frame.<locals>._convert_frame'\r\n\n\n### Minified repro\n\n_No response_\n\ncc @ezyang @gchanan @zou3519 @kadeng @msaroufim @wconstab @bdhirsh @anijain2305 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @Xia-Weiwen @ipiszy @soumith @ngimel"}

{"number": 93468, "owner": "vince62s", "title": "OpenNMT doesn't work with dynamic torch.compile", "create_time": "2022-12-06T17:31:39Z", "update_time": "2024-03-21T03:24:32Z", "body": "### \ud83d\udc1b Describe the bug\n\nWhile giving a try with pytorch2 on OpenNMT-py\r\n\r\nusing these two lines:\r\n```\r\n    rawmodel = build_model(model_opt, opt, vocabs, checkpoint)\r\n    model = torch.compile(rawmodel, fullgraph=True, backend='nvprims_aten')\r\n```\n\n### Error logs\n\ngetting this:\r\n```\r\nfrom user code:\r\n   File \"/home/vincent/nlp/OpenNMT-py/onmt/encoders/transformer.py\", line 126, in forward\r\n    mask = ~sequence_mask(src_len).unsqueeze(1)\r\n  File \"/home/vincent/nlp/OpenNMT-py/onmt/utils/misc.py\", line 58, in sequence_mask\r\n    return (torch.arange(0, max_len, device=lengths.device)\r\n\r\nSet torch._dynamo.config.verbose=True for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    torch._dynamo.config.suppress_errors = True\r\n```\n\n### Minified repro\n\n_No response_\n\ncc @ezyang @msaroufim @wconstab @bdhirsh @anijain2305 @zou3519 @soumith @ngimel"}

{"number": 93465, "owner": "BowenBao", "title": "Graph breaks with HuggingFace Stable Diffusion", "create_time": "2022-12-05T19:30:10Z", "update_time": "2023-12-27T11:27:58Z", "body": "### \ud83d\udc1b Describe the bug\n\nUsing this as a tracking issue for graph breaks in HuggingFace Stable Diffusion. I'm going over the outputs from `explain` api, and trying to understand the different graph breaks and how to avoid them.\r\n\r\n- [ ] Number of graphs do not match with number of graph break reasons. (Example script shows 105 graphs vs 75 reasons)\r\n- [ ] call_function UserDefinedObjectVariable: source is a user defined class wrapping torch.Tensor as attribute. Can this be supported in dynamo?\r\n    ```python\r\n    @dataclass\r\n    class Transformer2DModelOutput(BaseOutput):\r\n        sample: torch.FloatTensor\r\n\r\n    ...\r\n\r\n    def some_func(...):\r\n        ...\r\n        return Transformer2DModelOutput(sample=output)\r\n    ```\r\n- [ ] data dependent operator: aten._local_scalar_dense.default: What's the recommended step to work around this issue? Can symbolic number help supporting this inside dynamo?\r\n    ```\r\n      File \"/home/bowbao/diffusers/src/diffusers/schedulers/scheduling_pndm.py:337\", line 337, in step_plms\r\n        prev_sample = self._get_prev_sample(sample, timestep, prev_timestep, model_output)\r\n      File \"/home/bowbao/diffusers/src/diffusers/schedulers/scheduling_pndm.py:371\", line 371, in _get_prev_sample\r\n        alpha_prod_t = self.alphas_cumprod[timestep]\r\n    ```\r\n- [ ] Tensor.numpy: computation in numpy. \n\n### Error logs\n\n```log\r\nDynamo produced 105 graphs with 104 graph break and 1367 ops\r\n Break reasons:\r\n\r\n1. call_function UserDefinedObjectVariable(CLIPTokenizer) [ListVariable()] {'padding': ConstantVariable(str), 'max_length': ConstantVariable(int), 'truncation': ConstantVariable(bool), 'return_tensors': ConstantVariable(str)}\r\n  File \"/home/bowbao/diffusers/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:285\", line 285, in <graph break in _encode_prompt>\r\n    return_tensors=\"pt\",\r\n\r\n2. call_function UserDefinedClassVariable() [] {'sample': TensorVariable()}\r\n  File \"/home/bowbao/diffusers/src/diffusers/models/attention.py:221\", line 221, in forward\r\n    return Transformer2DModelOutput(sample=output)\r\n\r\n3. data dependent operator: aten._local_scalar_dense.default\r\n  File \"/home/bowbao/diffusers/src/diffusers/schedulers/scheduling_pndm.py:337\", line 337, in step_plms\r\n    prev_sample = self._get_prev_sample(sample, timestep, prev_timestep, model_output)\r\n  File \"/home/bowbao/diffusers/src/diffusers/schedulers/scheduling_pndm.py:371\", line 371, in _get_prev_sample\r\n    alpha_prod_t = self.alphas_cumprod[timestep]\r\n\r\n4. data dependent operator: aten._local_scalar_dense.default\r\n  File \"/home/bowbao/diffusers/src/diffusers/schedulers/scheduling_pndm.py:372\", line 372, in <graph break in _get_prev_sample>\r\n    alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\r\n\r\n5. call_function UserDefinedClassVariable() [] {'sample': TensorVariable()}\r\n  File \"/home/bowbao/diffusers/src/diffusers/models/vae.py:586\", line 586, in decode\r\n    return DecoderOutput(sample=dec)\r\n\r\n6. Tensor.numpy\r\n  File \"/home/bowbao/diffusers/src/diffusers/pipelines/stable_diffusion/safety_checker.py:56\", line 56, in forward\r\n    special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().float().numpy()\r\n\r\n7. Tensor.numpy\r\n  File \"/home/bowbao/diffusers/src/diffusers/pipelines/stable_diffusion/safety_checker.py:57\", line 57, in <graph break in forward>\r\n    cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().float().numpy()\r\n\r\nTorchDynamo compilation metrics:\r\nFunction                                             Runtimes (s)\r\n---------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nconvert_frame_assert.<locals>._convert_frame_assert  0.1300, 0.0086, 0.0030, 0.0030, 0.0000, 0.0042, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3416, 0.3279, 0.3279, 0.0081, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0115, 0.0000, 0.0120, 1.0618, 0.0001, 0.0001, 0.0000, 0.0000, 1.0440, 0.0074, 0.0036, 0.0057, 0.0000, 0.0029, 0.0016, 0.9361, 0.0241, 0.5783, 0.5774, 0.0163, 0.0016, 0.1725, 0.0350, 0.1384, 0.0017, 0.0000, 0.0000, 0.0000, 0.0026, 0.0029, 0.0000, 0.0352, 0.1380, 0.0060, 0.1713, 0.1389, 0.0014, 0.0021, 0.0023, 0.0351, 0.1375, 0.0060, 0.0377, 0.1374, 0.0014, 0.0021, 0.0023, 0.0347, 0.1368, 0.0060, 0.0704, 0.1692, 0.0024, 0.0012, 0.0007, 0.0349, 0.0031, 0.0023, 0.1366, 0.0014, 0.0021, 0.0023, 0.0350, 0.1263, 0.1744, 0.0375, 0.1372, 0.0384, 0.1364, 0.0381, 0.1366, 0.0082, 0.0375, 0.1365, 0.0375, 0.1366, 0.0374, 0.1367, 0.0082, 0.0375, 0.1374, 0.0374, 0.1375, 0.0375, 0.1362, 0.0016, 0.0000, 0.0021, 0.0023, 0.0123, 0.0158, 0.0062, 0.0074, 0.0065, 0.0153, 0.0034, 0.0017, 0.0000, 0.0022, 0.0022, 0.0187, 0.0165, 0.0034, 0.0178, 0.0155, 0.0034, 0.0209, 0.0189, 0.0036, 0.0227, 0.0213, 0.0035, 0.0238, 0.0217, 0.0036, 0.0237, 0.0216, 0.0034, 0.0240, 0.0216, 0.0035, 0.0235, 0.0219, 0.0034, 0.0236, 0.0217, 0.0036, 0.0235, 0.0216, 0.0035, 0.0237, 0.0216, 0.0035, 0.0233, 0.0216, 0.0036, 0.0240, 0.0216, 0.0034, 0.0233, 0.0217, 0.0035, 0.0236, 0.0217, 0.0034, 0.0235, 0.0215, 0.0035, 0.0239, 0.0218, 0.0036, 0.0235, 0.0222, 0.0035, 0.0236, 0.0216, 0.0037, 0.0236, 0.0217, 0.0034, 0.0238, 0.0215, 0.0035, 0.0235, 0.0216, 0.0036, 0.0240, 0.0216, 0.0034, 0.0234, 0.0216, 0.0035, 0.0245, 0.0218, 0.0035, 0.0235, 0.0216, 0.0037, 0.0240, 0.0216, 0.0035, 0.0236, 0.0215, 0.0036, 0.0238, 0.0218, 0.0035, 0.0241, 0.0216, 0.0035, 0.0237, 0.0215, 0.0037, 0.0237, 0.0217, 0.0035, 0.0236, 0.0216, 0.0036, 0.0243, 0.0221, 0.0036, 0.0236, 0.0215, 0.0037, 0.0238, 0.0220, 0.0037, 0.0237, 0.0215, 0.0037, 0.0234, 0.0219, 0.0035, 0.0237, 0.0216, 0.0035, 0.0235, 0.0219, 0.0035, 0.0236, 0.0216, 0.0035, 0.0237, 0.0216, 0.0035, 0.0237, 0.0215, 0.0035, 0.0236, 0.0216, 0.0035, 0.0238, 0.0216, 0.0035, 0.0236, 0.0215, 0.0035, 0.0238, 0.0218, 0.0035, 0.0234, 0.0215, 0.0036, 0.0237, 0.0221, 0.0035, 0.0235, 0.0216, 0.0150, 0.0035, 0.4317, 0.5953, 0.0018, 0.0000, 0.0022, 0.0024, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0281, 0.0753, 0.0055, 0.0000, 0.0000, 0.0000\r\ngraphs: 105\r\nreasons: 75\r\n```\n\n### Minified repro\n\n```python\r\nimport torch\r\nfrom diffusers import StableDiffusionPipeline\r\nfrom torch import _dynamo as torchdynamo\r\n\r\npipe = StableDiffusionPipeline.from_pretrained(\r\n    \"runwayml/stable-diffusion-v1-5\",\r\n    torch_dtype=torch.float16,\r\n    revision=\"fp16\",\r\n)\r\npipe = pipe.to(\"cuda\")\r\nprompt = \"a photo of an astronaut riding a horse on mars\"\r\n\r\ndef fn(x):\r\n    return pipe(x)\r\n\r\n(\r\n    explanation,\r\n    out_guards,\r\n    graphs,\r\n    ops_per_graph,\r\n    break_reasons,\r\n    explanation_verbose,\r\n) = torchdynamo.explain(fn, prompt)\r\n\r\nprint(explanation_verbose)  # Dynamo produced 105 graphs with 104 graph break and 1367 ops\r\nprint(f\"graphs: {len(graphs)}\")  # 105\r\nprint(f\"reasons: {len(break_reasons)}\")  # 75\r\n```"}

{"number": 93464, "owner": "noetits", "title": "wav2vec2 model: error trying to do inference", "create_time": "2022-12-05T14:18:16Z", "update_time": "2023-04-27T16:25:18Z", "body": "### \ud83d\udc1b Describe the bug\n\nI tried to do torch.compile() on the second most downloaded model of facebook on huggingface: https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\r\n\r\nI took the example of usage they provide and just added the torch.compile() step.\r\n\r\nI also tried with an another custom audio sample earlier, and strangely, it ran, but very slowly compared to the original model...\r\nBut it had the same prediction result\n\n### Error logs\n\nHere is the warnings and the traceback:\r\n\r\n```\r\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\nNo CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\r\nFound cached dataset librispeech_asr_dummy (/home/noetits/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\r\nIt is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\r\n/home/noetits/miniconda3/envs/.../lib/python3.10/site-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\r\n  warnings.warn(message, UserWarning)\r\n/home/noetits/miniconda3/envs/.../lib/python3.10/site-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\r\n  warnings.warn(message, UserWarning)\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\nCell In [1], line 17\r\n     14 input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values\r\n     16 # retrieve logits\r\n---> 17 logits = model(input_values).logits\r\n     19 # take argmax and decode\r\n     20 predicted_ids = torch.argmax(logits, dim=-1)\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/torch/nn/modules/module.py:1480, in Module._call_impl(self, *args, **kwargs)\r\n   1475 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1476 # this function, and just call forward.\r\n   1477 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1478         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1479         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1480     return forward_call(*args, **kwargs)\r\n   1481 # Do not call functions when jit is used\r\n   1482 full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:80, in OptimizedModule.forward(self, *args, **kwargs)\r\n     79 def forward(self, *args, **kwargs):\r\n---> 80     return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209, in _TorchDynamoContext.__call__.<locals>._fn(*args, **kwargs)\r\n    207 dynamic_ctx.__enter__()\r\n    208 try:\r\n--> 209     return fn(*args, **kwargs)\r\n    210 finally:\r\n    211     set_eval_frame(prior)\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1742, in Wav2Vec2ForCTC.forward(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\r\n   1732 r\"\"\"\r\n   1733 labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\r\n   1734     Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\r\n   (...)\r\n   1737     config.vocab_size - 1]`.\r\n   1738 \"\"\"\r\n   1740 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n-> 1742 outputs = self.wav2vec2(\r\n   1743     input_values,\r\n   1744     attention_mask=attention_mask,\r\n   1745     output_attentions=output_attentions,\r\n   1746     output_hidden_states=output_hidden_states,\r\n   1747     return_dict=return_dict,\r\n   1748 )\r\n   1750 hidden_states = outputs[0]\r\n   1751 hidden_states = self.dropout(hidden_states)\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/torch/nn/modules/module.py:1480, in Module._call_impl(self, *args, **kwargs)\r\n   1475 # If we don't have any hooks, we want to skip the rest of the logic in\r\n   1476 # this function, and just call forward.\r\n   1477 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1478         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1479         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1480     return forward_call(*args, **kwargs)\r\n   1481 # Do not call functions when jit is used\r\n   1482 full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1308, in Wav2Vec2Model.forward(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\r\n   1304         hidden_states[mask_feature_indices] = 0\r\n   1306     return hidden_states\r\n-> 1308 @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\r\n   1309 @add_code_sample_docstrings(\r\n   1310     processor_class=_PROCESSOR_FOR_DOC,\r\n   1311     checkpoint=_CHECKPOINT_FOR_DOC,\r\n   1312     output_type=Wav2Vec2BaseModelOutput,\r\n   1313     config_class=_CONFIG_FOR_DOC,\r\n   1314     modality=\"audio\",\r\n   1315     expected_output=_EXPECTED_OUTPUT_SHAPE,\r\n   1316 )\r\n   1317 def forward(\r\n   1318     self,\r\n   1319     input_values,\r\n   1320     attention_mask=None,\r\n   1321     mask_time_indices=None,\r\n   1322     output_attentions=None,\r\n   1323     output_hidden_states=None,\r\n   1324     return_dict=None,\r\n   1325 ):\r\n   1326     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\r\n   1327     output_hidden_states = (\r\n   1328         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\r\n   1329     )\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209, in _TorchDynamoContext.__call__.<locals>._fn(*args, **kwargs)\r\n    207 dynamic_ctx.__enter__()\r\n    208 try:\r\n--> 209     return fn(*args, **kwargs)\r\n    210 finally:\r\n    211     set_eval_frame(prior)\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/functorch/_src/aot_autograd.py:2107, in aot_module_simplified.<locals>.forward(*runtime_args)\r\n   2105 full_args.extend(params_flat)\r\n   2106 full_args.extend(runtime_args)\r\n-> 2107 return compiled_fn(full_args)\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/functorch/_src/aot_autograd.py:811, in make_boxed_func.<locals>.g(args)\r\n    810 def g(args):\r\n--> 811     return f(*args)\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/functorch/_src/aot_autograd.py:1687, in aot_dispatch_autograd.<locals>.debug_compiled_function(*args)\r\n   1681     elif not can_require_grad:\r\n   1682         assert not a.requires_grad, format_guard_bug_msg(\r\n   1683             aot_config,\r\n   1684             f\"{describe_input(i, aot_config)} would not require grad\"\r\n   1685         )\r\n-> 1687 return compiled_function(*args)\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/functorch/_src/aot_autograd.py:1551, in aot_dispatch_autograd.<locals>.compiled_function(*args)\r\n   1548 else:\r\n   1549     args_with_synthetic_bases = args\r\n-> 1551 all_outs = CompiledFunction.apply(*args_with_synthetic_bases)\r\n   1552 if CompiledFunction.num_aliasing_metadata_outs > 0:\r\n   1553     outs = all_outs[:-CompiledFunction.num_aliasing_metadata_outs]\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/functorch/_src/aot_autograd.py:1455, in aot_dispatch_autograd.<locals>.CompiledFunction.forward(ctx, *deduped_flat_tensor_args)\r\n   1447 @staticmethod\r\n   1448 def forward(ctx, *deduped_flat_tensor_args):\r\n   1449 \r\n   (...)\r\n   1453     # - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\r\n   1454     #   of the original view, and not the synthetic base\r\n-> 1455     fw_outs = call_func_with_args(\r\n   1456         CompiledFunction.compiled_fw, deduped_flat_tensor_args, disable_amp=disable_amp\r\n   1457     )\r\n   1459     num_non_aliased_outs = CompiledFunction.num_non_aliased_outs\r\n   1460     num_aliasing_metadata_outs = CompiledFunction.num_aliasing_metadata_outs\r\n\r\nFile ~/miniconda3/envs/.../lib/python3.10/site-packages/functorch/_src/aot_autograd.py:836, in call_func_with_args(f, args, steal_args, disable_amp)\r\n    834 try:\r\n    835     if hasattr(f, \"_boxed_call\"):\r\n--> 836         out = normalize_as_list(f(args))\r\n    837     else:\r\n    838         # TODO: Please remove soon\r\n    839         # https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\r\n    840         warnings.warn(\r\n    841             \"Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. \"\r\n    842             \"Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \"\r\n    843             \"See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\"\r\n    844         )\r\n\r\nFile /tmp/torchinductor_noetits/dh/cdhukqo6kp2i5wvu22fwo63insaj3b7qi4fq4a6rr52eycvrif32.py:1270, in call(args)\r\n   1268 del primals_33\r\n   1269 buf1 = aten.convolution(buf0, primals_1, primals_2, (5,), (0,), (1,), False, (0,), 1)\r\n-> 1270 assert_size_stride(buf1, (1, 512, 14879), (7618048, 14879, 1))\r\n   1271 del primals_2\r\n   1272 buf2 = empty_strided((1, 14879, 1), (14879, 1, 14879), device='cpu', dtype=torch.float32)\r\n\r\nAssertionError: expected size 512==512, stride 1==14879 at dim=1\r\n```\n\n### Minified repro\n\nI didn't try the minifier, but with the huggingface example, if you add just the `torch.compile()`, I assume you can reproduce the problem\r\n\r\nVersions:\r\nWorking on a WSL UBUNTU 22.04 LTS\r\n\r\ntorch                   1.14.0.dev20221204+cpu\r\ntorchaudio              0.14.0.dev20221204+cpu\r\ntorchvision             0.15.0.dev20221204+cpu\r\n\r\n```\r\n from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\r\n from datasets import load_dataset\r\n import torch\r\n \r\n # load model and processor\r\n processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\r\n model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\r\n model=torch.compile(model)\r\n     \r\n # load dummy dataset and read soundfiles\r\n ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\n \r\n # tokenize\r\n input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values\r\n \r\n # retrieve logits\r\n logits = model(input_values).logits\r\n \r\n # take argmax and decode\r\n predicted_ids = torch.argmax(logits, dim=-1)\r\n transcription = processor.batch_decode(predicted_ids)\r\n```\r\n\r\n\r\n"}

{"number": 93463, "owner": "ezyang", "title": "Traceable tensor subclasses cannot actually be used with AOTAutograd", "create_time": "2022-12-01T14:25:24Z", "update_time": "2023-02-01T01:02:14Z", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nThis patch fails:\r\n\r\n```\r\ndiff --git a/test/dynamo/test_modules.py b/test/dynamo/test_modules.py\r\nindex 6dde69efff..4821b27e05 100644\r\n--- a/test/dynamo/test_modules.py\r\n+++ b/test/dynamo/test_modules.py\r\n@@ -725,10 +725,12 @@ class NNModuleTests(torch._dynamo.test_case.TestCase):\r\n             x = torch.randn(1).as_subclass(TensorProxy)\r\n             cnt = torch._dynamo.testing.CompileCounter()\r\n             out1 = foo(x)\r\n-            opt_foo = torch._dynamo.optimize(cnt, nopython=True)(foo)\r\n+            assert isinstance(out1, TensorProxy)\r\n+            opt_foo = torch._dynamo.optimize(\"aot_eager\", nopython=True)(foo)\r\n             out2 = opt_foo(x)\r\n+            assert isinstance(out2, TensorProxy)\r\n \r\n-            self.assertEqual(cnt.op_count, 4)\r\n+            # self.assertEqual(cnt.op_count, 4)\r\n             self.assertTrue(torch._dynamo.testing.same(out1, out2))\r\n \r\n         finally:\r\n```\r\n\r\nc.f.\r\n\r\n```\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/convert_frame.py\", line 339, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/convert_frame.py\", line 395, in _compile\r\n    out_code = transform_code_object(code, transform)\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/bytecode_transformation.py\", line 341, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/convert_frame.py\", line 382, in transform\r\n    tracer.run()\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/symbolic_convert.py\", line 1615, in run\r\n    super().run()\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/symbolic_convert.py\", line 484, in run\r\n    and self.step()\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/symbolic_convert.py\", line 454, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/symbolic_convert.py\", line 1677, in RETURN_VALUE\r\n    self.output.compile_subgraph(self)\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/output_graph.py\", line 464, in compile_subgraph\r\n    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/output_graph.py\", line 511, in compile_and_call_fx_graph\r\n    compiled_fn = self.call_user_compiler(gm)\r\n  File \"/data/users/ezyang/a/pytorch/torch/_dynamo/output_graph.py\", line 551, in call_user_compiler\r\n    raise BackendCompilerFailed(self.compiler_fn, e) from e\r\ntorch._dynamo.exc.BackendCompilerFailed: compiler_fn raised UnsupportedFakeTensorException: meta converter nyi\r\n\r\nSet torch._dynamo.config.verbose=True for more information\r\n```\r\n\r\nThe root cause of the problem is that torch function should be eliminated from inputs to the compiler, but we just directly pass in the tensor subclass as is. But even if we eliminated it as input from the compiler, we also have to eliminate it from the runtime input to the compiled function, AND we need to reconstruct the subclasses from the output.\r\n\r\nEager accidentally works because the tests are too trivial, and the inner traced function is identical to the outer function calls, so we end up just doing the conventional torch function path.\r\n\r\n### Error logs\r\n\r\n_No response_\r\n\r\n### Minified repro\r\n\r\n_No response_"}

{"number": 93461, "owner": "ezyang", "title": "Support getattr/setattr user properties on Tensor", "create_time": "2022-12-01T04:42:19Z", "update_time": "2023-02-01T01:02:13Z", "body": "### \ud83d\udc1b Describe the bug\n\nTensor subclasses typically make use of storing properties on Tensor. This is not supported at all by TensorVariable and will cause graph breaks, making most applications of tensor subclasses not actually work.\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 93456, "owner": "ezyang", "title": "Way to run accuracy minifier on only one particular subgraph", "create_time": "2022-11-30T03:55:04Z", "update_time": "2024-07-11T20:36:47Z", "body": "### \ud83d\udc1b Describe the bug\n\nAccuracy minifier may get stuck on an early subgraph. In this case, it may be helpful to ask the minifier to not run on those subgraphs, and only run on the one that you suspect is problematic (or you can do some sort of ablation and find all the subgraphs that are failing accuracy, so you can then discard the ones that are just because the accuracy minifier doesn't work.)\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 93455, "owner": "mreso", "title": "Performance regression on interpolation in Kornia", "create_time": "2022-11-30T00:03:26Z", "update_time": "2023-02-01T01:02:06Z", "body": "### \ud83d\udc1b Describe the bug\n\nWhen compiling the rescale function of kornia.geometry.transform.rescale (which conv2d  + interpolation) with dynamo/inductor we see a performance regression of up to 4x in CUDA and smaller but constant regression on CPU.\r\n\r\nThese are timing results for eager vs dynamo:\r\n```\r\n[---------------------------------------- Rescale ----------------------------------------]\r\n                     |  eager_cpu  |  eager_cuda  |  dynamo_cpu  |  dynamo_cuda  |   opencv\r\n1 threads: --------------------------------------------------------------------------------\r\n      [1, 32, 0.5]   |       23.8  |     14.2     |       45.3   |      53.0     |     18.5\r\n      [1, 32, 1.5]   |       65.1  |     14.2     |       86.4   |      52.6     |     18.7\r\n      [1, 64, 0.5]   |       48.9  |     13.9     |       71.4   |      52.8     |     39.5\r\n      [1, 64, 1.5]   |      210.2  |     14.3     |      239.6   |      53.3     |     39.6\r\n      [1, 128, 0.5]  |      147.7  |     14.0     |      182.4   |      53.0     |    120.3\r\n      [1, 128, 1.5]  |      808.8  |     14.2     |      867.7   |      53.2     |    120.2\r\n      [1, 256, 0.5]  |      557.6  |     13.9     |      612.4   |      53.5     |    442.3\r\n      [1, 256, 1.5]  |     3214.4  |     14.2     |     3269.5   |      54.2     |    440.6\r\n      [1, 512, 0.5]  |     2155.5  |     14.3     |     2210.9   |      53.0     |   1736.9\r\n      [1, 512, 1.5]  |    12733.5  |     18.1     |    12769.2   |      61.7     |   1736.2\r\n      [2, 32, 0.5]   |       32.3  |     14.3     |       55.0   |      53.4     |     36.1\r\n      [2, 32, 1.5]   |      114.5  |     14.2     |      136.9   |      53.7     |     35.9\r\n      [2, 64, 0.5]   |       81.3  |     14.0     |      111.0   |      53.6     |     77.8\r\n      [2, 64, 1.5]   |      409.1  |     14.2     |      455.3   |      53.3     |     77.8\r\n      [2, 128, 0.5]  |      287.9  |     14.2     |      332.5   |      53.1     |    238.7\r\n      [2, 128, 1.5]  |     1597.3  |     14.3     |     1652.7   |      53.2     |    238.9\r\n      [2, 256, 0.5]  |     1085.8  |     14.0     |     1134.3   |      53.1     |    881.6\r\n      [2, 256, 1.5]  |     6348.1  |     14.1     |     6415.3   |      55.8     |    881.0\r\n      [2, 512, 0.5]  |     4236.6  |     14.1     |     4303.3   |      53.2     |   3464.7\r\n      [2, 512, 1.5]  |    25832.0  |     26.2     |    26018.7   |      70.9     |   3469.2\r\n      [5, 32, 0.5]   |       57.4  |     14.2     |       81.3   |      53.4     |     88.1\r\n      [5, 32, 1.5]   |      261.4  |     14.1     |      290.9   |      55.8     |     88.3\r\n      [5, 64, 0.5]   |      183.2  |     13.9     |      220.0   |      56.3     |    192.6\r\n      [5, 64, 1.5]   |     1009.4  |     14.3     |     1058.6   |      55.7     |    192.8\r\n      [5, 128, 0.5]  |      688.6  |     14.3     |      734.5   |      56.1     |    594.0\r\n      [5, 128, 1.5]  |     3953.8  |     13.9     |     4003.5   |      55.9     |    595.4\r\n      [5, 256, 0.5]  |     2658.2  |     14.2     |     2720.4   |      55.5     |   2210.5\r\n      [5, 256, 1.5]  |    15882.5  |     17.8     |    20457.2   |      61.2     |   2206.8\r\n      [5, 512, 0.5]  |    10680.2  |     14.0     |    10915.5   |      66.2     |   8679.6\r\n      [5, 512, 1.5]  |   111027.6  |     87.6     |   111251.9   |     138.8     |   8665.6\r\n      [9, 32, 0.5]   |       91.0  |     14.1     |      118.0   |      57.2     |    158.5\r\n      [9, 32, 1.5]   |      463.1  |     15.2     |      505.1   |      61.3     |    158.7\r\n      [9, 64, 0.5]   |      323.9  |     15.0     |      368.8   |      66.7     |    344.5\r\n      [9, 64, 1.5]   |     1799.3  |     15.2     |     1857.8   |      65.9     |    345.1\r\n      [9, 128, 0.5]  |     1222.4  |     15.4     |     1282.7   |      66.1     |   1072.3\r\n      [9, 128, 1.5]  |     7070.1  |     15.3     |     7134.2   |      65.8     |   1073.4\r\n      [9, 256, 0.5]  |     4778.1  |     15.5     |     4862.1   |      65.5     |   3974.5\r\n      [9, 128, 1.5]  |     7070.1  |     15.3     |     7134.2   |      65.8     |   1073.4 \r\n      [9, 256, 0.5]  |     4778.1  |     15.5     |     4862.1   |      65.5     |   3974.5\r\n      [9, 256, 1.5]  |    29078.0  |     30.2     |    32856.6   |      79.8     |   3972.6\r\n      [9, 512, 0.5]  |    19593.3  |     33.4     |    19761.1   |     117.8     |  15543.2\r\n      [9, 512, 1.5]  |   202600.8  |    147.0     |   202055.1   |     227.9     |  15562.6\r\n4 threads: --------------------------------------------------------------------------------\r\n      [1, 32, 0.5]   |       23.7  |     14.3     |       31.3   |      34.7     |     18.7\r\n      [1, 32, 1.5]   |       65.0  |     14.4     |       72.6   |      34.1     |     18.8\r\n      [1, 64, 0.5]   |       48.9  |     14.1     |       57.1   |      34.3     |     39.6\r\n      [1, 64, 1.5]   |      156.6  |     14.2     |      164.9   |      34.7     |     40.0\r\n      [1, 128, 0.5]  |      166.4  |     14.0     |      182.7   |      34.2     |    120.3\r\n      [1, 128, 1.5]  |      634.1  |     14.2     |      651.7   |      34.0     |    120.3\r\n      [1, 256, 0.5]  |      318.9  |     14.0     |      337.9   |      34.1     |    441.1\r\n      [1, 256, 1.5]  |     2394.8  |     14.3     |     2397.3   |      34.2     |    442.4\r\n      [1, 512, 0.5]  |     1121.7  |     14.3     |     1146.3   |      34.2     |   1738.7\r\n      [1, 512, 1.5]  |     9302.0  |     18.1     |     9266.6   |      34.3     |   1737.7\r\n      [2, 32, 0.5]   |       32.3  |     14.3     |       39.9   |      34.4     |     35.9\r\n      [2, 32, 1.5]   |      137.7  |     14.2     |      146.8   |      34.3     |     36.0\r\n      [2, 64, 0.5]   |       81.4  |     14.0     |       91.0   |      34.3     |     78.0\r\n      [2, 64, 1.5]   |      335.8  |     14.1     |      345.3   |      34.0     |     77.9\r\n      [2, 128, 0.5]  |      316.6  |     14.2     |      346.8   |      34.0     |    238.9\r\n      [2, 128, 1.5]  |     1219.4  |     14.4     |     1240.1   |      34.1     |    238.9\r\n      [2, 256, 0.5]  |      590.4  |     14.0     |      611.9   |      34.3     |    880.0\r\n      [2, 256, 1.5]  |     4657.5  |     14.1     |     4697.3   |      34.0     |    881.5\r\n      [2, 512, 0.5]  |     2165.1  |     14.1     |     2186.8   |      34.1     |   3466.0\r\n      [2, 512, 1.5]  |    18391.2  |     26.1     |    18398.1   |      41.8     |   3476.7\r\n      [5, 32, 0.5]   |       57.6  |     14.4     |       65.4   |      33.8     |     88.2\r\n      [5, 32, 1.5]   |      326.9  |     14.2     |      339.6   |      34.1     |     88.2\r\n      [5, 64, 0.5]   |      223.6  |     14.2     |      240.0   |      34.3     |    192.8\r\n      [5, 64, 1.5]   |      789.4  |     14.2     |      810.0   |      34.0     |    192.7\r\n      [5, 128, 0.5]  |      760.8  |     14.2     |      784.4   |      33.8     |    595.3\r\n      [5, 128, 1.5]  |     2983.4  |     14.2     |     2994.6   |      34.3     |    605.7\r\n      [5, 256, 0.5]  |     1405.6  |     14.3     |     1424.7   |      33.7     |   2210.3\r\n      [5, 256, 1.5]  |    11539.5  |     17.8     |    11553.5   |      34.2     |   2205.0\r\n      [5, 512, 0.5]  |     5361.9  |     14.0     |     5373.1   |      41.2     |   8668.3\r\n      [5, 512, 1.5]  |    61519.4  |     87.3     |    61506.9   |     113.1     |   8679.6\r\n      [9, 32, 0.5]   |       91.1  |     14.2     |       98.5   |      34.3     |    158.1\r\n      [9, 32, 1.5]   |      538.4  |     15.2     |      553.1   |      34.5     |    157.9\r\n      [9, 64, 0.5]   |      365.3  |     15.0     |      387.8   |      37.7     |    345.4\r\n      [9, 64, 1.5]   |     1388.3  |     15.2     |     1416.0   |      37.6     |    345.8\r\n      [9, 128, 0.5]  |     1284.0  |     15.4     |     1306.2   |      37.4     |   1073.4\r\n      [9, 128, 1.5]  |     5332.5  |     15.3     |     5407.2   |      37.5     |   1072.3\r\n      [9, 256, 0.5]  |     2490.2  |     15.5     |     2503.7   |      37.7     |   3969.1\r\n      [9, 256, 1.5]  |    20731.2  |     29.9     |    20729.8   |      48.7     |   3976.7\r\n      [9, 512, 0.5]  |     9657.5  |     33.4     |     9725.7   |      87.8     |  15561.7\r\n      [9, 512, 1.5]  |   113171.0  |    147.0     |   113178.5   |     197.6     |  15578.7\r\n\r\nTimes are in microseconds (us).\r\n```\r\n\r\nTo reproduce:\r\n```\r\npip install kornia opencv-python\r\ngit clone https://github.com/kornia/kornia-benchmark\r\ncd kornia-benchmark/dynamo\r\npython test_rescale.py\r\n```\r\n\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 93454, "owner": "hhaAndroid", "title": "MMDet 3.x cannot run successfully in inductor mode", "create_time": "2022-11-28T06:11:29Z", "update_time": "2023-02-01T01:02:05Z", "body": "### \ud83d\udc1b Describe the bug\n\nHi guys, I'm going to verify dynamo on mmdet 3.x branch, but I'm getting some errors\r\n\r\nEnvironmental information\r\n``` bash \r\nsys.platform: linux\r\nPython: 3.8.15 (default, Nov  4 2022, 20:59:55) [GCC 11.2.0]\r\nCUDA available: True\r\nnumpy_random_seed: 2147483648\r\nNVCC: Cuda compilation tools, release 11.7, V11.7.99\r\nGCC: gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)\r\nPyTorch: 1.14.0.dev20221123\r\nPyTorch compiling details: PyTorch built with:\r\n  - GCC 9.3\r\n  - C++ Version: 201402\r\n  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications\r\n  - Intel(R) MKL-DNN v2.7.0 (Git Hash 650085b2f3643aad05c629425983491d63b5c289)\r\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\r\n  - LAPACK is enabled (usually provided by MKL)\r\n  - NNPACK is enabled\r\n  - CPU capability usage: AVX2\r\n  - CUDA Runtime 11.7\r\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\r\n  - CuDNN 8.5\r\n  - Magma 2.6.1\r\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=1.14.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \r\n\r\nTorchVision: 0.15.0.dev20221123\r\nOpenCV: 4.6.0\r\nMMEngine: 0.3.1\r\nMMDetection: 3.0.0rc3+3f6dc71\r\n```\r\n\r\nThe installation command is as follows: \r\n\r\n```bash\r\ngit clone https://github.com/open-mmlab/mmdetection.git\r\ngit clone https://github.com/open-mmlab/mmcv.git\r\ngit clone https://github.com/C1rN09/mmengine.git\r\n\r\ncd mmengine\r\ngit checkout dynamo_1\r\npip install -e .\r\n\r\ncd ../mmcv\r\ngit checkout 2.x\r\nMMCV_WITH_OPS=1 pip install -e .\r\n\r\ncd ../mmdetection\r\ngit checkout dev-3.x\r\npip install -e .\r\n\r\npython tools/train.py configs/retinanet/retinanet_r50_fpn_1x_coco.py\r\n```\r\n\r\n## 1 eager mode\r\nI use eager mode to verify that it works successfully at first.\r\n\r\n![image](https://user-images.githubusercontent.com/17425982/204205920-f8fb4efc-bc9f-42e2-99bb-bfcd06635b0b.png)\r\n\r\nI modified the following code\uff1a` torch.Tensor -> torch.tensor.`  I got the following error after modifying the code\r\n\r\n![image (1)](https://user-images.githubusercontent.com/17425982/204205893-45dfb173-6e7d-49ef-a04b-a00a9a0770c3.png)\r\n\r\nI modified the following code\uff1a `range -> tuple(range)`.  I got the following error after modifying the code\r\n\r\n![image (2)](https://user-images.githubusercontent.com/17425982/204205972-f3c62fdc-b816-4db6-9d95-09eeed7feda6.png)\r\n\r\nRemove all `dtype` parameters and use `float()` instead\r\n-  https://github.com/open-mmlab/mmdetection/blob/dev-3.x/mmdet/models/task_modules/prior_generators/anchor_generator.py#L222\uff0c\r\n- https://github.com/open-mmlab/mmdetection/blob/dev-3.x/mmdet/models/dense_heads/base_dense_head.py#L262\r\n\r\nAt this point, it works successfully, but it is very slow and some warning \r\n\r\n![image (3)](https://user-images.githubusercontent.com/17425982/204206085-1f874bde-9a23-474e-bdd9-b2302b9de5ee.png)\r\n\r\n\r\n## 2 aot_eager mode\r\nIt can run but very slowly.\r\n\r\n## 3 inductor mode\r\nI get the error as below\r\n\r\n![image (4)](https://user-images.githubusercontent.com/17425982/204206153-e1492c9a-040d-4981-8d5d-82186ee54305.png)\r\n\r\n\r\nBy the way, I train with `MMYOLO`, which can run successfully in inductor mode with a simple modification. But it is very slow and will OOM. \r\n\r\n\r\n\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 93452, "owner": "ezyang", "title": "Dynamo is over-guarding on Tensor locals", "create_time": "2022-11-25T15:51:33Z", "update_time": "2023-02-01T01:02:02Z", "body": "### \ud83d\udc1b Describe the bug\n\ntwo for one example:\r\n\r\n```\r\nimport torch\r\nimport torch._dynamo\r\n\r\ndef g(x, y, z):\r\n    return x * 2 + 1\r\n\r\n@torch._dynamo.optimize(\"eager\")\r\ndef f(x, y, z):\r\n    print(\"woof\")\r\n    z = x * 2\r\n    z = x + 1\r\n    print(\"warf\")\r\n    return g(z, y, z)\r\n\r\nf(torch.randn(2), torch.randn(3), 4)\r\n```\r\n\r\nThe graph between woof and warf generates a TENSOR_MATCH guard on y. The function call to g generates a TENSOR_MATCH guard on y. In both cases, this guard is completely unnecessary.\r\n\r\nThe extra guards here are especially pernicious because if you have a long function body with some graph breaks, the intermediate fragments will guard on ALL of the inputs, even if the broken up subgraphs only deal with a subset of variables.\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 93445, "owner": "anijain2305", "title": "[accuracy] [aot_eager] mobilenet_v2_quantized_qat fails accuracy ", "create_time": "2022-11-23T07:21:31Z", "update_time": "2023-11-29T22:26:02Z", "body": "### \ud83d\udc1b Describe the bug\n\nI am expecting `aot_eager` to be really close to pytorch native for any model. But, `mobilenet_v2_quantized_qat` fails accuracy. Minifier is not helping here (I am going to dive deeper into this). So, a couple of questions\r\n\r\n1) Is something special about this model that can affect the accuracy?\r\n2) Long time ago - I relaxed the accuracy check for another similar model `resnet50_quantized_qat`, and used cosine similarity. But, I think its too relaxed. If I tighten the check back, this model fails accuracy as well.\r\n\r\n`python benchmarks/dynamo/torchbench.py --training --accuracy --device cuda --backend=aot_eager --float32 --only=mobilenet_v2_quantized_qat`\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_\n\ncc @ezyang @msaroufim @wconstab @bdhirsh @zou3519 @soumith @ngimel"}

{"number": 93437, "owner": "ezyang", "title": "Accuracy minifier can find spurious accuracy failures involving uninitialized memory", "create_time": "2022-11-21T20:36:26Z", "update_time": "2024-03-22T17:15:23Z", "body": "### \ud83d\udc1b Describe the bug\n\nSuppose you have a graph like\r\n\r\n```\r\ny = x.new_empty(10)\r\ny.fill_(0)\r\n```\r\n\r\nSuppose you are accuracy minifying. Removing the fill_ will cause an accuracy failure, because the uninitialized memory will trigger a difference. Oops.\r\n\r\nHere is a quick and dirty way to remove non-determinism from uninitialized CUDA memory\r\n\r\n```\r\ndiff --git a/c10/cuda/CUDACachingAllocator.cpp b/c10/cuda/CUDACachingAllocator.cpp\r\nindex 9876259522..f54e66138b 100644\r\n--- a/c10/cuda/CUDACachingAllocator.cpp\r\n+++ b/c10/cuda/CUDACachingAllocator.cpp\r\n@@ -2140,6 +2140,7 @@ class NativeCachingAllocator : public CUDAAllocator {\r\n       if (C10_UNLIKELY(interp)) {\r\n         (*interp)->trace_gpu_memory_allocation(reinterpret_cast<uintptr_t>(r));\r\n       }\r\n+      cudaMemset(r, 0, size);\r\n       return {r, r, &uncached_delete, Device(DeviceType::CUDA, device)};\r\n     }\r\n     if (size != 0) {\r\n@@ -2147,6 +2148,7 @@ class NativeCachingAllocator : public CUDAAllocator {\r\n       const_cast<NativeCachingAllocator*>(this)->malloc(\r\n           &r, device, size, cuda::getCurrentCUDAStream(device));\r\n     }\r\n+    cudaMemset(r, 0, size);\r\n     return {r, r, &local_raw_delete, Device(DeviceType::CUDA, device)};\r\n   }\r\n   DeleterFnPtr raw_deleter() const override {\r\n```\r\n\r\nNeed some sort of proper fix.\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_\n\ncc @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @soumith @wconstab @ngimel"}

{"number": 93419, "owner": "ezyang", "title": "torchdynamo is not properly setting up input tracking (e.g., for symbolic shape guards) for view bases", "create_time": "2022-11-12T05:26:58Z", "update_time": "2024-03-22T17:15:05Z", "body": "### \ud83d\udc1b Describe the bug\n\nWhen we convert a real tensor to a fake tensor, if the original tensor was a differentiable view we will also convert its base to a fake tensor, so we can construct a fake tensor that also looks like a view. In dynamic shapes, this means that we will allocate symbolic sizes for both the base as well as the view, and it turned out that in some circumstances we could end up producing guards on the base! This is problematic, because dynamo only considers the view as a new tensor inputs and only tracks its symints. The assert that fails is typically:\r\n\r\n```\r\n        assert expr_found, f\"Failed to find {expr}\"\r\n```\r\n\r\nin DynamoGuardPrinter in this case.\r\n\r\nIn [92ef26b96bf0c36e4b8ad87dc0419f46dfc65c69](https://github.com/pytorch/pytorch/pull/84246/commits/92ef26b96bf0c36e4b8ad87dc0419f46dfc65c69) we make this problem less likely to occur by suppressing guards that occur when creating fake tensors, since the naughty guard we observed in practice was caused by the `as_strided` call we use to setup the view performing a bounds check. But it would be better to fix this properly, especially since we still have \"failed to find sX\" in our tests.\r\n\r\nThe general idea is that whenever we allocate symbolic integers for a tensor, we should ALWAYS set it up in the environment. However, it is difficult to tell how many inputs a tensor will actually be (there is not only the _base field, but also the grad field); so it may be easiest if fake tensor converter communicates this information via a callback that has also says the attribute path, so we can make an appropriate source for the tensor.\r\n\r\ncc @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @wconstab @soumith @ngimel @voznesenskym \n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 88813, "owner": "peterbell10", "title": "Inductor may merge two output tensors into one", "create_time": "2022-11-10T15:09:35Z", "update_time": "2024-03-26T19:40:11Z", "body": "### \ud83d\udc1b Describe the bug\n\nConsider the following example:\r\n```python\r\nimport torch._dynamo\r\nimport logging\r\n\r\ndef g(a):\r\n    b = a * 2\r\n    c = a * 2\r\n    return b, c\r\n\r\nx = torch.rand((1000000,), device=\"cuda\", requires_grad=True)\r\nexpect = g(x)\r\nactual = torch._dynamo.optimize(\"inductor\")(g)(x)\r\nassert expect[0] is not expect[1]\r\nassert actual[0] is actual[1]\r\n```\r\n\r\nThe outputs have been merged into a single tensor, so downstream users of this function may get silently wrong results if `a` or `b` are mutated.\n\n### Versions\n\n```\r\nPyTorch version: 1.14.0a0+git034872d\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.7\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.1 LTS (x86_64)\r\nGCC version: (conda-forge gcc 9.5.0-17) 9.5.0\r\nClang version: Could not collect\r\nCMake version: version 3.24.1\r\n\r\nPython version: 3.8 (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 2060\r\nGPU 1: NVIDIA GeForce RTX 2060\r\n\r\nNvidia driver version: 515.65.01\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn.so.8\r\n/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\r\n/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\r\n/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\r\n/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\r\n/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\r\n/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n```\n\ncc @ezyang @msaroufim @bdhirsh @anijain2305 @zou3519 @chauhang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @amjames @desertfire @mlazos @soumith @yanboliang @chunyuan-w @Xia-Weiwen"}

{"number": 93609, "owner": "ezyang", "title": "Don't store example_value on FX node meta", "create_time": "2022-11-04T15:41:12Z", "update_time": "2023-02-01T01:07:12Z", "body": "### \ud83d\udc1b Describe the bug\n\nStoring it in the meta dict implies that we will pass it to the backend compiler. But actually we strip all of the example values from the FX graph when we're done. So this is really just an internal concept to Dynamo. I propose we store it on TensorVariable instead.\r\n\r\nBTW, you don't want to actually pass this metadata on to the backend, because in the presence of metadata mutation it can be misleading. E.g.:\r\n\r\n```\r\n@torch._dynamo.optimize(my_backend)\r\ndef f(x):\r\n    y = x.clone()\r\n    y.transpose_(0, 1)\r\n    return y\r\n\r\nf(torch.randn(4, 5))\r\n```\r\n\r\nWhat do you expect the example value of y to be?\r\n\r\ncc @eellison @Chillee @voznesenskym @jansel \n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 93604, "owner": "yanboliang", "title": "TorchBench - moco - RuntimeError: Tensors must be CUDA and dense", "create_time": "2022-11-02T18:28:13Z", "update_time": "2023-02-01T01:07:06Z", "body": "### \ud83d\udc1b Describe the bug\n\nRun TB model moco\r\n```\r\nbenchmarks/dynamo/torchbench.py -d cuda --inductor --training --float32 --accuracy --no-skip --only moco\r\n```\n\n### Error logs\n\n```\r\ncuda train moco                               TorchDynamo optimized model failed to run because of following error\r\nERROR:common:\r\nfrom user code:\r\n   File \"/scratch/ybliang/work/repos/torchbenchmark/torchbenchmark/models/moco/moco/builder.py\", line 172, in <graph break in concat_all_gather>\r\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\r\nSet torch._dynamo.config.verbose=True for more information\r\nYou can suppress this exception and fall back to eager by setting:\r\n    torchdynamo.config.suppress_errors = True\r\n==========\r\nTraceback (most recent call last):\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/tensor.py\", line 131, in _get_fake_value\r\n    return wrap_fake_exception(\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/utils.py\", line 725, in wrap_fake_exception\r\n    return fn()\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/tensor.py\", line 132, in <lambda>\r\n    lambda: _run_node(tx.output, node, args, kwargs, nnmodule)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/tensor.py\", line 51, in _run_node\r\n    return node.target(*args, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/distributed/distributed_c10d.py\", line 1318, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/distributed/distributed_c10d.py\", line 2313, in all_gather\r\n    work = default_pg.allgather([tensor_list], [tensor])\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_subclasses/fake_tensor.py\", line 894, in __torch_dispatch__\r\n    r = func(*args, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_ops.py\", line 257, in __call__\r\n    return self._op(*args, **kwargs or {})\r\nRuntimeError: Tensors must be CUDA and dense\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"/scratch/ybliang/work/repos/pytorch/benchmarks/dynamo/common.py\", line 1095, in check_accuracy\r\n    new_result = optimized_model_iter_fn(model, example_inputs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/eval_frame.py\", line 157, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/benchmarks/dynamo/common.py\", line 999, in run_n_iterations\r\n    self.model_iter_fn(mod, inputs, collect_outputs=False)\r\n  File \"/scratch/ybliang/work/repos/pytorch/benchmarks/dynamo/torchbench.py\", line 332, in forward_and_backward_pass\r\n    cloned_inputs = clone_inputs(inputs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/benchmarks/dynamo/torchbench.py\", line 335, in <graph break in forward_and_backward_pass>\r\n    pred = mod(*cloned_inputs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/nn/modules/module.py\", line 1423, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/nn/parallel/distributed.py\", line 1093, in forward\r\n    output = self._run_ddp_forward(*inputs, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/nn/parallel/distributed.py\", line 1047, in _run_ddp_forward\r\n    return module_to_run(*inputs[0], **kwargs[0])\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/nn/modules/module.py\", line 1423, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/torchbenchmark/torchbenchmark/models/moco/moco/builder.py\", line 130, in forward\r\n    self._momentum_update_key_encoder()  # update the key encoder\r\n  File \"/scratch/ybliang/work/repos/torchbenchmark/torchbenchmark/models/moco/moco/builder.py\", line 133, in <graph break in forward>\r\n    im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/torchbenchmark/torchbenchmark/models/moco/moco/builder.py\", line 76, in _batch_shuffle_ddp\r\n    x_gather = concat_all_gather(x)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/torchbenchmark/torchbenchmark/models/moco/moco/builder.py\", line 171, in concat_all_gather\r\n    for _ in range(torch.distributed.get_world_size())]\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/eval_frame.py\", line 236, in catch_errors\r\n    return callback(frame, cache_size)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/convert_frame.py\", line 466, in _convert_frame\r\n    result = inner_convert(frame, cache_size)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/convert_frame.py\", line 118, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/utils.py\", line 92, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/convert_frame.py\", line 339, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/convert_frame.py\", line 394, in _compile\r\n    out_code = transform_code_object(code, transform)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/bytecode_transformation.py\", line 341, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/convert_frame.py\", line 382, in transform\r\n    tracer.run()\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py\", line 1452, in run\r\n    super().run()\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py\", line 352, in run\r\n    and self.step()\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py\", line 322, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py\", line 174, in wrapper\r\n    return inner_fn(self, inst)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py\", line 807, in CALL_FUNCTION_KW\r\n    self.call_function(fn, args, kwargs)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py\", line 264, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/torch.py\", line 381, in call_function\r\n    tensor_variable = TensorVariable.create(\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/tensor.py\", line 201, in create\r\n    example_value = _get_fake_value(proxy.node, tx)\r\n  File \"/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/tensor.py\", line 145, in _get_fake_value\r\n    raise TorchRuntimeError() from e\r\ntorch._dynamo.exc.TorchRuntimeError: \r\nfrom user code:\r\n   File \"/scratch/ybliang/work/repos/torchbenchmark/torchbenchmark/models/moco/moco/builder.py\", line 172, in <graph break in concat_all_gather>\r\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\r\nSet torch._dynamo.config.verbose=True for more information\r\nYou can suppress this exception and fall back to eager by setting:\r\n    torchdynamo.config.suppress_errors = True\r\n==========\r\nFAIL\r\n```\r\n\r\nLook at ```\"/scratch/ybliang/work/repos/torchbenchmark/torchbenchmark/models/moco/moco/builder.py\", line 172```, the failure op is ```c10d.allgather_.default```. It seems ```moco``` model explicitly calling these ops rather than calling them through DDP. We have to think about how to support such case.\n\n### Minified repro\n\n_No response_"}

{"number": 93602, "owner": "mreso", "title": "Diffuser pipeline device attribute broken when using optimized model", "create_time": "2022-11-02T00:28:57Z", "update_time": "2023-02-01T01:07:04Z", "body": "### \ud83d\udc1b Describe the bug\n\nHi,\r\n\r\njust adding this for tracking purposes as a solution is potentially provided by [this PR](https://github.com/pytorch/pytorch/pull/88164).\r\n\r\nThe issue is that when using a compiled model in a [DDPMPipeline](https://github.com/huggingface/diffusers/blob/v0.6.0/src/diffusers/pipelines/ddpm/pipeline_ddpm.py#L24) the self.device property does not give the correct result. It [always return \"cpu\"](https://github.com/huggingface/diffusers/blob/ad9d7ce4763f8fb2a9e620bff017830c26086c36/src/diffusers/pipeline_utils.py#L213) as the optimized model class does not inherent from nn.Module.\r\n\r\nHere is a simple repro:\r\n```\r\nimport torch\r\nimport torch._dynamo as dynamo\r\nfrom diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\r\n\r\nmodel = UNet2DModel(\r\n    sample_size=128,  # the target image resolution\r\n    in_channels=3,  # the number of input channels, 3 for RGB images\r\n    out_channels=3,  # the number of output channels\r\n    layers_per_block=2,  # how many ResNet layers to use per UNet block\r\n    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\r\n    down_block_types=( \r\n        \"DownBlock2D\",  # a regular ResNet downsampling block\r\n        \"DownBlock2D\", \r\n        \"DownBlock2D\", \r\n        \"DownBlock2D\", \r\n        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\r\n        \"DownBlock2D\",\r\n    ), \r\n    up_block_types=(\r\n        \"UpBlock2D\",  # a regular ResNet upsampling block\r\n        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\r\n        \"UpBlock2D\", \r\n        \"UpBlock2D\", \r\n        \"UpBlock2D\", \r\n        \"UpBlock2D\"  \r\n      ),\r\n)\r\n\r\nif torch.cuda.is_available():\r\n    model.to('cuda')\r\n\r\nmodel = torch._dynamo.optimize('inductor')(model)\r\n\r\nnoise_scheduler = DDPMScheduler(num_train_timesteps=1000)\r\n\r\npipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\r\n\r\nif torch.cuda.is_available():\r\n    pipeline.to('cuda')\r\n\r\npipeline(\r\n    batch_size = 16, \r\n    generator=torch.manual_seed(42),\r\n    )\r\n```\r\n\r\nPR [88164](https://github.com/pytorch/pytorch/pull/88164) will solve this as it makes a compiled model a nn.Module.\n\n### Error logs\n\n```\r\nTraceback (most recent call last):\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/variables/tensor.py\", line 131, in _get_fake_value\r\n    return wrap_fake_exception(\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/utils.py\", line 725, in wrap_fake_exception\r\n    return fn()\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/variables/tensor.py\", line 132, in <lambda>\r\n    lambda: _run_node(tx.output, node, args, kwargs, nnmodule)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/variables/tensor.py\", line 56, in _run_node\r\n    return nnmodule(*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1423, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 860, in __torch_dispatch__\r\n    return decomposition_table[func](*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_prims_common/wrappers.py\", line 212, in _fn\r\n    result = fn(*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_decomp/decompositions.py\", line 63, in inner\r\n    r = f(*tree_map(increase_prec, args), **tree_map(increase_prec, kwargs))\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_decomp/decompositions.py\", line 1111, in addmm\r\n    out = alpha * torch.mm(mat1, mat2)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 901, in __torch_dispatch__\r\n    return self.wrap_meta_outputs_with_default_device_logic(r, func, args, kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 934, in wrap_meta_outputs_with_default_device_logic\r\n    return tree_map(partial(wrap), r)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/utils/_pytree.py\", line 195, in tree_map\r\n    return tree_unflatten([fn(i) for i in flat_args], spec)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/utils/_pytree.py\", line 195, in <listcomp>\r\n    return tree_unflatten([fn(i) for i in flat_args], spec)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 951, in wrap\r\n    ) = FakeTensor._find_common_device(func, args, kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 650, in _find_common_device\r\n    tree_map(merge_devices, args)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/utils/_pytree.py\", line 195, in tree_map\r\n    return tree_unflatten([fn(i) for i in flat_args], spec)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/utils/_pytree.py\", line 195, in <listcomp>\r\n    return tree_unflatten([fn(i) for i in flat_args], spec)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_subclasses/fake_tensor.py\", line 646, in merge_devices\r\n    raise RuntimeError(\r\nRuntimeError: Unhandled FakeTensor Device Propagation for aten.mm.default, found two different devices cpu, cuda:0\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/home/mreso/torchdynamo/diffuser_training/train_repro.py\", line 46, in <module>\r\n    pipeline(\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/diffusers/pipelines/ddpm/pipeline_ddpm.py\", line 84, in __call__\r\n    model_output = self.unet(image, t).sample\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 137, in __call__\r\n    return self.forward(*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 134, in forward\r\n    return optimized_forward(*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 157, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 236, in catch_errors\r\n    return callback(frame, cache_size)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 466, in _convert_frame\r\n    result = inner_convert(frame, cache_size)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 118, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/utils.py\", line 92, in time_wrapper\r\n    r = func(*args, **kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 339, in _convert_frame_assert\r\n    return _compile(\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 394, in _compile\r\n    out_code = transform_code_object(code, transform)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py\", line 341, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py\", line 382, in transform\r\n    tracer.run()\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 1452, in run\r\n    super().run()\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 352, in run\r\n    and self.step()\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 322, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 174, in wrapper\r\n    return inner_fn(self, inst)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 758, in CALL_FUNCTION\r\n    self.call_function(fn, args, {})\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 264, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/variables/nn_module.py\", line 221, in call_function\r\n    return tx.inline_user_function_return(\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 293, in inline_user_function_return\r\n    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 1524, in inline_call\r\n    return cls.inline_call_(parent, func, args, kwargs)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 1578, in inline_call_\r\n    tracer.run()\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 352, in run\r\n    and self.step()\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 322, in step\r\n    getattr(self, inst.opname)(inst)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 174, in wrapper\r\n    return inner_fn(self, inst)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 758, in CALL_FUNCTION\r\n    self.call_function(fn, args, {})\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py\", line 264, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/variables/nn_module.py\", line 201, in call_function\r\n    return variables.TensorVariable.create(\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/variables/tensor.py\", line 201, in create\r\n    example_value = _get_fake_value(proxy.node, tx)\r\n  File \"/fsx/users/mreso/conda/envs/tdn/lib/python3.9/site-packages/torch/_dynamo/variables/tensor.py\", line 145, in _get_fake_value\r\n    raise TorchRuntimeError() from e\r\ntorch._dynamo.exc.TorchRuntimeError:\r\n\r\nSet torch._dynamo.config.verbose=True for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    torchdynamo.config.suppress_errors = True\r\n==========\r\n```\n\n### Minified repro\n\nMinifier does not work here but I am not sure if it should in these cases."}

{"number": 93596, "owner": "ezyang", "title": "Minifier doesn't work on DebertaForQuestionAnswering", "create_time": "2022-10-30T02:31:47Z", "update_time": "2024-08-22T00:11:19Z", "body": "### \ud83d\udc1b Describe the bug\n\nBuild pytorch e4a8661ab84022c1bff622c6d2f6e679180b1df5\r\n\r\nApply this patch\r\n\r\n```\r\ndiff --git a/torch/_dynamo/optimizations/backends.py b/torch/_dynamo/optimizations/backends.py\r\nindex 660e7a5ca56..d53733bec73 100644\r\n--- a/torch/_dynamo/optimizations/backends.py\r\n+++ b/torch/_dynamo/optimizations/backends.py\r\n@@ -552,7 +552,8 @@ def cudagraphs_inner(model, inputs, copy_outputs=True):\r\n def aot_autograd(subgraph, **kwargs):\r\n     def _wrapped_bw_compiler(*args, **kwargs):\r\n         # stop TorchDynamo from trying to compile our generated backwards pass\r\n-        return disable(bw_compiler(*args, **kwargs))\r\n+        bwd_func = disable(bw_compiler)(*args, **kwargs)\r\n+        return disable(bwd_func)\r\n \r\n     bw_compiler = kwargs.get(\"bw_compiler\") or kwargs[\"fw_compiler\"]\r\n     kwargs[\"bw_compiler\"] = _wrapped_bw_compiler\r\n```\r\n\r\nDeberta failures accuracy with aot_eager, but the minifier doesn't do anything\r\n\r\n```\r\n$ pp PYTHONUNBUFFERED=1 TORCHDYNAMO_REPRO_AFTER=\"dynamo\" TORCHDYNAMO_REPRO_LEVEL=4  python benchmarks/dynamo/huggingface.py  --accuracy --backend aot_eager --training --only DebertaForQuestionAnswering\r\n[2022-10-29 19:30:33,226] torch._dynamo.testing: [WARNING] High loss value alert - 6.63. Can result in unstable gradients.\r\ncuda train DebertaForQuestionAnswering        [2022-10-29 19:30:33,509] torch._dynamo.testing: [WARNING] High loss value alert - 6.63. Can result in unstable gradients.\r\n[2022-10-29 19:30:33,626] torch._dynamo.testing: [WARNING] High loss value alert - 6.63. Can result in unstable gradients.\r\n[2022-10-29 19:30:33,731] torch._dynamo.testing: [WARNING] High loss value alert - 6.63. Can result in unstable gradients.\r\n[2022-10-29 19:30:54,042] torch._dynamo.testing: [WARNING] High loss value alert - 6.63. Can result in unstable gradients.\r\n[2022-10-29 19:30:54,050] torch._dynamo.utils: [ERROR] RMSE (res-fp64): 0.00016, (ref-fp64): 0.00000 and shape=torch.Size([50265, 768])\r\n[2022-10-29 19:30:54,050] torch._dynamo.utils: [ERROR] Accuracy failed for key name deberta.embeddings.word_embeddings.weight.grad\r\n```\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

{"number": 93592, "owner": "ezyang", "title": "Turning on minifier causes bug to go away (on DebertaForMaskedLM)", "create_time": "2022-10-30T00:37:29Z", "update_time": "2024-08-22T00:11:07Z", "body": "### \ud83d\udc1b Describe the bug\n\n1. Check out and build pytorch at e4a8661ab84022c1bff622c6d2f6e679180b1df5 (Oct 29, before breaking commit was reverted)\r\n2. Run `python benchmarks/dynamo/huggingface.py  --accuracy --backend inductor --training --only DebertaForMaskedLM`, it fails\r\n3. Run `TORCHDYNAMO_REPRO_AFTER=dynamo python benchmarks/dynamo/huggingface.py  --accuracy --backend inductor --training --only DebertaForMaskedLM`, it passes\n\n### Error logs\n\nhttps://www.internalfb.com/intern/paste/P543371878/\n\n### Minified repro\n\nminifier did not work"}

{"number": 93580, "owner": "ezyang", "title": "AssertionError: Unknown expression s2", "create_time": "2022-10-26T13:50:52Z", "update_time": "2023-02-01T01:06:36Z", "body": "### \ud83d\udc1b Describe the bug\n\n```\r\nTORCHDYNAMO_DYNAMIC_SHAPES=1 AOT_DYNAMIC_SHAPES=0 time python benchmarks/dynamo/huggingface.py  --accuracy --backend eager --training --only AllenaiLongformerBase\r\n```\r\n\r\non master\r\n\r\n```\r\n  File \"/private/home/ezyang/.local/lib/python3.8/site-packages/sympy/printing/printer.py\", line 331, in _print\r\n    return printmethod(expr, **kwargs)\r\n  File \"/private/home/ezyang/.local/lib/python3.8/site-packages/sympy/printing/str.py\", line 779, in _print_Relational\r\n    return '%s(%s, %s)' % (charmap[expr.rel_op], self._print(expr.lhs),\r\n  File \"/private/home/ezyang/.local/lib/python3.8/site-packages/sympy/printing/printer.py\", line 331, in _print    return printmethod(expr, **kwargs)\r\n  File \"/raid/ezyang/pytorch-scratch2/torch/_dynamo/guards.py\", line 526, in _print_Symbol\r\n    assert expr in self.expr_to_tensor_ref, f\"Unknown expression {expr}\"\r\nAssertionError: Unknown expression s2\r\n```\n\n### Error logs\n\n_No response_\n\n### Minified repro\n\n_No response_"}

