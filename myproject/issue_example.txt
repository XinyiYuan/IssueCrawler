{"number": 134062, "owner": "wconstab", "title": "TORCH_NCCL_NAN_CHECK fails in certain scenarios", "body": "### Repro 1: IMA during PP points to Bug1 in certain collectives/ P2P\r\ngives an Illegal Memory Access: `CUDA_LAUNCH_BLOCKING=1 TORCH_NCCL_NAN_CHECK=1 python test/distributed/pipelining/test_schedule_multiproc.py -k test_forward_only_ScheduleClass0`\r\n\r\n<details><summary>Test output showing 'irecv' failing DSA</summary>\r\n======================================================================                                                                                                                        \r\nERROR: test_forward_only_ScheduleClass0 (__mp_main__.ScheduleTest)                                                                                                                            \r\n----------------------------------------------------------------------                                                                                                                        \r\nTraceback (most recent call last):                                                                                                                                                            \r\n  File \"/data/users/whc/pytorch/torch/testing/_internal/common_utils.py\", line 2918, in wrapper                                                                                               \r\n    method(*args, **kwargs)                                                                                                                                                                   \r\n  File \"/data/users/whc/pytorch/torch/testing/_internal/common_utils.py\", line 529, in instantiated_test                                                                                      \r\n    test(self, **param_kwargs)                                                                                                                                                                \r\n  File \"/data/users/whc/pytorch/test/distributed/pipelining/test_schedule_multiproc.py\", line 100, in test_forward_only                                                                       \r\n    out = schedule.step()                                                                                                                                                                     \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 694, in step                                                                                                 \r\n    self._step_microbatches(args_split, kwargs_split, targets_split, losses)                                                                                                                  \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 733, in _step_microbatches                                                                                   \r\n    works = _sorted_batch_p2p(ops, desc=\"fwd_recv\")                                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 628, in _sorted_batch_p2p                                                                                    \r\n    work_by_peer[peer] = _batch_p2p(ops, desc=desc)                                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 603, in _batch_p2p                                                                                           \r\n    return dist.batch_isend_irecv(p2p_ops).pop()                                                                                                                                              \r\n  File \"/data/users/whc/pytorch/torch/distributed/distributed_c10d.py\", line 2374, in batch_isend_irecv                                                                                       \r\n    p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/distributed_c10d.py\", line 2109, in irecv                                                                                                   \r\n    return pg.recv([tensor], src, tag)                                                                                                                                                        \r\nRuntimeError: CUDA error: an illegal memory access was encountered                                                                                                                            \r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                                                                                  </details>\r\n\r\nThe error above made me wonder, are we checking for NAN on the \"input\" tensor to operations that don't have to have valid inputs?  (E.g. recv takes 'tensor' as input, but then writes to it.  the values in 'tensor' can be garbage at the beginning- checking for NAN would be pointless).\r\n\r\nIndeed, we are checking input tensors to all pointToPoint() and collective() without consideration of whether they are 'real' inputs or 'output placeholders'.  This is a definite bug.\r\n\r\n### **Bug 1**: we check for NAN even on 'output' inputs.  We should only check tensors that are true inputs, depending on the type of operation.\r\n\r\nHowever, a simple fix to avoid the nan check on RECV ops isn't enough.  It does fix the exact error above, but presents a new (more confusing) one:\r\n\r\nNote: a complete fix for this bug needs to also gate nan checking on certain collectives- e.g. for scatter, only one rank has a valid input, others should be ignored.\r\n\r\n<details><summary>Fix (only nancheck for OpType::SEND)</summary>```\r\ndiff --git a/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp b/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\nindex 5970e489abd..94a40d0c0fc 100644\r\n--- a/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\n+++ b/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\n@@ -2964,7 +2964,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\r\n     PreProcess pre,\r\n     PostProcess post,\r\n     const char* profilingTitle) {\r\n-  if (enableNanCheck_) {\r\n+  if (opType == OpType::SEND && enableNanCheck_) {\r\n     checkForNan(tensor);\r\n   }\r\n```</details>\r\n\r\n<details><summary>New error complaining about garbage during `~TensorImpl`</summary>\r\nEterminate called after throwing an instance of 'c10::Error'                                                                                                                                  \r\n  what():  CUDA error: an illegal memory access was encountered                                                                                                                               \r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                                                                                           \r\n                                                                                                                                                                                              \r\nException raised from c10_cuda_check_implementation at /data/users/whc/pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):                                                        \r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xf6 (0x7fa99339a206 in /data/users/whc/pytorch/torch/lib/\r\nlibc10.so)                                                                                                                                                                                    \r\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xd3 (0x7fa9933984e3 i\r\nn /data/users/whc/pytorch/torch/lib/libc10.so)                                                                                                                                                \r\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x139 (0x7fa9947700f9 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                     \r\nframe #3: <unknown function> + 0x247ba (0x7fa99473e7ba in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #4: <unknown function> + 0x23050 (0x7fa99473d050 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #5: <unknown function> + 0x3b66a (0x7fa99475566a in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                 \r\nframe #6: <unknown function> + 0x16565 (0x7fa994730565 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #7: <unknown function> + 0x74d0b6 (0x7fa989f4d0b6 in /data/users/whc/pytorch/torch/lib/libtorch_python.so)                                                                              \r\nframe #8: c10::TensorImpl::~TensorImpl() + 0xdf (0x7fa99337867f in /data/users/whc/pytorch/torch/lib/libc10.so)                                                                               \r\nframe #9: c10::TensorImpl::~TensorImpl() + 0xa (0x7fa99337879a in /data/users/whc/pytorch/torch/lib/libc10.so)                                                            \r\nframe #10: torch::autograd::SavedVariable::reset_data() + 0xbc (0x7fa976ba454c in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                          \r\nframe #11: <unknown function> + 0x3e0e61e (0x7fa97580e61e in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #12: <unknown function> + 0x5173107 (0x7fa976b73107 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #13: torch::autograd::deleteNode(torch::autograd::Node*) + 0x138 (0x7fa976b73038 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                              \r\nframe #14: <unknown function> + 0x40ad57b (0x7fa975aad57b in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #15: <unknown function> + 0xc6ddea (0x7fa97266ddea in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)\r\nframe #16: <unknown function> + 0x5153536 (0x7fa976b53536 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #17: <unknown function> + 0x51535b9 (0x7fa976b535b9 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                       \r\nframe #18: c10::TensorImpl::~TensorImpl() + 0x99 (0x7fa993378639 in /data/users/whc/pytorch/torch/lib/libc10.so)                                                                    \r\nframe #19: c10::TensorImpl::~TensorImpl() + 0xa (0x7fa99337879a in /data/users/whc/pytorch/torch/lib/libc10.so)                            \r\nframe #20: <unknown function> + 0x8256a1 (0x7fa98a0256a1 in /data/users/whc/pytorch/torch/lib/libtorch_python.so)                                                                     \r\nframe #21: THPVariable_subclass_dealloc(_object*) + 0x18e (0x7fa98a025b3e in /data/users/whc/pytorch/torch/lib/libtorch_python.so)           \r\n</details>\r\n\r\n### Repro2 shows another IMA that appears unrelated to Bug1\r\n<details><summary>Repro 2</summary>\r\nRun via `CUDA_LAUNCH_BLOCKING=1 torchrun --nproc_per_node 2 repro.py`\r\n\r\n```\r\nimport torch\r\nimport os\r\nimport torch.distributed.distributed_c10d as c10d\r\n\r\ndef repro(rank, world_size):\r\n    os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\r\n    device = torch.device(\"cuda:%d\" % rank)\r\n    c10d.init_process_group(\r\n        backend=\"nccl\", rank=rank, world_size=world_size\r\n    )\r\n    \r\n    x = torch.ones((10,), dtype=torch.float32, device=device)\r\n    c10d.all_reduce(x)\r\n    print(f\"After first allreduce: {x=}\")\r\n    c10d.all_reduce(x)\r\n    print(f\"After second allreduce: {x=}\")\r\n    torch.cuda.synchronize()\r\n    c10d.destroy_process_group()\r\n    print(\"clean exit\")\r\n\r\nif __name__ == \"__main__\":\r\n    repro(int(os.environ[\"RANK\"]), int(os.environ[\"WORLD_SIZE\"]))\r\n```\r\n</details>\r\n\r\nRepro 2 should avoid the issue of the 'output' inputs being used becuase it only uses AllReduce operator, for which all inputs are valid inputs on all ranks.  A few notable points for this repro:\r\n\r\n* having only 1 allreduce, things pass\r\n* adding extra cuda synchronize() calls and print statements does not change anything, but i tried them as an unsuccessful way to narrow down the error\r\n* adding the second allreduce (and nan-check) causes an IMA error with a stack trace similar to the one above- implicating deallocation and `~TensorImpl`\r\ncc @XilunWu @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @d4l3k @c-p-i-o"}

{"number": 134062, "owner": "wconstab", "title": "TORCH_NCCL_NAN_CHECK fails in certain scenarios", "body": "### Repro 1: IMA during PP points to Bug1 in certain collectives/ P2P\r\ngives an Illegal Memory Access: `CUDA_LAUNCH_BLOCKING=1 TORCH_NCCL_NAN_CHECK=1 python test/distributed/pipelining/test_schedule_multiproc.py -k test_forward_only_ScheduleClass0`\r\n\r\n<details><summary>Test output showing 'irecv' failing DSA</summary>\r\n======================================================================                                                                                                                        \r\nERROR: test_forward_only_ScheduleClass0 (__mp_main__.ScheduleTest)                                                                                                                            \r\n----------------------------------------------------------------------                                                                                                                        \r\nTraceback (most recent call last):                                                                                                                                                            \r\n  File \"/data/users/whc/pytorch/torch/testing/_internal/common_utils.py\", line 2918, in wrapper                                                                                               \r\n    method(*args, **kwargs)                                                                                                                                                                   \r\n  File \"/data/users/whc/pytorch/torch/testing/_internal/common_utils.py\", line 529, in instantiated_test                                                                                      \r\n    test(self, **param_kwargs)                                                                                                                                                                \r\n  File \"/data/users/whc/pytorch/test/distributed/pipelining/test_schedule_multiproc.py\", line 100, in test_forward_only                                                                       \r\n    out = schedule.step()                                                                                                                                                                     \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 694, in step                                                                                                 \r\n    self._step_microbatches(args_split, kwargs_split, targets_split, losses)                                                                                                                  \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 733, in _step_microbatches                                                                                   \r\n    works = _sorted_batch_p2p(ops, desc=\"fwd_recv\")                                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 628, in _sorted_batch_p2p                                                                                    \r\n    work_by_peer[peer] = _batch_p2p(ops, desc=desc)                                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/pipelining/schedules.py\", line 603, in _batch_p2p                                                                                           \r\n    return dist.batch_isend_irecv(p2p_ops).pop()                                                                                                                                              \r\n  File \"/data/users/whc/pytorch/torch/distributed/distributed_c10d.py\", line 2374, in batch_isend_irecv                                                                                       \r\n    p2p_op.op(p2p_op.tensor, p2p_op.peer, p2p_op.group, p2p_op.tag)                                                                                                                           \r\n  File \"/data/users/whc/pytorch/torch/distributed/distributed_c10d.py\", line 2109, in irecv                                                                                                   \r\n    return pg.recv([tensor], src, tag)                                                                                                                                                        \r\nRuntimeError: CUDA error: an illegal memory access was encountered                                                                                                                            \r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                                                                                  </details>\r\n\r\nThe error above made me wonder, are we checking for NAN on the \"input\" tensor to operations that don't have to have valid inputs?  (E.g. recv takes 'tensor' as input, but then writes to it.  the values in 'tensor' can be garbage at the beginning- checking for NAN would be pointless).\r\n\r\nIndeed, we are checking input tensors to all pointToPoint() and collective() without consideration of whether they are 'real' inputs or 'output placeholders'.  This is a definite bug.\r\n\r\n### **Bug 1**: we check for NAN even on 'output' inputs.  We should only check tensors that are true inputs, depending on the type of operation.\r\n\r\nHowever, a simple fix to avoid the nan check on RECV ops isn't enough.  It does fix the exact error above, but presents a new (more confusing) one:\r\n\r\nNote: a complete fix for this bug needs to also gate nan checking on certain collectives- e.g. for scatter, only one rank has a valid input, others should be ignored.\r\n\r\n<details><summary>Fix (only nancheck for OpType::SEND)</summary>```\r\ndiff --git a/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp b/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\nindex 5970e489abd..94a40d0c0fc 100644\r\n--- a/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\n+++ b/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\r\n@@ -2964,7 +2964,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::pointToPoint(\r\n     PreProcess pre,\r\n     PostProcess post,\r\n     const char* profilingTitle) {\r\n-  if (enableNanCheck_) {\r\n+  if (opType == OpType::SEND && enableNanCheck_) {\r\n     checkForNan(tensor);\r\n   }\r\n```</details>\r\n\r\n<details><summary>New error complaining about garbage during `~TensorImpl`</summary>\r\nEterminate called after throwing an instance of 'c10::Error'                                                                                                                                  \r\n  what():  CUDA error: an illegal memory access was encountered                                                                                                                               \r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.                                                                                                                           \r\n                                                                                                                                                                                              \r\nException raised from c10_cuda_check_implementation at /data/users/whc/pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):                                                        \r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xf6 (0x7fa99339a206 in /data/users/whc/pytorch/torch/lib/\r\nlibc10.so)                                                                                                                                                                                    \r\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xd3 (0x7fa9933984e3 i\r\nn /data/users/whc/pytorch/torch/lib/libc10.so)                                                                                                                                                \r\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x139 (0x7fa9947700f9 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                     \r\nframe #3: <unknown function> + 0x247ba (0x7fa99473e7ba in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #4: <unknown function> + 0x23050 (0x7fa99473d050 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #5: <unknown function> + 0x3b66a (0x7fa99475566a in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                 \r\nframe #6: <unknown function> + 0x16565 (0x7fa994730565 in /data/users/whc/pytorch/torch/lib/libc10_cuda.so)                                                                                   \r\nframe #7: <unknown function> + 0x74d0b6 (0x7fa989f4d0b6 in /data/users/whc/pytorch/torch/lib/libtorch_python.so)                                                                              \r\nframe #8: c10::TensorImpl::~TensorImpl() + 0xdf (0x7fa99337867f in /data/users/whc/pytorch/torch/lib/libc10.so)                                                                               \r\nframe #9: c10::TensorImpl::~TensorImpl() + 0xa (0x7fa99337879a in /data/users/whc/pytorch/torch/lib/libc10.so)                                                            \r\nframe #10: torch::autograd::SavedVariable::reset_data() + 0xbc (0x7fa976ba454c in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                          \r\nframe #11: <unknown function> + 0x3e0e61e (0x7fa97580e61e in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #12: <unknown function> + 0x5173107 (0x7fa976b73107 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #13: torch::autograd::deleteNode(torch::autograd::Node*) + 0x138 (0x7fa976b73038 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                              \r\nframe #14: <unknown function> + 0x40ad57b (0x7fa975aad57b in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #15: <unknown function> + 0xc6ddea (0x7fa97266ddea in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)\r\nframe #16: <unknown function> + 0x5153536 (0x7fa976b53536 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                               \r\nframe #17: <unknown function> + 0x51535b9 (0x7fa976b535b9 in /data/users/whc/pytorch/torch/lib/libtorch_cpu.so)                                                                       \r\nframe #18: c10::TensorImpl::~TensorImpl() + 0x99 (0x7fa993378639 in /data/users/whc/pytorch/torch/lib/libc10.so)                                                                    \r\nframe #19: c10::TensorImpl::~TensorImpl() + 0xa (0x7fa99337879a in /data/users/whc/pytorch/torch/lib/libc10.so)                            \r\nframe #20: <unknown function> + 0x8256a1 (0x7fa98a0256a1 in /data/users/whc/pytorch/torch/lib/libtorch_python.so)                                                                     \r\nframe #21: THPVariable_subclass_dealloc(_object*) + 0x18e (0x7fa98a025b3e in /data/users/whc/pytorch/torch/lib/libtorch_python.so)           \r\n</details>\r\n\r\n### Repro2 shows another IMA that appears unrelated to Bug1\r\n<details><summary>Repro 2</summary>\r\nRun via `CUDA_LAUNCH_BLOCKING=1 torchrun --nproc_per_node 2 repro.py`\r\n\r\n```\r\nimport torch\r\nimport os\r\nimport torch.distributed.distributed_c10d as c10d\r\n\r\ndef repro(rank, world_size):\r\n    os.environ[\"TORCH_NCCL_NAN_CHECK\"] = \"1\"\r\n    device = torch.device(\"cuda:%d\" % rank)\r\n    c10d.init_process_group(\r\n        backend=\"nccl\", rank=rank, world_size=world_size\r\n    )\r\n    \r\n    x = torch.ones((10,), dtype=torch.float32, device=device)\r\n    c10d.all_reduce(x)\r\n    print(f\"After first allreduce: {x=}\")\r\n    c10d.all_reduce(x)\r\n    print(f\"After second allreduce: {x=}\")\r\n    torch.cuda.synchronize()\r\n    c10d.destroy_process_group()\r\n    print(\"clean exit\")\r\n\r\nif __name__ == \"__main__\":\r\n    repro(int(os.environ[\"RANK\"]), int(os.environ[\"WORLD_SIZE\"]))\r\n```\r\n</details>\r\n\r\nRepro 2 should avoid the issue of the 'output' inputs being used becuase it only uses AllReduce operator, for which all inputs are valid inputs on all ranks.  A few notable points for this repro:\r\n\r\n* having only 1 allreduce, things pass\r\n* adding extra cuda synchronize() calls and print statements does not change anything, but i tried them as an unsuccessful way to narrow down the error\r\n* adding the second allreduce (and nan-check) causes an IMA error with a stack trace similar to the one above- implicating deallocation and `~TensorImpl`\r\ncc @XilunWu @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @d4l3k @c-p-i-o"}

{"number": 133010, "owner": "d4l3k", "title": "torch.multiprocessing.start_processes is blocking with large input arguments", "body": "### \ud83d\udc1b Describe the bug\r\n\r\nWith small input arguments (<64kb) `start_processes` runs quickly as the processes are launched asynchronously.\r\n\r\nWhen they're large we end up blocking in https://github.com/python/cpython/blob/main/Lib/multiprocessing/popen_spawn_posix.py#L62 when writing to the pipe. The default pipe buffer size is 64kb so larger than that requires the child process to fully start.\r\n\r\nhttps://unix.stackexchange.com/questions/11946/how-big-is-the-pipe-buffer\r\n\r\nRepro:\r\n```\r\nimport time\r\nfrom torch.multiprocessing.spawn import start_processes\r\nimport os\r\n\r\ntime.sleep(1)\r\n\r\ndef trainer(rank):\r\n    print(rank)\r\n\r\nif __name__ == '__main__':\r\n    world_size = 10\r\n\r\n    start = time.perf_counter()\r\n\r\n    args = [\"1\"*100000]\r\n\r\n    ctx = start_processes(\r\n            fn=trainer,\r\n            args=args,\r\n            nprocs=world_size,\r\n            start_method=\"spawn\",\r\n            join=False,\r\n        )\r\n\r\n    print(f\"Time taken: {time.perf_counter() - start}\")\r\n```\r\n\r\n### Versions\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.5.0a0+git21d4c48\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.2\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Stream 9 (x86_64)\r\nGCC version: (GCC) 11.4.1 20231218 (Red Hat 11.4.1-3)\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.34\r\n\r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.19.0-0_fbk21_hardened_12633_g4db063a1bcb5-x86_64-with-glibc2.34\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100\r\nGPU 1: NVIDIA H100\r\nGPU 2: NVIDIA H100\r\nGPU 3: NVIDIA H100\r\n\r\nNvidia driver version: 525.105.17\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.8.1\r\n/usr/lib64/libcudnn.so.9.1.1\r\n/usr/lib64/libcudnn_adv.so.9.1.1\r\n/usr/lib64/libcudnn_adv_infer.so.8.8.1\r\n/usr/lib64/libcudnn_adv_train.so.8.8.1\r\n/usr/lib64/libcudnn_cnn.so.9.1.1\r\n/usr/lib64/libcudnn_cnn_infer.so.8.8.1\r\n/usr/lib64/libcudnn_cnn_train.so.8.8.1\r\n/usr/lib64/libcudnn_engines_precompiled.so.9.1.1\r\n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.1.1\r\n/usr/lib64/libcudnn_graph.so.9.1.1\r\n/usr/lib64/libcudnn_heuristic.so.9.1.1\r\n/usr/lib64/libcudnn_ops.so.9.1.1\r\n/usr/lib64/libcudnn_ops_infer.so.8.8.1\r\n/usr/lib64/libcudnn_ops_train.so.8.8.1\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          184\r\nOn-line CPU(s) list:             0-183\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 9654 96-Core Processor\r\nCPU family:                      25\r\nModel:                           17\r\nThread(s) per core:              1\r\nCore(s) per socket:              184\r\nSocket(s):                       1\r\nStepping:                        1\r\nBogoMIPS:                        4792.79\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean pausefilter pfthreshold v_vmsave_vmload vgif avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm arch_capabilities\r\nVirtualization:                  AMD-V\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       11.5 MiB (184 instances)\r\nL1i cache:                       11.5 MiB (184 instances)\r\nL2 cache:                        92 MiB (184 instances)\r\nL3 cache:                        2.9 GiB (184 instances)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-183\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==6.1.0\r\n[pip3] flake8-bugbear==23.3.23\r\n[pip3] flake8-coding==1.3.3\r\n[pip3] flake8-comprehensions==3.15.0\r\n[pip3] flake8-executable==2.1.3\r\n[pip3] flake8-logging-format==0.9.0\r\n[pip3] flake8-pyi==23.3.1\r\n[pip3] flake8-simplify==0.19.3\r\n[pip3] mypy==1.10.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.0\r\n[pip3] optree==0.12.1\r\n[pip3] pytorch-triton==3.0.0+a9bc1a3647\r\n[pip3] torch==2.5.0a0+git21d4c48\r\n[pip3] torchx==0.6.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] magma-cuda121             2.6.1                         1    pytorch\r\n[conda] mkl                       2023.1.0         h213fc3f_46344  \r\n[conda] mkl-include               2023.2.0            intel_49495    intel\r\n[conda] mkl-service               2.4.0           py310h5eee18b_1  \r\n[conda] mkl-static                2023.2.0            intel_49495    intel\r\n[conda] mkl_fft                   1.3.8           py310h5eee18b_0  \r\n[conda] mkl_random                1.2.4           py310hdb19cb5_0  \r\n[conda] numpy                     1.26.0                   pypi_0    pypi\r\n[conda] numpy-base                1.26.4          py310hb5e798b_0  \r\n[conda] optree                    0.12.1                   pypi_0    pypi\r\n[conda] pytorch-triton            3.0.0+a9bc1a3647          pypi_0    pypi\r\n[conda] torch                     2.5.0a0+git21d4c48           dev_0    <develop>\r\n[conda] torchfix                  0.4.0                    pypi_0    pypi\r\n[conda] torchx                    0.6.0                    pypi_0    pypi\r\n```\r\n\r\ncc @VitalyFedyunin"}

